{"id": "2504.12308", "pdf": "https://arxiv.org/pdf/2504.12308", "abs": "https://arxiv.org/abs/2504.12308", "authors": ["Devansh Singh", "Sundaraparipurnan Narayanan"], "title": "Unmasking the Reality of PII Masking Models: Performance Gaps and the Call for Accountability", "categories": ["cs.CL", "cs.CY", "cs.IR"], "comment": null, "summary": "Privacy Masking is a critical concept under data privacy involving\nanonymization and de-anonymization of personally identifiable information\n(PII). Privacy masking techniques rely on Named Entity Recognition (NER)\napproaches under NLP support in identifying and classifying named entities in\neach text. NER approaches, however, have several limitations including (a)\ncontent sensitivity including ambiguous, polysemic, context dependent or domain\nspecific content, (b) phrasing variabilities including nicknames and alias,\ninformal expressions, alternative representations, emerging expressions,\nevolving naming conventions and (c) formats or syntax variations, typos,\nmisspellings. However, there are a couple of PII datasets that have been widely\nused by researchers and the open-source community to train models on PII\ndetection or masking. These datasets have been used to train models including\nPiiranha and Starpii, which have been downloaded over 300k and 580k times on\nHuggingFace. We examine the quality of the PII masking by these models given\nthe limitations of the datasets and of the NER approaches. We curate a dataset\nof 17K unique, semi-synthetic sentences containing 16 types of PII by compiling\ninformation from across multiple jurisdictions including India, U.K and U.S. We\ngenerate sentences (using language models) containing these PII at five\ndifferent NER detection feature dimensions - (1) Basic Entity Recognition, (2)\nContextual Entity Disambiguation, (3) NER in Noisy & Real-World Data, (4)\nEvolving & Novel Entities Detection and (5) Cross-Lingual or multi-lingual NER)\nand 1 in adversarial context. We present the results and exhibit the privacy\nexposure caused by such model use (considering the extent of lifetime downloads\nof these models). We conclude by highlighting the gaps in measuring performance\nof the models and the need for contextual disclosure in model cards for such\nmodels."}
{"id": "2504.12311", "pdf": "https://arxiv.org/pdf/2504.12311", "abs": "https://arxiv.org/abs/2504.12311", "authors": ["Enming Zhang", "Liwen Cao", "Yanru Wu", "Zijie Zhao", "Guan Wang", "Yang Li"], "title": "Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer", "categories": ["cs.CL"], "comment": null, "summary": "Prompt tuning has emerged as a lightweight adaptation strategy for adapting\nfoundation models to downstream tasks, particularly in resource-constrained\nsystems. As pre-trained prompts have become valuable intellectual assets,\ncombining multiple source prompts offers a promising approach to enhance\ngeneralization to new tasks by leveraging complementary knowledge from diverse\nsources. However, naive aggregation of these prompts often leads to\nrepresentation collapse due to mutual interference, undermining their\ncollective potential. To address these challenges, we propose HGPrompt, an\nadaptive framework for multi-source prompt transfer that learns optimal\nensemble weights by jointly optimizing dual objectives: transferability and\nstability. Specifically, we first introduce an information-theoretic metric to\nevaluate the transferability of prompt-induced features on the target task,\ncapturing the intrinsic alignment between the feature representations.\nAdditionally, we propose a novel Gradient Alignment Regularization to mitigate\ngradient conflicts among prompts, enabling stable and coherent knowledge\ntransfer from multiple sources while suppressing interference. Extensive\nexperiments on the large-scale VTAB benchmark demonstrate that HGPrompt\nachieves state-of-the-art performance, validating its effectiveness in\nmulti-source prompt transfer."}
{"id": "2504.12312", "pdf": "https://arxiv.org/pdf/2504.12312", "abs": "https://arxiv.org/abs/2504.12312", "authors": ["Zihao Xu", "Junchen Ding", "Yiling Lou", "Kun Zhang", "Dong Gong", "Yuekang Li"], "title": "Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved significant progress in language\nunderstanding and reasoning. Evaluating and analyzing their logical reasoning\nabilities has therefore become essential. However, existing datasets and\nbenchmarks are often limited to overly simplistic, unnatural, or contextually\nconstrained examples. In response to the growing demand, we introduce\nSmartyPat-Bench, a challenging, naturally expressed, and systematically labeled\nbenchmark derived from real-world high-quality Reddit posts containing subtle\nlogical fallacies. Unlike existing datasets and benchmarks, it provides more\ndetailed annotations of logical fallacies and features more diverse data. To\nfurther scale up the study and address the limitations of manual data\ncollection and labeling - such as fallacy-type imbalance and labor-intensive\nannotation - we introduce SmartyPat, an automated framework powered by logic\nprogramming-based oracles. SmartyPat utilizes Prolog rules to systematically\ngenerate logically fallacious statements, which are then refined into fluent\nnatural-language sentences by LLMs, ensuring precise fallacy representation.\nExtensive evaluation demonstrates that SmartyPat produces fallacies comparable\nin subtlety and quality to human-generated content and significantly\noutperforms baseline methods. Finally, experiments reveal nuanced insights into\nLLM capabilities, highlighting that while excessive reasoning steps hinder\nfallacy detection accuracy, structured reasoning enhances fallacy\ncategorization performance."}
{"id": "2504.12313", "pdf": "https://arxiv.org/pdf/2504.12313", "abs": "https://arxiv.org/abs/2504.12313", "authors": ["Xiaoyan Zhao", "Yang Deng", "Wenjie Wang", "Hongzhan lin", "Hong Cheng", "Rui Zhang", "See-Kiong Ng", "Tat-Seng Chua"], "title": "Exploring the Impact of Personality Traits on Conversational Recommender Systems: A Simulation with Large Language Models", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Conversational Recommender Systems (CRSs) engage users in multi-turn\ninteractions to deliver personalized recommendations. The emergence of large\nlanguage models (LLMs) further enhances these systems by enabling more natural\nand dynamic user interactions. However, a key challenge remains in\nunderstanding how personality traits shape conversational recommendation\noutcomes. Psychological evidence highlights the influence of personality traits\non user interaction behaviors. To address this, we introduce an LLM-based\npersonality-aware user simulation for CRSs (PerCRS). The user agent induces\ncustomizable personality traits and preferences, while the system agent\npossesses the persuasion capability to simulate realistic interaction in CRSs.\nWe incorporate multi-aspect evaluation to ensure robustness and conduct\nextensive analysis from both user and system perspectives. Experimental results\ndemonstrate that state-of-the-art LLMs can effectively generate diverse user\nresponses aligned with specified personality traits, thereby prompting CRSs to\ndynamically adjust their recommendation strategies. Our experimental analysis\noffers empirical insights into the impact of personality traits on the outcomes\nof conversational recommender systems."}
{"id": "2504.12314", "pdf": "https://arxiv.org/pdf/2504.12314", "abs": "https://arxiv.org/abs/2504.12314", "authors": ["Hao Li", "Liuzhenghao Lv", "He Cao", "Zijing Liu", "Zhiyuan Yan", "Yu Wang", "Yonghong Tian", "Yu Li", "Li Yuan"], "title": "How to Detect and Defeat Molecular Mirage: A Metric-Driven Benchmark for Hallucination in LLM-based Molecular Comprehension", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Large language models are increasingly used in scientific domains, especially\nfor molecular understanding and analysis. However, existing models are affected\nby hallucination issues, resulting in errors in drug design and utilization. In\nthis paper, we first analyze the sources of hallucination in LLMs for molecular\ncomprehension tasks, specifically the knowledge shortcut phenomenon observed in\nthe PubChem dataset. To evaluate hallucination in molecular comprehension tasks\nwith computational efficiency, we introduce \\textbf{Mol-Hallu}, a novel\nfree-form evaluation metric that quantifies the degree of hallucination based\non the scientific entailment relationship between generated text and actual\nmolecular properties. Utilizing the Mol-Hallu metric, we reassess and analyze\nthe extent of hallucination in various LLMs performing molecular comprehension\ntasks. Furthermore, the Hallucination Reduction Post-processing stage~(HRPP) is\nproposed to alleviate molecular hallucinations, Experiments show the\neffectiveness of HRPP on decoder-only and encoder-decoder molecular LLMs. Our\nfindings provide critical insights into mitigating hallucination and improving\nthe reliability of LLMs in scientific applications."}
{"id": "2504.12315", "pdf": "https://arxiv.org/pdf/2504.12315", "abs": "https://arxiv.org/abs/2504.12315", "authors": ["Xingguang Ji", "Jiakang Wang", "Hongzhi Zhang", "Jingyuan Zhang", "Haonan Zhou", "Chenxi Sun", "Yahui Liu", "Qi Wang", "Fuzheng Zhang"], "title": "Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "With the development of Multimodal Large Language Models (MLLMs), numerous\noutstanding accomplishments have emerged within the open-source community. Due\nto the complexity of creating and training multimodal data pairs, it is still a\ncomputational and time-consuming process to build powerful MLLMs. In this work,\nwe introduce Capybara-OMNI, an MLLM that trains in a lightweight and efficient\nmanner and supports understanding text, image, video, and audio modalities. We\npresent in detail the framework design, the data construction, and the training\nrecipe, to develop an MLLM step-by-step to obtain competitive performance. We\nalso provide exclusive benchmarks utilized in our experiments to show how to\nproperly verify understanding capabilities across different modalities. Results\nshow that by following our guidance, we can efficiently build an MLLM that\nachieves competitive performance among models of the same scale on various\nmultimodal benchmarks. Additionally, to enhance the multimodal instruction\nfollowing and conversational capabilities of the model, we further discuss how\nto train the chat version upon an MLLM understanding model, which is more in\nline with user habits for tasks like real-time interaction with humans. We\npublicly disclose the Capybara-OMNI model, along with its chat-based version.\nThe disclosure includes both the model weights, a portion of the training data,\nand the inference codes, which are made available on GitHub."}
{"id": "2504.12316", "pdf": "https://arxiv.org/pdf/2504.12316", "abs": "https://arxiv.org/abs/2504.12316", "authors": ["Jingyuan Zhang", "Hongzhi Zhang", "Zhou Haonan", "Chenxi Sun", "Xingguang ji", "Jiakang Wang", "Fanheng Kong", "Yahui Liu", "Qi Wang", "Fuzheng Zhang"], "title": "Data Metabolism: An Efficient Data Design Schema For Vision Language Model", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "To be presented at ICLR 2025, First Workshop on Open Science for\n  Foundation Models", "summary": "Data curation plays a crucial role in training powerful Visual Language\nModels (VLMs). In this work, we introduce the concept of Data Metabolism and\npresent our data-centric framework to build VLMs throughout the development\nlifecycle. Starting from a standard model architecture, we discuss and provide\ninsights into two crucial development steps: data curation and iteration,\nforming a closed-loop system that continuously improves model performance. We\nshow a detailed codebook on how to process existing massive datasets and build\nuser-specific data flywheel. As a demonstration, we release a VLM, named\nCapybara-VL, which excels in typical multimodal tasks (e.g. , visual question\nanswering, scientific reasoning, and text-rich tasks). Despite its relatively\ncompact size, Capybara-VL surpasses several open-source models that are up to\n10 times larger in size. Moreover, it achieves results that are on par with\nthose of several leading proprietary models, demonstrating its remarkable\ncompetitiveness. These results highlight the power of our data-centric\nframework and the potential of training smaller and more efficient VLMs."}
{"id": "2504.12317", "pdf": "https://arxiv.org/pdf/2504.12317", "abs": "https://arxiv.org/abs/2504.12317", "authors": ["Dingkang Lin", "Naixuan Zhao", "Dan Tian", "Jiang Li"], "title": "ChatGPT as Linguistic Equalizer? Quantifying LLM-Driven Lexical Shifts in Academic Writing", "categories": ["cs.CL"], "comment": "13 pages, 2 figures", "summary": "The advent of ChatGPT has profoundly reshaped scientific research practices,\nparticularly in academic writing, where non-native English-speakers (NNES)\nhistorically face linguistic barriers. This study investigates whether ChatGPT\nmitigates these barriers and fosters equity by analyzing lexical complexity\nshifts across 2.8 million articles from OpenAlex (2020-2024). Using the Measure\nof Textual Lexical Diversity (MTLD) to quantify vocabulary sophistication and a\ndifference-in-differences (DID) design to identify causal effects, we\ndemonstrate that ChatGPT significantly enhances lexical complexity in\nNNES-authored abstracts, even after controlling for article-level controls,\nauthorship patterns, and venue norms. Notably, the impact is most pronounced in\npreprint papers, technology- and biology-related fields and lower-tier\njournals. These findings provide causal evidence that ChatGPT reduces\nlinguistic disparities and promotes equity in global academia."}
{"id": "2504.12320", "pdf": "https://arxiv.org/pdf/2504.12320", "abs": "https://arxiv.org/abs/2504.12320", "authors": ["Jennifer Haase", "Paul H. P. Hanel", "Sebastian Pokutta"], "title": "Has the Creativity of Large-Language Models peaked? An analysis of inter- and intra-LLM variability", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "19 pages + Appendix, 13 figure", "summary": "Following the widespread adoption of ChatGPT in early 2023, numerous studies\nreported that large language models (LLMs) can match or even surpass human\nperformance in creative tasks. However, it remains unclear whether LLMs have\nbecome more creative over time, and how consistent their creative output is. In\nthis study, we evaluated 14 widely used LLMs -- including GPT-4, Claude, Llama,\nGrok, Mistral, and DeepSeek -- across two validated creativity assessments: the\nDivergent Association Task (DAT) and the Alternative Uses Task (AUT). Contrary\nto expectations, we found no evidence of increased creative performance over\nthe past 18-24 months, with GPT-4 performing worse than in previous studies.\nFor the more widely used AUT, all models performed on average better than the\naverage human, with GPT-4o and o3-mini performing best. However, only 0.28% of\nLLM-generated responses reached the top 10% of human creativity benchmarks.\nBeyond inter-model differences, we document substantial intra-model\nvariability: the same LLM, given the same prompt, can produce outputs ranging\nfrom below-average to original. This variability has important implications for\nboth creativity research and practical applications. Ignoring such variability\nrisks misjudging the creative potential of LLMs, either inflating or\nunderestimating their capabilities. The choice of prompts affected LLMs\ndifferently. Our findings underscore the need for more nuanced evaluation\nframeworks and highlight the importance of model selection, prompt design, and\nrepeated assessment when using Generative AI (GenAI) tools in creative\ncontexts."}
{"id": "2504.12321", "pdf": "https://arxiv.org/pdf/2504.12321", "abs": "https://arxiv.org/abs/2504.12321", "authors": ["Charlotte Siska", "Anush Sankaran"], "title": "AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the past few years, Language Models (LMs) have shown par-human\ncapabilities in several domains. Despite their practical applications and\nexceeding user consumption, they are susceptible to jailbreaks when malicious\ninput exploits the LM's weaknesses, causing it to deviate from its intended\nbehavior. Current defensive strategies either classify the input prompt as\nadversarial or prevent LMs from generating harmful outputs. However, it is\nchallenging to explain the reason behind the malicious nature of the jailbreak,\nwhich results in a wide variety of closed-box approaches. In this research, we\npropose and demonstrate that system-prompt attention from Small Language Models\n(SLMs) can be used to characterize adversarial prompts, providing a novel,\nexplainable, and cheaper defense approach called AttentionDefense. Our research\nsuggests that the attention mechanism is an integral component in understanding\nand explaining how LMs respond to malicious input that is not captured in the\nsemantic meaning of text embeddings. The proposed AttentionDefense is evaluated\nagainst existing jailbreak benchmark datasets. Ablation studies show that\nSLM-based AttentionDefense has equivalent or better jailbreak detection\nperformance compared to text embedding-based classifiers and GPT-4 zero-shot\ndetectors.To further validate the efficacy of the proposed approach, we\ngenerate a dataset of novel jailbreak variants of the existing benchmark\ndataset using a closed-loop LLM-based multi-agent system. We demonstrate that\nthe proposed AttentionDefense approach performs robustly on this novel\njailbreak dataset while existing approaches suffer in performance.\nAdditionally, for practical purposes AttentionDefense is an ideal solution as\nit has the computation requirements of a small LM but the performance of a LLM\ndetector."}
{"id": "2504.12322", "pdf": "https://arxiv.org/pdf/2504.12322", "abs": "https://arxiv.org/abs/2504.12322", "authors": ["Xin Gao", "Qizhi Pei", "Zinan Tang", "Yu Li", "Honglin Lin", "Jiang Wu", "Conghui He", "Lijun Wu"], "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA."}
{"id": "2504.12323", "pdf": "https://arxiv.org/pdf/2504.12323", "abs": "https://arxiv.org/abs/2504.12323", "authors": ["Zheng Zhang", "Ning Li", "Qi Liu", "Rui Li", "Weibo Gao", "Qingyang Mao", "Zhenya Huang", "Baosheng Yu", "Dacheng Tao"], "title": "The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nretrieving relevant document from external knowledge sources. By referencing\nthis external knowledge, RAG effectively reduces the generation of factually\nincorrect content and addresses hallucination issues within LLMs. Recently,\nthere has been growing attention to improving the performance and efficiency of\nRAG systems from various perspectives. While these advancements have yielded\nsignificant results, the application of RAG in domains with considerable\nsocietal implications raises a critical question about fairness: What impact\ndoes the introduction of the RAG paradigm have on the fairness of LLMs? To\naddress this question, we conduct extensive experiments by varying the LLMs,\nretrievers, and retrieval sources. Our experimental analysis reveals that the\nscale of the LLMs plays a significant role in influencing fairness outcomes\nwithin the RAG framework. When the model scale is smaller than 8B, the\nintegration of retrieval mechanisms often exacerbates unfairness in small-scale\nLLMs (e.g., LLaMA3.2-1B, Mistral-7B, and LLaMA3-8B). To mitigate the fairness\nissues introduced by RAG for small-scale LLMs, we propose two approaches,\nFairFT and FairFilter. Specifically, in FairFT, we align the retriever with the\nLLM in terms of fairness, enabling it to retrieve documents that facilitate\nfairer model outputs. In FairFilter, we propose a fairness filtering mechanism\nto filter out biased content after retrieval. Finally, we validate our proposed\napproaches on real-world datasets, demonstrating their effectiveness in\nimproving fairness while maintaining performance."}
{"id": "2504.12324", "pdf": "https://arxiv.org/pdf/2504.12324", "abs": "https://arxiv.org/abs/2504.12324", "authors": ["Mengying Yuan", "Wangzi Xuan", "Fei Li"], "title": "Cross-Document Cross-Lingual Natural Language Inference via RST-enhanced Graph Fusion and Interpretability Prediction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural Language Inference (NLI) is a fundamental task in both natural\nlanguage processing and information retrieval. While NLI has developed many\nsub-directions such as sentence-level NLI, document-level NLI and cross-lingual\nNLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In\nthis paper, we propose a novel paradigm for CDCL-NLI that extends traditional\nNLI capabilities to multi-document, multilingual scenarios. To support this\ntask, we construct a high-quality CDCL-NLI dataset including 1,110 instances\nand spanning 26 languages. To build a baseline for this task, we also propose\nan innovative method that integrates RST-enhanced graph fusion and\ninterpretability prediction. Our method employs RST (Rhetorical Structure\nTheory) on RGAT (Relation-aware Graph Attention Network) for cross-document\ncontext modeling, coupled with a structure-aware semantic alignment mechanism\nbased on lexical chains for cross-lingual understanding. For NLI\ninterpretability, we develop an EDU-level attribution framework that generates\nextractive explanations. Extensive experiments demonstrate our approach's\nsuperior performance, achieving significant improvements over both traditional\nNLI models such as DocNLI and R2F, as well as LLMs like Llama3 and GPT-4o. Our\nwork sheds light on the study of NLI and will bring research interest on\ncross-document cross-lingual context understanding, semantic retrieval and\ninterpretability inference. Our dataset and code are available at\n\\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-Link for peer\nreview}."}
{"id": "2504.12325", "pdf": "https://arxiv.org/pdf/2504.12325", "abs": "https://arxiv.org/abs/2504.12325", "authors": ["Haiqi Zhang", "Zhengyuan Zhu", "Zeyu Zhang", "Chengkai Li"], "title": "LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "With the vast expansion of content on social media platforms, analyzing and\ncomprehending online discourse has become increasingly complex. This paper\nintroduces LLMTaxo, a novel framework leveraging large language models for the\nautomated construction of taxonomy of factual claims from social media by\ngenerating topics from multi-level granularities. This approach aids\nstakeholders in more effectively navigating the social media landscapes. We\nimplement this framework with different models across three distinct datasets\nand introduce specially designed taxonomy evaluation metrics for a\ncomprehensive assessment. With the evaluations from both human evaluators and\nGPT-4, the results indicate that LLMTaxo effectively categorizes factual claims\nfrom social media, and reveals that certain models perform better on specific\ndatasets."}
{"id": "2504.12326", "pdf": "https://arxiv.org/pdf/2504.12326", "abs": "https://arxiv.org/abs/2504.12326", "authors": ["Shahriar Noroozizadeh", "Jeremy C. Weiss"], "title": "Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Clinical case reports and discharge summaries may be the most complete and\naccurate summarization of patient encounters, yet they are finalized, i.e.,\ntimestamped after the encounter. Complementary data structured streams become\navailable sooner but suffer from incompleteness. To train models and algorithms\non more complete and temporally fine-grained data, we construct a pipeline to\nphenotype, extract, and annotate time-localized findings within case reports\nusing large language models. We apply our pipeline to generate an open-access\ntextual time series corpus for Sepsis-3 comprising 2,139 case reports from the\nPubmed-Open Access (PMOA) Subset. To validate our system, we apply it on PMOA\nand timeline annotations from I2B2/MIMIC-IV and compare the results to\nphysician-expert annotations. We show high recovery rates of clinical findings\n(event match rates: O1-preview--0.755, Llama 3.3 70B Instruct--0.753) and\nstrong temporal ordering (concordance: O1-preview--0.932, Llama 3.3 70B\nInstruct--0.932). Our work characterizes the ability of LLMs to time-localize\nclinical findings in text, illustrating the limitations of LLM use for temporal\nreconstruction and providing several potential avenues of improvement via\nmultimodal integration."}
{"id": "2504.12327", "pdf": "https://arxiv.org/pdf/2504.12327", "abs": "https://arxiv.org/abs/2504.12327", "authors": ["Yuxi Ma", "Yongqian Peng", "Yixin Zhu"], "title": "Word Embeddings Track Social Group Changes Across 70 Years in China", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Language encodes societal beliefs about social groups through word patterns.\nWhile computational methods like word embeddings enable quantitative analysis\nof these patterns, studies have primarily examined gradual shifts in Western\ncontexts. We present the first large-scale computational analysis of Chinese\nstate-controlled media (1950-2019) to examine how revolutionary social\ntransformations are reflected in official linguistic representations of social\ngroups. Using diachronic word embeddings at multiple temporal resolutions, we\nfind that Chinese representations differ significantly from Western\ncounterparts, particularly regarding economic status, ethnicity, and gender.\nThese representations show distinct evolutionary dynamics: while stereotypes of\nethnicity, age, and body type remain remarkably stable across political\nupheavals, representations of gender and economic classes undergo dramatic\nshifts tracking historical transformations. This work advances our\nunderstanding of how officially sanctioned discourse encodes social structure\nthrough language while highlighting the importance of non-Western perspectives\nin computational social science."}
{"id": "2504.12328", "pdf": "https://arxiv.org/pdf/2504.12328", "abs": "https://arxiv.org/abs/2504.12328", "authors": ["Jialun Zhong", "Wei Shen", "Yanzeng Li", "Songyang Gao", "Hua Lu", "Yicheng Chen", "Yang Zhang", "Wei Zhou", "Jinjie Gu", "Lei Zou"], "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reward Model (RM) has demonstrated impressive potential for enhancing Large\nLanguage Models (LLM), as RM can serve as a proxy for human preferences,\nproviding signals to guide LLMs' behavior in various tasks. In this paper, we\nprovide a comprehensive overview of relevant research, exploring RMs from the\nperspectives of preference collection, reward modeling, and usage. Next, we\nintroduce the applications of RMs and discuss the benchmarks for evaluation.\nFurthermore, we conduct an in-depth analysis of the challenges existing in the\nfield and dive into the potential research directions. This paper is dedicated\nto providing beginners with a comprehensive introduction to RMs and\nfacilitating future studies. The resources are publicly available at\ngithub\\footnote{https://github.com/JLZhong23/awesome-reward-models}."}
{"id": "2504.12329", "pdf": "https://arxiv.org/pdf/2504.12329", "abs": "https://arxiv.org/abs/2504.12329", "authors": ["Wang Yang", "Xiang Yue", "Vipin Chaudhary", "Xiaotian Han"], "title": "Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances leverage post-training to enhance model reasoning\nperformance, which typically requires costly training pipelines and still\nsuffers from inefficient, overly lengthy outputs. We introduce Speculative\nThinking, a training-free framework that enables large reasoning models to\nguide smaller ones during inference at the reasoning level, distinct from\nspeculative decoding, which operates at the token level. Our approach is based\non two observations: (1) reasoning-supportive tokens such as \"wait\" frequently\nappear after structural delimiters like \"\\n\\n\", serving as signals for\nreflection or continuation; and (2) larger models exhibit stronger control over\nreflective behavior, reducing unnecessary backtracking while improving\nreasoning quality. By strategically delegating reflective steps to a more\ncapable model, our method significantly boosts the reasoning accuracy of\nreasoning models while shortening their output. With the assistance of the 32B\nreasoning model, the 1.5B model's accuracy on MATH500 increases from 83.2% to\n89.4%, marking a substantial improvement of 6.2%. Simultaneously, the average\noutput length is reduced from 5439 tokens to 4583 tokens, representing a 15.7%\ndecrease. Moreover, when applied to a non-reasoning model\n(Qwen-2.5-7B-Instruct), our framework boosts its accuracy from 74.0% to 81.8%\non the same benchmark, achieving a relative improvement of 7.8%."}
{"id": "2504.12330", "pdf": "https://arxiv.org/pdf/2504.12330", "abs": "https://arxiv.org/abs/2504.12330", "authors": ["Pei Liu", "Xin Liu", "Ruoyu Yao", "Junming Liu", "Siyuan Meng", "Ding Wang", "Jun Ma"], "title": "HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) augments Large Language Models\n(LLMs) with external knowledge, conventional single-agent RAG remains\nfundamentally limited in resolving complex queries demanding coordinated\nreasoning across heterogeneous data ecosystems. We present HM-RAG, a novel\nHierarchical Multi-agent Multimodal RAG framework that pioneers collaborative\nintelligence for dynamic knowledge synthesis across structured, unstructured,\nand graph-based data. The framework is composed of three-tiered architecture\nwith specialized agents: a Decomposition Agent that dissects complex queries\ninto contextually coherent sub-tasks via semantic-aware query rewriting and\nschema-guided context augmentation; Multi-source Retrieval Agents that carry\nout parallel, modality-specific retrieval using plug-and-play modules designed\nfor vector, graph, and web-based databases; and a Decision Agent that uses\nconsistency voting to integrate multi-source answers and resolve discrepancies\nin retrieval results through Expert Model Refinement. This architecture attains\ncomprehensive query understanding by combining textual, graph-relational, and\nweb-derived evidence, resulting in a remarkable 12.95% improvement in answer\naccuracy and a 3.56% boost in question classification accuracy over baseline\nRAG systems on the ScienceQA and CrisisMMD benchmarks. Notably, HM-RAG\nestablishes state-of-the-art results in zero-shot settings on both datasets.\nIts modular architecture ensures seamless integration of new data modalities\nwhile maintaining strict data governance, marking a significant advancement in\naddressing the critical challenges of multimodal reasoning and knowledge\nsynthesis in RAG systems. Code is available at\nhttps://github.com/ocean-luna/HMRAG."}
{"id": "2504.12331", "pdf": "https://arxiv.org/pdf/2504.12331", "abs": "https://arxiv.org/abs/2504.12331", "authors": ["Xiangju Li", "Dong Yang", "Xiaogang Zhu", "Faliang Huang", "Peng Zhang", "Zhongying Zhao"], "title": "Span-level Emotion-Cause-Category Triplet Extraction with Instruction Tuning LLMs and Data Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Span-level emotion-cause-category triplet extraction represents a novel and\ncomplex challenge within emotion cause analysis. This task involves identifying\nemotion spans, cause spans, and their associated emotion categories within the\ntext to form structured triplets. While prior research has predominantly\nconcentrated on clause-level emotion-cause pair extraction and span-level\nemotion-cause detection, these methods often confront challenges originating\nfrom redundant information retrieval and difficulty in accurately determining\nemotion categories, particularly when emotions are expressed implicitly or\nambiguously. To overcome these challenges, this study explores a fine-grained\napproach to span-level emotion-cause-category triplet extraction and introduces\nan innovative framework that leverages instruction tuning and data augmentation\ntechniques based on large language models. The proposed method employs\ntask-specific triplet extraction instructions and utilizes low-rank adaptation\nto fine-tune large language models, eliminating the necessity for intricate\ntask-specific architectures. Furthermore, a prompt-based data augmentation\nstrategy is developed to address data scarcity by guiding large language models\nin generating high-quality synthetic training data. Extensive experimental\nevaluations demonstrate that the proposed approach significantly outperforms\nexisting baseline methods, achieving at least a 12.8% improvement in span-level\nemotion-cause-category triplet extraction metrics. The results demonstrate the\nmethod's effectiveness and robustness, offering a promising avenue for\nadvancing research in emotion cause analysis. The source code is available at\nhttps://github.com/zxgnlp/InstruDa-LLM."}
{"id": "2504.12332", "pdf": "https://arxiv.org/pdf/2504.12332", "abs": "https://arxiv.org/abs/2504.12332", "authors": ["Mingrui Zan", "Yunquan Zhang", "Boyang Zhang", "Fangming Liu", "Daning Cheng"], "title": "Can the capability of Large Language Models be described by human ability? A Meta Study", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Users of Large Language Models (LLMs) often perceive these models as\nintelligent entities with human-like capabilities. However, the extent to which\nLLMs' capabilities truly approximate human abilities remains a topic of debate.\nIn this paper, to characterize the capabilities of LLMs in relation to human\ncapabilities, we collected performance data from over 80 models across 37\nevaluation benchmarks. The evaluation benchmarks are categorized into 6 primary\nabilities and 11 sub-abilities in human aspect. Then, we then clustered the\nperformance rankings into several categories and compared these clustering\nresults with classifications based on human ability aspects. Our findings lead\nto the following conclusions: 1. We have confirmed that certain capabilities of\nLLMs with fewer than 10 billion parameters can indeed be described using human\nability metrics; 2. While some abilities are considered interrelated in humans,\nthey appear nearly uncorrelated in LLMs; 3. The capabilities possessed by LLMs\nvary significantly with the parameter scale of the model."}
{"id": "2504.12333", "pdf": "https://arxiv.org/pdf/2504.12333", "abs": "https://arxiv.org/abs/2504.12333", "authors": ["Andrés Isaza-Giraldo", "Paulo Bala", "Lucas Pereira"], "title": "Meta-Evaluating Local LLMs: Rethinking Performance Metrics for Serious Games", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "2nd HEAL Workshop at CHI Conference on Human Factors in Computing\n  Systems. April 26, 2025. Yokohama, Japan", "summary": "The evaluation of open-ended responses in serious games presents a unique\nchallenge, as correctness is often subjective. Large Language Models (LLMs) are\nincreasingly being explored as evaluators in such contexts, yet their accuracy\nand consistency remain uncertain, particularly for smaller models intended for\nlocal execution. This study investigates the reliability of five small-scale\nLLMs when assessing player responses in \\textit{En-join}, a game that simulates\ndecision-making within energy communities. By leveraging traditional binary\nclassification metrics (including accuracy, true positive rate, and true\nnegative rate), we systematically compare these models across different\nevaluation scenarios. Our results highlight the strengths and limitations of\neach model, revealing trade-offs between sensitivity, specificity, and overall\nperformance. We demonstrate that while some models excel at identifying correct\nresponses, others struggle with false positives or inconsistent evaluations.\nThe findings highlight the need for context-aware evaluation frameworks and\ncareful model selection when deploying LLMs as evaluators. This work\ncontributes to the broader discourse on the trustworthiness of AI-driven\nassessment tools, offering insights into how different LLM architectures handle\nsubjective evaluation tasks."}
{"id": "2504.12334", "pdf": "https://arxiv.org/pdf/2504.12334", "abs": "https://arxiv.org/abs/2504.12334", "authors": ["Zongxian Yang", "Jiayu Qian", "Zhi-An Huang", "Kay Chen Tan"], "title": "QM-ToT: A Medical Tree of Thoughts Reasoning Framework for Quantized Model", "categories": ["cs.CL"], "comment": "8 pages", "summary": "Large language models (LLMs) face significant challenges in specialized\nbiomedical tasks due to the inherent complexity of medical reasoning and the\nsensitive nature of clinical data. Existing LLMs often struggle with intricate\nmedical terminology and the need for accurate clinical insights, leading to\nperformance reduction when quantized for resource-constrained deployment. To\naddress these issues, we propose Quantized Medical Tree of Thought (QM-ToT), a\npath-based reasoning framework. QM-ToT leverages a Tree of Thought (ToT)\nreasoning approach to decompose complex medical problems into manageable\nsubtasks, coupled with evaluator assessment layers. This framework facilitates\nsubstantial performance improvements in INT4-quantized models on the\nchallenging MedQAUSMLE dataset. Specifically, we demonstrate a remarkable\naccuracy increase from 34% to 50% for the LLaMA2-70b model and from 58.77% to\n69.49% for LLaMA-3.1-8b. Besides, we also proposed an effect data distillation\nmethod based on ToT. Compared to the traditional distillation method, we\nachieved an improvement of 86. 27% while using only 3.9% of the data.This work,\nfor the first time, showcases the potential of ToT to significantly enhance\nperformance on complex biomedical tasks, establishing a crucial foundation for\nfuture advances in deploying high-performing quantized LLM in resource-limited\nmedical settings."}
{"id": "2504.12335", "pdf": "https://arxiv.org/pdf/2504.12335", "abs": "https://arxiv.org/abs/2504.12335", "authors": ["Alden Dima", "James Foulds", "Shimei Pan", "Philip Feldman"], "title": "You've Changed: Detecting Modification of Black-Box Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "26 pages, 4 figures", "summary": "Large Language Models (LLMs) are often provided as a service via an API,\nmaking it challenging for developers to detect changes in their behavior. We\npresent an approach to monitor LLMs for changes by comparing the distributions\nof linguistic and psycholinguistic features of generated text. Our method uses\na statistical test to determine whether the distributions of features from two\nsamples of text are equivalent, allowing developers to identify when an LLM has\nchanged. We demonstrate the effectiveness of our approach using five OpenAI\ncompletion models and Meta's Llama 3 70B chat model. Our results show that\nsimple text features coupled with a statistical test can distinguish between\nlanguage models. We also explore the use of our approach to detect prompt\ninjection attacks. Our work enables frequent LLM change monitoring and avoids\ncomputationally expensive benchmark evaluations."}
{"id": "2504.12337", "pdf": "https://arxiv.org/pdf/2504.12337", "abs": "https://arxiv.org/abs/2504.12337", "authors": ["Anna-Carolina Haensch"], "title": "\"It Listens Better Than My Therapist\": Exploring Social Media Discourse on LLMs as Mental Health Tool", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.SI", "J.4"], "comment": "This study does not endorse or encourage the use of AI tools as\n  substitutes for professional mental health support. The findings are\n  presented for research purposes only, and any interpretation should take into\n  account the limitations and potential risks of relying on AI in mental health\n  contexts", "summary": "The emergence of generative AI chatbots such as ChatGPT has prompted growing\npublic and academic interest in their role as informal mental health support\ntools. While early rule-based systems have been around for several years, large\nlanguage models (LLMs) offer new capabilities in conversational fluency,\nempathy simulation, and availability. This study explores how users engage with\nLLMs as mental health tools by analyzing over 10,000 TikTok comments from\nvideos referencing LLMs as mental health tools. Using a self-developed tiered\ncoding schema and supervised classification models, we identify user\nexperiences, attitudes, and recurring themes. Results show that nearly 20% of\ncomments reflect personal use, with these users expressing overwhelmingly\npositive attitudes. Commonly cited benefits include accessibility, emotional\nsupport, and perceived therapeutic value. However, concerns around privacy,\ngeneric responses, and the lack of professional oversight remain prominent. It\nis important to note that the user feedback does not indicate which therapeutic\nframework, if any, the LLM-generated output aligns with. While the findings\nunderscore the growing relevance of AI in everyday practices, they also\nhighlight the urgent need for clinical and ethical scrutiny in the use of AI\nfor mental health support."}
{"id": "2504.12338", "pdf": "https://arxiv.org/pdf/2504.12338", "abs": "https://arxiv.org/abs/2504.12338", "authors": ["David Anderson", "Michaela Anderson", "Margret Bjarnadottir", "Stephen Mahar", "Shriyan Reyya"], "title": "Paging Dr. GPT: Extracting Information from Clinical Notes to Enhance Patient Predictions", "categories": ["cs.CL", "cs.LG", "I.2.0"], "comment": "Paper and Online Supplement combined into one PDF. 26 pages. 2\n  figures", "summary": "There is a long history of building predictive models in healthcare using\ntabular data from electronic medical records. However, these models fail to\nextract the information found in unstructured clinical notes, which document\ndiagnosis, treatment, progress, medications, and care plans. In this study, we\ninvestigate how answers generated by GPT-4o-mini (ChatGPT) to simple clinical\nquestions about patients, when given access to the patient's discharge summary,\ncan support patient-level mortality prediction. Using data from 14,011\nfirst-time admissions to the Coronary Care or Cardiovascular Intensive Care\nUnits in the MIMIC-IV Note dataset, we implement a transparent framework that\nuses GPT responses as input features in logistic regression models. Our\nfindings demonstrate that GPT-based models alone can outperform models trained\non standard tabular data, and that combining both sources of information yields\neven greater predictive power, increasing AUC by an average of 5.1 percentage\npoints and increasing positive predictive value by 29.9 percent for the\nhighest-risk decile. These results highlight the value of integrating large\nlanguage models (LLMs) into clinical prediction tasks and underscore the\nbroader potential for using LLMs in any domain where unstructured text data\nremains an underutilized resource."}
{"id": "2504.12339", "pdf": "https://arxiv.org/pdf/2504.12339", "abs": "https://arxiv.org/abs/2504.12339", "authors": ["Yaodong Song", "Hongjie Chen", "Jie Lian", "Yuxin Zhang", "Guangmin Xia", "Zehan Li", "Genliang Zhao", "Jian Kang", "Yongxiang Li", "Jie Li"], "title": "GOAT-TTS: LLM-based Text-To-Speech Generation Optimized via A Dual-Branch Architecture", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "While large language models (LLMs) have revolutionized text-to-speech (TTS)\nsynthesis through discrete tokenization paradigms, current architectures\nexhibit fundamental tensions between three critical dimensions: 1) irreversible\nloss of acoustic characteristics caused by quantization of speech prompts; 2)\nstringent dependence on precisely aligned prompt speech-text pairs that limit\nreal-world deployment; and 3) catastrophic forgetting of the LLM's native text\ncomprehension during optimization for speech token generation. To address these\nchallenges, we propose an LLM-based text-to-speech Generation approach\nOptimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework\nintroduces two key innovations: (1) The modality-alignment branch combines a\nspeech encoder and projector to capture continuous acoustic embeddings,\nenabling bidirectional correlation between paralinguistic features (language,\ntimbre, emotion) and semantic text representations without transcript\ndependency; (2) The speech-generation branch employs modular fine-tuning on\ntop-k layers of an LLM for speech token prediction while freezing the bottom-k\nlayers to preserve foundational linguistic knowledge. Moreover, multi-token\nprediction is introduced to support real-time streaming TTS synthesis.\nExperimental results demonstrate that our GOAT-TTS achieves performance\ncomparable to state-of-the-art TTS models while validating the efficacy of\nsynthesized dialect speech data."}
{"id": "2504.12341", "pdf": "https://arxiv.org/pdf/2504.12341", "abs": "https://arxiv.org/abs/2504.12341", "authors": ["Linqing Chen", "Weilei Wang", "Yubin Xia", "Wentao Wu", "Peng Xu", "Zilong Bai", "Jie Fang", "Chaobo Xu", "Ran Hu", "Licong Xu", "Haoran Hua", "Jing Sun", "Hanmeng Zhong", "Jin Liu", "Tian Qiu", "Haowen Liu", "Meng Hu", "Xiuwen Li", "Fei Gao", "Yong Gu", "Tao Shi", "Chaochao Wang", "Jianping Lu", "Cheng Sun", "Yixin Wang", "Shengjie Yang", "Yuancheng Li", "Lu Jin", "Lisha Zhang", "Fu Bian", "Zhongkai Ye", "Lidong Pei", "Changyang Tu"], "title": "Streamlining Biomedical Research with Specialized LLMs", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose a novel system that integrates state-of-the-art,\ndomain-specific large language models with advanced information retrieval\ntechniques to deliver comprehensive and context-aware responses. Our approach\nfacilitates seamless interaction among diverse components, enabling\ncross-validation of outputs to produce accurate, high-quality responses\nenriched with relevant data, images, tables, and other modalities. We\ndemonstrate the system's capability to enhance response precision by leveraging\na robust question-answering model, significantly improving the quality of\ndialogue generation. The system provides an accessible platform for real-time,\nhigh-fidelity interactions, allowing users to benefit from efficient\nhuman-computer interaction, precise retrieval, and simultaneous access to a\nwide range of literature and data. This dramatically improves the research\nefficiency of professionals in the biomedical and pharmaceutical domains and\nfacilitates faster, more informed decision-making throughout the R\\&D process.\nFurthermore, the system proposed in this paper is available at\nhttps://synapse-chat.patsnap.com."}
{"id": "2504.12342", "pdf": "https://arxiv.org/pdf/2504.12342", "abs": "https://arxiv.org/abs/2504.12342", "authors": ["Hanmeng Zhong", "Linqing Chen", "Weilei Wang", "Wentao Wu"], "title": "Benchmarking Biopharmaceuticals Retrieval-Augmented Generation Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Recently, the application of the retrieval-augmented Large Language Models\n(LLMs) in specific domains has gained significant attention, especially in\nbiopharmaceuticals. However, in this context, there is no benchmark\nspecifically designed for biopharmaceuticals to evaluate LLMs. In this paper,\nwe introduce the Biopharmaceuticals Retrieval-Augmented Generation Evaluation\n(BRAGE) , the first benchmark tailored for evaluating LLMs' Query and Reference\nUnderstanding Capability (QRUC) in the biopharmaceutical domain, available in\nEnglish, French, German and Chinese. In addition, Traditional\nQuestion-Answering (QA) metrics like accuracy and exact match fall short in the\nopen-ended retrieval-augmented QA scenarios. To address this, we propose a\ncitation-based classification method to evaluate the QRUC of LLMs to understand\nthe relationship between queries and references. We apply this method to\nevaluate the mainstream LLMs on BRAGE. Experimental results show that there is\na significant gap in the biopharmaceutical QRUC of mainstream LLMs, and their\nQRUC needs to be improved."}
{"id": "2504.12344", "pdf": "https://arxiv.org/pdf/2504.12344", "abs": "https://arxiv.org/abs/2504.12344", "authors": ["Nay Myat Min", "Long H. Pham", "Yige Li", "Jun Sun"], "title": "Propaganda via AI? A Study on Semantic Backdoors in Large Language Models", "categories": ["cs.CL"], "comment": "18 pages, 1 figure", "summary": "Large language models (LLMs) demonstrate remarkable performance across myriad\nlanguage tasks, yet they remain vulnerable to backdoor attacks, where\nadversaries implant hidden triggers that systematically manipulate model\noutputs. Traditional defenses focus on explicit token-level anomalies and\ntherefore overlook semantic backdoors-covert triggers embedded at the\nconceptual level (e.g., ideological stances or cultural references) that rely\non meaning-based cues rather than lexical oddities. We first show, in a\ncontrolled finetuning setting, that such semantic backdoors can be implanted\nwith only a small poisoned corpus, establishing their practical feasibility. We\nthen formalize the notion of semantic backdoors in LLMs and introduce a\nblack-box detection framework, RAVEN (short for \"Response Anomaly Vigilance for\nuncovering semantic backdoors\"), which combines semantic entropy with\ncross-model consistency analysis. The framework probes multiple models with\nstructured topic-perspective prompts, clusters the sampled responses via\nbidirectional entailment, and flags anomalously uniform outputs; cross-model\ncomparison isolates model-specific anomalies from corpus-wide biases. Empirical\nevaluations across diverse LLM families (GPT-4o, Llama, DeepSeek, Mistral)\nuncover previously undetected semantic backdoors, providing the first\nproof-of-concept evidence of these hidden vulnerabilities and underscoring the\nurgent need for concept-level auditing of deployed language models. We\nopen-source our code and data at https://github.com/NayMyatMin/RAVEN."}
{"id": "2504.12345", "pdf": "https://arxiv.org/pdf/2504.12345", "abs": "https://arxiv.org/abs/2504.12345", "authors": ["Yutong Xia", "Ao Qu", "Yunhan Zheng", "Yihong Tang", "Dingyi Zhuang", "Yuxuan Liang", "Cathy Wu", "Roger Zimmermann", "Jinhua Zhao"], "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language Models", "categories": ["cs.CL", "cs.CY", "cs.MA"], "comment": null, "summary": "Urban causal research is essential for understanding the complex dynamics of\ncities and informing evidence-based policies. However, it is challenged by the\ninefficiency and bias of hypothesis generation, barriers to multimodal data\ncomplexity, and the methodological fragility of causal experimentation. Recent\nadvances in large language models (LLMs) present an opportunity to rethink how\nurban causal analysis is conducted. This Perspective examines current urban\ncausal research by analyzing taxonomies that categorize research topics, data\nsources, and methodological approaches to identify structural gaps. We then\nintroduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four\ndistinct modular agents responsible for hypothesis generation, data\nengineering, experiment design and execution, and results interpretation with\npolicy recommendations. We propose evaluation criteria for rigor and\ntransparency and reflect on implications for human-AI collaboration, equity,\nand accountability. We call for a new research agenda that embraces\nAI-augmented workflows not as replacements for human expertise but as tools to\nbroaden participation, improve reproducibility, and unlock more inclusive forms\nof urban causal reasoning."}
{"id": "2504.12347", "pdf": "https://arxiv.org/pdf/2504.12347", "abs": "https://arxiv.org/abs/2504.12347", "authors": ["Mika Setälä", "Pieta Sikström", "Ville Heilala", "Tommi Kärkkäinen"], "title": "Mathematical Capabilities of Large Language Models in Finnish Matriculation Examination", "categories": ["cs.CL", "cs.AI", "cs.CY", "K.3; I.2"], "comment": null, "summary": "Large language models (LLMs) have shown increasing promise in educational\nsettings, yet their mathematical reasoning has been considered evolving. This\nstudy evaluates the mathematical capabilities of various LLMs using the Finnish\nmatriculation examination, a high-stakes digital test for upper secondary\neducation. Initial tests yielded moderate performance corresponding to\nmid-range grades, but later evaluations demonstrated substantial improvements\nas the language models evolved. Remarkably, some models achieved near-perfect\nor perfect scores, matching top student performance and qualifying for\nuniversity admission. Our findings highlight the rapid advances in the\nmathematical proficiency of LLMs and illustrate their potential to also support\neducational assessments at scale."}
{"id": "2504.12350", "pdf": "https://arxiv.org/pdf/2504.12350", "abs": "https://arxiv.org/abs/2504.12350", "authors": ["Jing Wang", "Jeremy C Weiss"], "title": "A Large-Language Model Framework for Relative Timeline Extraction from PubMed Case Reports", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Timing of clinical events is central to characterization of patient\ntrajectories, enabling analyses such as process tracing, forecasting, and\ncausal reasoning. However, structured electronic health records capture few\ndata elements critical to these tasks, while clinical reports lack temporal\nlocalization of events in structured form. We present a system that transforms\ncase reports into textual time series-structured pairs of textual events and\ntimestamps. We contrast manual and large language model (LLM) annotations\n(n=320 and n=390 respectively) of ten randomly-sampled PubMed open-access\n(PMOA) case reports (N=152,974) and assess inter-LLM agreement (n=3,103; N=93).\nWe find that the LLM models have moderate event recall(O1-preview: 0.80) but\nhigh temporal concordance among identified events (O1-preview: 0.95). By\nestablishing the task, annotation, and assessment systems, and by demonstrating\nhigh concordance, this work may serve as a benchmark for leveraging the PMOA\ncorpus for temporal analytics."}
{"id": "2504.12355", "pdf": "https://arxiv.org/pdf/2504.12355", "abs": "https://arxiv.org/abs/2504.12355", "authors": ["Muhammad Ahmad", "Muhammad Waqas", "ldar Batyrshin", "Grigori Sidorov"], "title": "Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies."}
{"id": "2504.12357", "pdf": "https://arxiv.org/pdf/2504.12357", "abs": "https://arxiv.org/abs/2504.12357", "authors": ["Reece Adamson", "Erin Song"], "title": "Replicating ReLM Results: Validating Large Language Models with ReLM", "categories": ["cs.CL"], "comment": null, "summary": "Validating Large Language Models with ReLM explores the application of formal\nlanguages to evaluate and control Large Language Models (LLMs) for\nmemorization, bias, and zero-shot performance. Current approaches for\nevaluating these types behavior are often slow, imprecise, costly, or introduce\nbiases of their own, but are necessary due to the importance of this behavior\nwhen productionizing LLMs. This project reproduces key results from the\noriginal ReLM paper and expounds on the approach and applications with an\nemphasis on the relevance to the field of systems for machine learning."}
{"id": "2504.12360", "pdf": "https://arxiv.org/pdf/2504.12360", "abs": "https://arxiv.org/abs/2504.12360", "authors": ["Mieczysław A. Kłopotek", "Sławomir T. Wierzchoń", "Bartłomiej Starosta", "Dariusz Czerski", "Piotr Borkowski"], "title": "A Method for Handling Negative Similarities in Explainable Graph Spectral Clustering of Text Documents -- Extended Version", "categories": ["cs.CL", "cs.AI"], "comment": "1 figure, 17 pages, this is an extended version of a paper accepted\n  for the 25th International Conference on Computational Science (ICCS), 7-9\n  July 2025", "summary": "This paper investigates the problem of Graph Spectral Clustering with\nnegative similarities, resulting from document embeddings different from the\ntraditional Term Vector Space (like doc2vec, GloVe, etc.). Solutions for\ncombinatorial Laplacians and normalized Laplacians are discussed. An\nexperimental investigation shows the advantages and disadvantages of 6\ndifferent solutions proposed in the literature and in this research. The\nresearch demonstrates that GloVe embeddings frequently cause failures of\nnormalized Laplacian based GSC due to negative similarities. Furthermore,\napplication of methods curing similarity negativity leads to accuracy\nimprovement for both combinatorial and normalized Laplacian based GSC. It also\nleads to applicability for GloVe embeddings of explanation methods developed\noriginally bythe authors for Term Vector Space embeddings."}
{"id": "2504.12427", "pdf": "https://arxiv.org/pdf/2504.12427", "abs": "https://arxiv.org/abs/2504.12427", "authors": ["Nikhil Kandpal", "Colin Raffel"], "title": "Position: The Most Expensive Part of an LLM should be its Training Data", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "8 pages, 3 figures", "summary": "Training a state-of-the-art Large Language Model (LLM) is an increasingly\nexpensive endeavor due to growing computational, hardware, energy, and\nengineering demands. Yet, an often-overlooked (and seldom paid) expense is the\nhuman labor behind these models' training data. Every LLM is built on an\nunfathomable amount of human effort: trillions of carefully written words\nsourced from books, academic papers, codebases, social media, and more. This\nposition paper aims to assign a monetary value to this labor and argues that\nthe most expensive part of producing an LLM should be the compensation provided\nto training data producers for their work. To support this position, we study\n64 LLMs released between 2016 and 2024, estimating what it would cost to pay\npeople to produce their training datasets from scratch. Even under highly\nconservative estimates of wage rates, the costs of these models' training\ndatasets are 10-1000 times larger than the costs to train the models\nthemselves, representing a significant financial liability for LLM providers.\nIn the face of the massive gap between the value of training data and the lack\nof compensation for its creation, we highlight and discuss research directions\nthat could enable fairer practices in the future."}
{"id": "2504.12459", "pdf": "https://arxiv.org/pdf/2504.12459", "abs": "https://arxiv.org/abs/2504.12459", "authors": ["Jack Merullo", "Noah A. Smith", "Sarah Wiegreffe", "Yanai Elazar"], "title": "On Linear Representations and Pretraining Data Frequency in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025", "summary": "Pretraining data has a direct impact on the behaviors and quality of language\nmodels (LMs), but we only understand the most basic principles of this\nrelationship. While most work focuses on pretraining data's effect on\ndownstream task behavior, we investigate its relationship to LM\nrepresentations. Previous work has discovered that, in language models, some\nconcepts are encoded `linearly' in the representations, but what factors cause\nthese representations to form? We study the connection between pretraining data\nfrequency and models' linear representations of factual relations. We find\nevidence that the formation of linear representations is strongly connected to\npretraining term frequencies; specifically for subject-relation-object fact\ntriplets, both subject-object co-occurrence frequency and in-context learning\naccuracy for the relation are highly correlated with linear representations.\nThis is the case across all phases of pretraining. In OLMo-7B and GPT-J, we\ndiscover that a linear representation consistently (but not exclusively) forms\nwhen the subjects and objects within a relation co-occur at least 1k and 2k\ntimes, respectively, regardless of when these occurrences happen during\npretraining. Finally, we train a regression model on measurements of linear\nrepresentation quality in fully-trained LMs that can predict how often a term\nwas seen in pretraining. Our model achieves low error even on inputs from a\ndifferent model with a different pretraining dataset, providing a new method\nfor estimating properties of the otherwise-unknown training data of closed-data\nmodels. We conclude that the strength of linear representations in LMs contains\nsignal about the models' pretraining corpora that may provide new avenues for\ncontrolling and improving model behavior: particularly, manipulating the\nmodels' training data to meet specific frequency thresholds."}
{"id": "2504.12466", "pdf": "https://arxiv.org/pdf/2504.12466", "abs": "https://arxiv.org/abs/2504.12466", "authors": ["Cal Blanco", "Gavin Dsouza", "Hugo Lin", "Chelsey Rush"], "title": "SLURG: Investigating the Feasibility of Generating Synthetic Online Fallacious Discourse", "categories": ["cs.CL"], "comment": "15 pages, 11 figures", "summary": "In our paper we explore the definition, and extrapolation of fallacies as\nthey pertain to the automatic detection of manipulation on social media. In\nparticular we explore how these logical fallacies might appear in the real\nworld i.e internet forums. We discovered a prevalence of misinformation /\nmisguided intention in discussion boards specifically centered around the\nUkrainian Russian Conflict which serves to narrow the domain of our task.\nAlthough automatic fallacy detection has gained attention recently, most\ndatasets use unregulated fallacy taxonomies or are limited to formal linguistic\ndomains like political debates or news reports. Online discourse, however,\noften features non-standardized and diverse language not captured in these\ndomains. We present Shady Linguistic Utterance Replication-Generation (SLURG)\nto address these limitations, exploring the feasibility of generating synthetic\nfallacious forum-style comments using large language models (LLMs),\nspecifically DeepHermes-3-Mistral-24B. Our findings indicate that LLMs can\nreplicate the syntactic patterns of real data} and that high-quality few-shot\nprompts enhance LLMs' ability to mimic the vocabulary diversity of online\nforums."}
{"id": "2504.12474", "pdf": "https://arxiv.org/pdf/2504.12474", "abs": "https://arxiv.org/abs/2504.12474", "authors": ["Azadeh Beiranvand", "Seyed Mehdi Vahidipour"], "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 3 figures", "summary": "Text-attributed graphs (TAGs) present unique challenges in representation\nlearning by requiring models to capture both the semantic richness of\nnode-associated texts and the structural dependencies of the graph. While graph\nneural networks (GNNs) excel at modeling topological information, they lack the\ncapacity to process unstructured text. Conversely, large language models (LLMs)\nare proficient in text understanding but are typically unaware of graph\nstructure. In this work, we propose BiGTex (Bidirectional Graph Text), a novel\narchitecture that tightly integrates GNNs and LLMs through stacked Graph-Text\nFusion Units. Each unit allows for mutual attention between textual and\nstructural representations, enabling information to flow in both directions,\ntext influencing structure and structure guiding textual interpretation. The\nproposed architecture is trained using parameter-efficient fine-tuning (LoRA),\nkeeping the LLM frozen while adapting to task-specific signals. Extensive\nexperiments on five benchmark datasets demonstrate that BiGTex achieves\nstate-of-the-art performance in node classification and generalizes effectively\nto link prediction. An ablation study further highlights the importance of soft\nprompting and bi-directional attention in the model's success."}
{"id": "2504.12491", "pdf": "https://arxiv.org/pdf/2504.12491", "abs": "https://arxiv.org/abs/2504.12491", "authors": ["Hansi Zeng", "Kai Hui", "Honglei Zhuang", "Zhen Qin", "Zhenrui Yue", "Hamed Zamani", "Dana Alon"], "title": "Can Pre-training Indicators Reliably Predict Fine-tuning Outcomes of LLMs?", "categories": ["cs.CL"], "comment": null, "summary": "While metrics available during pre-training, such as perplexity, correlate\nwell with model performance at scaling-laws studies, their predictive\ncapacities at a fixed model size remain unclear, hindering effective model\nselection and development. To address this gap, we formulate the task of\nselecting pre-training checkpoints to maximize downstream fine-tuning\nperformance as a pairwise classification problem: predicting which of two LLMs,\ndiffering in their pre-training, will perform better after supervised\nfine-tuning (SFT). We construct a dataset using 50 1B parameter LLM variants\nwith systematically varied pre-training configurations, e.g., objectives or\ndata, and evaluate them on diverse downstream tasks after SFT. We first conduct\na study and demonstrate that the conventional perplexity is a misleading\nindicator. As such, we introduce novel unsupervised and supervised proxy\nmetrics derived from pre-training that successfully reduce the relative\nperformance prediction error rate by over 50%. Despite the inherent complexity\nof this task, we demonstrate the practical utility of our proposed proxies in\nspecific scenarios, paving the way for more efficient design of pre-training\nschemes optimized for various downstream tasks."}
{"id": "2504.12494", "pdf": "https://arxiv.org/pdf/2504.12494", "abs": "https://arxiv.org/abs/2504.12494", "authors": ["Jianlin Shi", "Qiwei Gan", "Elizabeth Hanchrow", "Annie Bowles", "John Stanley", "Adam P. Bress", "Jordana B. Cohen", "Patrick R. Alba"], "title": "Accelerating Clinical NLP at Scale with a Hybrid Framework with Reduced GPU Demands: A Case Study in Dementia Identification", "categories": ["cs.CL"], "comment": "This manuscript has been submitted to AMIA 2025 annual symposium\n  (https://amia.org/education-events/amia-2025-annual-symposium)", "summary": "Clinical natural language processing (NLP) is increasingly in demand in both\nclinical research and operational practice. However, most of the\nstate-of-the-art solutions are transformers-based and require high\ncomputational resources, limiting their accessibility. We propose a hybrid NLP\nframework that integrates rule-based filtering, a Support Vector Machine (SVM)\nclassifier, and a BERT-based model to improve efficiency while maintaining\naccuracy. We applied this framework in a dementia identification case study\ninvolving 4.9 million veterans with incident hypertension, analyzing 2.1\nbillion clinical notes. At the patient level, our method achieved a precision\nof 0.90, a recall of 0.84, and an F1-score of 0.87. Additionally, this NLP\napproach identified over three times as many dementia cases as structured data\nmethods. All processing was completed in approximately two weeks using a single\nmachine with dual A40 GPUs. This study demonstrates the feasibility of hybrid\nNLP solutions for large-scale clinical text analysis, making state-of-the-art\nmethods more accessible to healthcare organizations with limited computational\nresources."}
{"id": "2504.12495", "pdf": "https://arxiv.org/pdf/2504.12495", "abs": "https://arxiv.org/abs/2504.12495", "authors": ["Sireesh Gururaja", "Nupoor Gandhi", "Jeremiah Milbauer", "Emma Strubell"], "title": "Beyond Text: Characterizing Domain Expert Needs in Document Research", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Working with documents is a key part of almost any knowledge work, from\ncontextualizing research in a literature review to reviewing legal precedent.\nRecently, as their capabilities have expanded, primarily text-based NLP systems\nhave often been billed as able to assist or even automate this kind of work.\nBut to what extent are these systems able to model these tasks as experts\nconceptualize and perform them now? In this study, we interview sixteen domain\nexperts across two domains to understand their processes of document research,\nand compare it to the current state of NLP systems. We find that our\nparticipants processes are idiosyncratic, iterative, and rely extensively on\nthe social context of a document in addition its content; existing approaches\nin NLP and adjacent fields that explicitly center the document as an object,\nrather than as merely a container for text, tend to better reflect our\nparticipants' priorities, though they are often less accessible outside their\nresearch communities. We call on the NLP community to more carefully consider\nthe role of the document in building useful tools that are accessible,\npersonalizable, iterative, and socially aware."}
{"id": "2504.12516", "pdf": "https://arxiv.org/pdf/2504.12516", "abs": "https://arxiv.org/abs/2504.12516", "authors": ["Jason Wei", "Zhiqing Sun", "Spencer Papay", "Scott McKinney", "Jeffrey Han", "Isa Fulford", "Hyung Won Chung", "Alex Tachard Passos", "William Fedus", "Amelia Glaese"], "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents", "categories": ["cs.CL"], "comment": null, "summary": "We present BrowseComp, a simple yet challenging benchmark for measuring the\nability for agents to browse the web. BrowseComp comprises 1,266 questions that\nrequire persistently navigating the internet in search of hard-to-find,\nentangled information. Despite the difficulty of the questions, BrowseComp is\nsimple and easy-to-use, as predicted answers are short and easily verifiable\nagainst reference answers. BrowseComp for browsing agents can be seen as\nanalogous to how programming competitions are an incomplete but useful\nbenchmark for coding agents. While BrowseComp sidesteps challenges of a true\nuser query distribution, like generating long answers or resolving ambiguity,\nit measures the important core capability of exercising persistence and\ncreativity in finding information. BrowseComp can be found at\nhttps://github.com/openai/simple-evals."}
{"id": "2504.12522", "pdf": "https://arxiv.org/pdf/2504.12522", "abs": "https://arxiv.org/abs/2504.12522", "authors": ["Alexander Shypula", "Shuo Li", "Botong Zhang", "Vishakh Padmakumar", "Kayo Yin", "Osbert Bastani"], "title": "Evaluating the Diversity and Quality of LLM Generated Content", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025 Third Workshop on Deep Learning for Code", "summary": "Recent work suggests that preference-tuning techniques--including\nReinforcement Learning from Human Preferences (RLHF) methods like PPO and GRPO,\nas well as alternatives like DPO--reduce diversity, creating a dilemma given\nthat such models are widely deployed in applications requiring diverse outputs.\nTo address this, we introduce a framework for measuring effective semantic\ndiversity--diversity among outputs that meet quality thresholds--which better\nreflects the practical utility of large language models (LLMs). Using\nopen-ended tasks that require no human intervention, we find counterintuitive\nresults: although preference-tuned models--especially those trained via\nRL--exhibit reduced lexical and syntactic diversity, they produce greater\neffective semantic diversity than SFT or base models, not from increasing\ndiversity among high-quality outputs, but from generating more high-quality\noutputs overall. We discover that preference tuning reduces syntactic diversity\nwhile preserving semantic diversity--revealing a distinction between diversity\nin form and diversity in content that traditional metrics often overlook. Our\nanalysis further shows that smaller models are consistently more\nparameter-efficient at generating unique content within a fixed sampling\nbudget, offering insights into the relationship between model scaling and\ndiversity. These findings have important implications for applications that\nrequire diverse yet high-quality outputs, from creative assistance to synthetic\ndata generation."}
{"id": "2504.12523", "pdf": "https://arxiv.org/pdf/2504.12523", "abs": "https://arxiv.org/abs/2504.12523", "authors": ["Aochong Oliver Li", "Tanya Goyal"], "title": "Memorization vs. Reasoning: Updating LLMs with New Knowledge", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 3 figures", "summary": "Large language models (LLMs) encode vast amounts of pre-trained knowledge in\ntheir parameters, but updating them as real-world information evolves remains a\nchallenge. Existing methodologies and benchmarks primarily target entity\nsubstitutions, failing to capture the full breadth of complex real-world\ndynamics. In this paper, we introduce Knowledge Update Playground (KUP), an\nautomatic pipeline for simulating realistic knowledge updates reflected in an\nevidence corpora. KUP's evaluation framework includes direct and indirect\nprobes to both test memorization of updated facts and reasoning over them, for\nany update learning methods. Next, we present a lightweight method called\nmemory conditioned training (MCT), which conditions tokens in the update corpus\non self-generated \"memory\" tokens during training. Our strategy encourages LLMs\nto surface and reason over newly memorized knowledge at inference. Our results\non two strong LLMs show that (1) KUP benchmark is highly challenging, with the\nbest CPT models achieving $<2\\%$ in indirect probing setting (reasoning) and\n(2) MCT training significantly outperforms prior continued pre-training (CPT)\nbaselines, improving direct probing (memorization) results by up to $25.4\\%$."}
{"id": "2504.12549", "pdf": "https://arxiv.org/pdf/2504.12549", "abs": "https://arxiv.org/abs/2504.12549", "authors": ["Iris Ma", "Ian Domingo", "Alberto Krone-Martins", "Pierre Baldi", "Cristina V. Lopes"], "title": "Memorization: A Close Look at Books", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "To what extent can entire books be extracted from LLMs? Using the Llama 3 70B\nfamily of models, and the \"prefix-prompting\" extraction technique, we were able\nto auto-regressively reconstruct, with a very high level of similarity, one\nentire book (Alice's Adventures in Wonderland) from just the first 500 tokens.\nWe were also able to obtain high extraction rates on several other books,\npiece-wise. However, these successes do not extend uniformly to all books. We\nshow that extraction rates of books correlate with book popularity and thus,\nlikely duplication in the training data.\n  We also confirm the undoing of mitigations in the instruction-tuned Llama\n3.1, following recent work (Nasr et al., 2025). We further find that this\nundoing comes from changes to only a tiny fraction of weights concentrated\nprimarily in the lower transformer blocks. Our results provide evidence of the\nlimits of current regurgitation mitigation strategies and introduce a framework\nfor studying how fine-tuning affects the retrieval of verbatim memorization in\naligned LLMs."}
{"id": "2504.12553", "pdf": "https://arxiv.org/pdf/2504.12553", "abs": "https://arxiv.org/abs/2504.12553", "authors": ["Zahra Pourbahman", "Fatemeh Rajabi", "Mohammadhossein Sadeghi", "Omid Ghahroodi", "Somaye Bakhshaei", "Arash Amini", "Reza Kazemi", "Mahdieh Soleymani Baghshah"], "title": "ELAB: Extensive LLM Alignment Benchmark in Persian Language", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a comprehensive evaluation framework for aligning Persian\nLarge Language Models (LLMs) with critical ethical dimensions, including\nsafety, fairness, and social norms. It addresses the gaps in existing LLM\nevaluation frameworks by adapting them to Persian linguistic and cultural\ncontexts. This benchmark creates three types of Persian-language benchmarks:\n(i) translated data, (ii) new data generated synthetically, and (iii) new\nnaturally collected data. We translate Anthropic Red Teaming data, AdvBench,\nHarmBench, and DecodingTrust into Persian. Furthermore, we create\nProhibiBench-fa, SafeBench-fa, FairBench-fa, and SocialBench-fa as new datasets\nto address harmful and prohibited content in indigenous culture. Moreover, we\ncollect extensive dataset as GuardBench-fa to consider Persian cultural norms.\nBy combining these datasets, our work establishes a unified framework for\nevaluating Persian LLMs, offering a new approach to culturally grounded\nalignment evaluation. A systematic evaluation of Persian LLMs is performed\nacross the three alignment aspects: safety (avoiding harmful content), fairness\n(mitigating biases), and social norms (adhering to culturally accepted\nbehaviors). We present a publicly available leaderboard that benchmarks Persian\nLLMs with respect to safety, fairness, and social norms at:\nhttps://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation."}
{"id": "2504.12560", "pdf": "https://arxiv.org/pdf/2504.12560", "abs": "https://arxiv.org/abs/2504.12560", "authors": ["Elahe Khatibi", "Ziyu Wang", "Amir M. Rahmani"], "title": "CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly enhanced large\nlanguage models (LLMs) in knowledge-intensive tasks by incorporating external\nknowledge retrieval. However, existing RAG frameworks primarily rely on\nsemantic similarity and correlation-driven retrieval, limiting their ability to\ndistinguish true causal relationships from spurious associations. This results\nin responses that may be factually grounded but fail to establish\ncause-and-effect mechanisms, leading to incomplete or misleading insights. To\naddress this issue, we introduce Causal Dynamic Feedback for Adaptive\nRetrieval-Augmented Generation (CDF-RAG), a framework designed to improve\ncausal consistency, factual accuracy, and explainability in generative\nreasoning. CDF-RAG iteratively refines queries, retrieves structured causal\ngraphs, and enables multi-hop causal reasoning across interconnected knowledge\nsources. Additionally, it validates responses against causal pathways, ensuring\nlogically coherent and factually grounded outputs. We evaluate CDF-RAG on four\ndiverse datasets, demonstrating its ability to improve response accuracy and\ncausal correctness over existing RAG-based methods. Our code is publicly\navailable at https://github.com/ elakhatibi/CDF-RAG."}
{"id": "2504.12563", "pdf": "https://arxiv.org/pdf/2504.12563", "abs": "https://arxiv.org/abs/2504.12563", "authors": ["Haris Riaz", "Sourav Bhabesh", "Vinayak Arannil", "Miguel Ballesteros", "Graham Horwood"], "title": "MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "33 pages, 17 figures. Preprint", "summary": "Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data\ngenerated using larger Language models. Questions remain about leveraging\nsynthetic data for other use cases, such as adapting LLMs to specific domains.\nA key limitation of synthetic data is low diversity, which negatively impacts\nits downstream applicability for improving other models. To address this, we\npropose MetaSynth, a method for generating synthetic data that enhances\ndiversity through meta-prompting, where a language model orchestrates multiple\n\"expert\" LLM agents to collaboratively generate data. Using only 25 million\ntokens of synthetic data generated with MetaSynth, we successfully adapt a\nwell-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and\nBiomedicine-without compromising the capabilities of the resulting model in\ngeneral tasks. In addition, we evaluate the diversity of our synthetic data\nusing seven automated metrics, and find that it approaches the diversity of LLM\npre-training corpora.\n  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms\nthe base LLM, showing improvements of up to 4.08% in Finance and 13.75% in\nBiomedicine. The same model shows degraded performance when trained on data\ngenerated using a template prompt, even when the template includes prior\ngenerations and varying In-Context exemplars of real data. Our findings suggest\nthat a few million tokens of diverse synthetic data without mixing any real\ndata, is sufficient for effective domain adaptation when using MetaSynth."}
{"id": "2504.12585", "pdf": "https://arxiv.org/pdf/2504.12585", "abs": "https://arxiv.org/abs/2504.12585", "authors": ["Liyi Zhang", "Veniamin Veselovsky", "R. Thomas McCoy", "Thomas L. Griffiths"], "title": "Identifying and Mitigating the Influence of the Prior Distribution in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "16 pages, 5 figures", "summary": "Large language models (LLMs) sometimes fail to respond appropriately to\ndeterministic tasks -- such as counting or forming acronyms -- because the\nimplicit prior distribution they have learned over sequences of tokens\ninfluences their responses. In this work, we show that, in at least some cases,\nLLMs actually compute the information needed to perform these tasks correctly,\nand we identify some interventions that can allow them to access this\ninformation to improve their performance. First, we show that simply prompting\nthe language model to not rely on its prior knowledge leads to dramatic\nimprovements in prior-dominated tasks. We then use mechanistic interpretability\ntechniques to localize the prior within the LLM and manipulate the extent to\nwhich that prior influences its responses. Specifically, we show that it is\npossible to identify layers of the underlying neural network that correlate\nwith the prior probability of a response and that lightweight finetuning of\nthese layers with basic prompts on prior-dominated tasks achieves high\nperformance on held-out answers. These results suggest that the information\nrequired to produce a correct response is contained within the representations\nof the problems formed by the models. Furthermore, we show that this finetuning\nis significantly more effective for prior-dominated tasks, and that the error\nafter finetuning is no longer correlated with the prior. Our results suggest\nthat it may be possible to define effective methods for manipulating the extent\nto which LLMs rely upon their priors in solving problems, potentially\nincreasing their performance in settings where LLMs hallucinate for reasons\nrelated to the prior probability of token sequences."}
{"id": "2504.12597", "pdf": "https://arxiv.org/pdf/2504.12597", "abs": "https://arxiv.org/abs/2504.12597", "authors": ["Liangyu Xu", "Yingxiu Zhao", "Jingyun Wang", "Yingyao Wang", "Bu Pi", "Chen Wang", "Mingliang Zhang", "Jihao Gu", "Xiang Li", "Xiaoyong Zhu", "Jun Song", "Bo Zheng"], "title": "GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning", "categories": ["cs.CL"], "comment": "10 pages, 8 figures", "summary": "Geometry problem-solving (GPS), a challenging task requiring both visual\ncomprehension and symbolic reasoning, effectively measures the reasoning\ncapabilities of multimodal large language models (MLLMs). Humans exhibit strong\nreasoning ability in this task through accurate identification and adaptive\napplication of geometric principles within visual contexts. However, existing\nbenchmarks fail to jointly assess both dimensions of the human-like geometric\nreasoning mechanism in MLLMs, remaining a critical gap in assessing their\nability to tackle GPS. To this end, we introduce GeoSense, the first\ncomprehensive bilingual benchmark designed to systematically evaluate the\ngeometric reasoning abilities of MLLMs through the lens of geometric\nprinciples. GeoSense features a five-level hierarchical framework of geometric\nprinciples spanning plane and solid geometry, an intricately annotated dataset\nof 1,789 problems, and an innovative evaluation strategy. Through extensive\nexperiments on GeoSense with various open-source and closed-source MLLMs, we\nobserve that Gemini-2.0-pro-flash performs best, achieving an overall score of\n$65.3$. Our in-depth analysis reveals that the identification and application\nof geometric principles remain a bottleneck for leading MLLMs, jointly\nhindering their reasoning abilities. These findings underscore GeoSense's\npotential to guide future advancements in MLLMs' geometric reasoning\ncapabilities, paving the way for more robust and human-like reasoning in\nartificial intelligence."}
{"id": "2504.12633", "pdf": "https://arxiv.org/pdf/2504.12633", "abs": "https://arxiv.org/abs/2504.12633", "authors": ["Younghun Lee", "Dan Goldwasser"], "title": "Towards Characterizing Subjectivity of Individuals through Modeling Value Conflicts and Trade-offs", "categories": ["cs.CL"], "comment": "8 pages", "summary": "Large Language Models (LLMs) not only have solved complex reasoning problems\nbut also exhibit remarkable performance in tasks that require subjective\ndecision making. Existing studies suggest that LLM generations can be\nsubjectively grounded to some extent, yet exploring whether LLMs can account\nfor individual-level subjectivity has not been sufficiently studied. In this\npaper, we characterize subjectivity of individuals on social media and infer\ntheir moral judgments using LLMs. We propose a framework, SOLAR (Subjective\nGround with Value Abstraction), that observes value conflicts and trade-offs in\nthe user-generated texts to better represent subjective ground of individuals.\nEmpirical results show that our framework improves overall inference results as\nwell as performance on controversial situations. Additionally, we qualitatively\nshow that SOLAR provides explanations about individuals' value preferences,\nwhich can further account for their judgments."}
{"id": "2504.12637", "pdf": "https://arxiv.org/pdf/2504.12637", "abs": "https://arxiv.org/abs/2504.12637", "authors": ["Linda He", "Jue Wang", "Maurice Weber", "Shang Zhu", "Ben Athiwaratkun", "Ce Zhang"], "title": "Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation", "categories": ["cs.CL", "cs.AI"], "comment": "26 pages, 5 figures", "summary": "Large Language Models (LLMs) struggle with long-context reasoning, not only\ndue to the quadratic scaling of computational complexity with sequence length\nbut also because of the scarcity and expense of annotating long-context data.\nThere has been barely any open-source work that systematically ablates\nlong-context data, nor is there any openly available instruction tuning dataset\nwith contexts surpassing 100K tokens. To bridge this gap, we introduce a novel\npost-training synthetic data generation strategy designed to efficiently extend\nthe context window of LLMs while preserving their general task performance. Our\napproach scalably extends to arbitrarily long context lengths, unconstrained by\nthe length of available real-world data, which effectively addresses the\nscarcity of raw long-context data. Through a step-by-step rotary position\nembedding (RoPE) scaling training strategy, we demonstrate that our model, with\na context length of up to 1M tokens, performs well on the RULER benchmark and\nInfiniteBench and maintains robust performance on general language tasks."}
{"id": "2504.12663", "pdf": "https://arxiv.org/pdf/2504.12663", "abs": "https://arxiv.org/abs/2504.12663", "authors": ["Xiaotian Zhang", "Ruizhe Chen", "Yang Feng", "Zuozhu Liu"], "title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning language models with human preferences presents significant\nchallenges, particularly in achieving personalization without incurring\nexcessive computational costs. Existing methods rely on reward signals and\nadditional annotated data, limiting their scalability and adaptability to\ndiverse human values. To address these challenges, we introduce Persona-judge,\na novel discriminative paradigm that enables training-free personalized\nalignment with unseen preferences. Instead of optimizing policy parameters\nthrough external reward feedback, Persona-judge leverages the intrinsic\npreference judgment capabilities of the model. Specifically, a draft model\ngenerates candidate tokens conditioned on a given preference, while a judge\nmodel, embodying another preference, cross-validates the predicted tokens\nwhether to be accepted. Experimental results demonstrate that Persona-judge,\nusing the inherent preference evaluation mechanisms of the model, offers a\nscalable and computationally efficient solution to personalized alignment,\npaving the way for more adaptive customized alignment."}
{"id": "2504.12673", "pdf": "https://arxiv.org/pdf/2504.12673", "abs": "https://arxiv.org/abs/2504.12673", "authors": ["Singon Kim", "Gunho Jung", "Seong-Whan Lee"], "title": "ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Abstractive compression utilizes smaller langauge models to condense\nquery-relevant context, reducing computational costs in retrieval-augmented\ngeneration (RAG). However,retrieved documents often include information that is\neither irrelevant to answering the query or misleading due to factual incorrect\ncontent, despite having high relevance scores. This behavior indicates that\nabstractive compressors are more likely to omit important information essential\nfor the correct answer, especially in long contexts where attention dispersion\noccurs. To address this issue, we categorize retrieved documents in a more\nfine-grained manner and propose Abstractive Compression Robust against Noise\n(ACoRN), which introduces two novel training steps. First, we use offline data\naugmentation on the training dataset to enhance compressor robustness against\ntwo distinct types of retrieval noise. Second, since the language modelbased\ncompressor cannot fully utilize information from multiple retrieved documents\nand exhibits positional bias, we perform finetuning to generate summaries\ncentered around key information that directly supports the correct answer. Our\nexperiments demonstrate that T5-large, trained with ACoRN as a compressor,\nimproves EM and F1 scores while preserving the answer string, which could serve\nas direct evidence. ACoRN excels on datasets with many accuracy-reducing\ndocuments, making it highly useful in real-world scenarios."}
{"id": "2504.12681", "pdf": "https://arxiv.org/pdf/2504.12681", "abs": "https://arxiv.org/abs/2504.12681", "authors": ["Kun-Woo Kim", "Ji-Hoon Park", "Ju-Min Han", "Seong-Whan Lee"], "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCNN 2025", "summary": "Large Language Models (LLMs) trained on extensive datasets often learn\nsensitive information, which raises significant social and legal concerns under\nprinciples such as the \"Right to be forgotten.\" Retraining entire models from\nscratch to remove undesired information is both costly and impractical.\nFurthermore, existing single-domain unlearning methods fail to address\nmulti-domain scenarios, where knowledge is interwoven across domains such as\nprivacy and copyright, creating overlapping representations that lead to\nexcessive knowledge removal or degraded performance. To tackle these issues, we\npropose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain\nunlearning framework. GRAIL leverages gradient information from multiple\ndomains to precisely distinguish the unlearning scope from the retention scope,\nand applies an adaptive parameter-wise localization strategy to selectively\nremove targeted knowledge while preserving critical parameters for each domain.\nExperimental results on unlearning benchmarks show that GRAIL achieves\nunlearning success on par with the existing approaches, while also\ndemonstrating up to 17% stronger knowledge retention success compared to the\nprevious state-of-art method. Our findings establish a new paradigm for\neffectively managing and regulating sensitive information in large-scale\npre-trained language models."}
{"id": "2504.12687", "pdf": "https://arxiv.org/pdf/2504.12687", "abs": "https://arxiv.org/abs/2504.12687", "authors": ["Weijie Lv", "Xuan Xia", "Sheng-Jun Huang"], "title": "Data-efficient LLM Fine-tuning for Code Generation", "categories": ["cs.CL"], "comment": "arXiv admin note: text overlap with arXiv:2408.02193", "summary": "Large language models (LLMs) have demonstrated significant potential in code\ngeneration tasks. However, there remains a performance gap between open-source\nand closed-source models. To address this gap, existing approaches typically\ngenerate large amounts of synthetic data for fine-tuning, which often leads to\ninefficient training. In this work, we propose a data selection strategy in\norder to improve the effectiveness and efficiency of training for code-based\nLLMs. By prioritizing data complexity and ensuring that the sampled subset\naligns with the distribution of the original dataset, our sampling strategy\neffectively selects high-quality data. Additionally, we optimize the\ntokenization process through a \"dynamic pack\" technique, which minimizes\npadding tokens and reduces computational resource consumption. Experimental\nresults show that when training on 40% of the OSS-Instruct dataset, the\nDeepSeek-Coder-Base-6.7B model achieves an average performance of 66.9%,\nsurpassing the 66.1% performance with the full dataset. Moreover, training time\nis reduced from 47 minutes to 34 minutes, and the peak GPU memory decreases\nfrom 61.47 GB to 42.72 GB during a single epoch. Similar improvements are\nobserved with the CodeLlama-Python-7B model on the Evol-Instruct dataset. By\noptimizing both data selection and tokenization, our approach not only improves\nmodel performance but also improves training efficiency."}
{"id": "2504.12691", "pdf": "https://arxiv.org/pdf/2504.12691", "abs": "https://arxiv.org/abs/2504.12691", "authors": ["Yiyou Sun", "Yu Gai", "Lijie Chen", "Abhilasha Ravichander", "Yejin Choi", "Dawn Song"], "title": "Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) frequently generate hallucinations-content that\ndeviates from factual accuracy or provided context-posing challenges for\ndiagnosis due to the complex interplay of underlying causes. This paper\nintroduces a subsequence association framework to systematically trace and\nunderstand hallucinations. Our key insight is that hallucinations arise when\ndominant hallucinatory associations outweigh faithful ones. Through theoretical\nand empirical analyses, we demonstrate that decoder-only transformers\neffectively function as subsequence embedding models, with linear layers\nencoding input-output associations. We propose a tracing algorithm that\nidentifies causal subsequences by analyzing hallucination probabilities across\nrandomized input contexts. Experiments show our method outperforms standard\nattribution techniques in identifying hallucination causes and aligns with\nevidence from the model's training corpus. This work provides a unified\nperspective on hallucinations and a robust framework for their tracing and\nanalysis."}
{"id": "2504.12723", "pdf": "https://arxiv.org/pdf/2504.12723", "abs": "https://arxiv.org/abs/2504.12723", "authors": ["James Hale", "Sushrita Rakshit", "Kushal Chawla", "Jeanne M. Brett", "Jonathan Gratch"], "title": "KODIS: A Multicultural Dispute Resolution Dialogue Corpus", "categories": ["cs.CL"], "comment": null, "summary": "We present KODIS, a dyadic dispute resolution corpus containing thousands of\ndialogues from over 75 countries. Motivated by a theoretical model of culture\nand conflict, participants engage in a typical customer service dispute\ndesigned by experts to evoke strong emotions and conflict. The corpus contains\na rich set of dispositional, process, and outcome measures. The initial\nanalysis supports theories of how anger expressions lead to escalatory spirals\nand highlights cultural differences in emotional expression. We make this\ncorpus and data collection framework available to the community."}
{"id": "2504.12734", "pdf": "https://arxiv.org/pdf/2504.12734", "abs": "https://arxiv.org/abs/2504.12734", "authors": ["Yongrui Chen", "Junhao He", "Linbo Fu", "Shenyu Zhang", "Rihui Jin", "Xinbang Dai", "Jiaqi Li", "Dehai Min", "Nan Hu", "Yuxin Zhang", "Guilin Qi", "Yi Huang", "Tongtong Wu"], "title": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions (NLQs) by using structured sources such as tables, databases, and\nknowledge graphs in a unified way. Existing USKR methods either rely on\nemploying task-specific strategies or custom-defined representations, which\nstruggle to leverage the knowledge transfer between different SKR tasks or\nalign with the prior of LLMs, thereby limiting their performance. This paper\nproposes a novel USKR framework named \\textsc{Pandora}, which takes advantage\nof \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge\nrepresentation for alignment with LLM pre-training. It employs an LLM to\ngenerate textual reasoning steps and executable Python code for each question.\nDemonstrations are drawn from a memory of training examples that cover various\nSKR tasks, facilitating knowledge transfer. Extensive experiments on four\nbenchmarks involving three SKR tasks demonstrate that \\textsc{Pandora}\noutperforms existing unified frameworks and competes effectively with\ntask-specific methods."}
{"id": "2504.12737", "pdf": "https://arxiv.org/pdf/2504.12737", "abs": "https://arxiv.org/abs/2504.12737", "authors": ["Chenghao Fan", "Zhenyi Lu", "Jie Tian"], "title": "Chinese-Vicuna: A Chinese Instruction-following Llama-based Model", "categories": ["cs.CL"], "comment": "Chinese-Vicuna Technique Report", "summary": "Chinese-Vicuna is an open-source, resource-efficient language model designed\nto bridge the gap in Chinese instruction-following capabilities by fine-tuning\nMeta's LLaMA architecture using Low-Rank Adaptation (LoRA). Targeting\nlow-resource environments, it enables cost-effective deployment on consumer\nGPUs (e.g., RTX-2080Ti for 7B models) and supports domain-specific adaptation\nin fields like healthcare and law. By integrating hybrid datasets (BELLE and\nGuanaco) and 4-bit quantization (QLoRA), the model achieves competitive\nperformance in tasks such as translation, code generation, and domain-specific\nQ\\&A. The project provides a comprehensive toolkit for model conversion, CPU\ninference, and multi-turn dialogue interfaces, emphasizing accessibility for\nresearchers and developers. Evaluations indicate competitive performance across\nmedical tasks, multi-turn dialogue coherence, and real-time legal updates.\nChinese-Vicuna's modular design, open-source ecosystem, and community-driven\nenhancements position it as a versatile foundation for Chinese LLM\napplications."}
{"id": "2504.12767", "pdf": "https://arxiv.org/pdf/2504.12767", "abs": "https://arxiv.org/abs/2504.12767", "authors": ["Fatma Elsafoury", "David Hartmann"], "title": "Out of Sight Out of Mind, Out of Sight Out of Mind: Measuring Bias in Language Models Against Overlooked Marginalized Groups in Regional Contexts", "categories": ["cs.CL"], "comment": null, "summary": "We know that language models (LMs) form biases and stereotypes of minorities,\nleading to unfair treatments of members of these groups, thanks to research\nmainly in the US and the broader English-speaking world. As the negative\nbehavior of these models has severe consequences for society and individuals,\nindustry and academia are actively developing methods to reduce the bias in\nLMs. However, there are many under-represented groups and languages that have\nbeen overlooked so far. This includes marginalized groups that are specific to\nindividual countries and regions in the English speaking and Western world, but\ncrucially also almost all marginalized groups in the rest of the world. The UN\nestimates, that between 600 million to 1.2 billion people worldwide are members\nof marginalized groups and in need for special protection. If we want to\ndevelop inclusive LMs that work for everyone, we have to broaden our\nunderstanding to include overlooked marginalized groups and low-resource\nlanguages and dialects.\n  In this work, we contribute to this effort with the first study investigating\noffensive stereotyping bias in 23 LMs for 270 marginalized groups from Egypt,\nthe remaining 21 Arab countries, Germany, the UK, and the US. Additionally, we\ninvestigate the impact of low-resource languages and dialects on the study of\nbias in LMs, demonstrating the limitations of current bias metrics, as we\nmeasure significantly higher bias when using the Egyptian Arabic dialect versus\nModern Standard Arabic. Our results show, LMs indeed show higher bias against\nmany marginalized groups in comparison to dominant groups. However, this is not\nthe case for Arabic LMs, where the bias is high against both marginalized and\ndominant groups in relation to religion and ethnicity.\n  Our results also show higher intersectional bias against Non-binary, LGBTQIA+\nand Black women."}
{"id": "2504.12773", "pdf": "https://arxiv.org/pdf/2504.12773", "abs": "https://arxiv.org/abs/2504.12773", "authors": ["Yicheng Pan", "Zhenrong Zhang", "Pengfei Hu", "Jiefeng Ma", "Jun Du", "Jianshu Zhang", "Quan Liu", "Jianqing Gao", "Feng Ma"], "title": "Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have achieved\nremarkable progress in general domains and demonstrated promise in multimodal\nmathematical reasoning. However, applying MLLMs to geometry problem solving\n(GPS) remains challenging due to lack of accurate step-by-step solution data\nand severe hallucinations during reasoning. In this paper, we propose GeoGen, a\npipeline that can automatically generates step-wise reasoning paths for\ngeometry diagrams. By leveraging the precise symbolic reasoning,\n\\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To\nfurther enhance the logical reasoning ability of MLLMs, we train\n\\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated\nby GeoGen. Serving as a bridge between natural language and symbolic systems,\nGeoLogic enables symbolic tools to help verifying MLLM outputs, making the\nreasoning process more rigorous and alleviating hallucinations. Experimental\nresults show that our approach consistently improves the performance of MLLMs,\nachieving remarkable results on benchmarks for geometric reasoning tasks. This\nimprovement stems from our integration of the strengths of LLMs and symbolic\nsystems, which enables a more reliable and interpretable approach for the GPS\ntask. Codes are available at https://github.com/ycpNotFound/GeoGen."}
{"id": "2504.12805", "pdf": "https://arxiv.org/pdf/2504.12805", "abs": "https://arxiv.org/abs/2504.12805", "authors": ["Takaya Arita", "Wenxian Zheng", "Reiji Suzuki", "Fuminori Akiba"], "title": "Assesing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "30 pages, 13 figures, 1 table", "summary": "This study explored how large language models (LLMs) perform in two areas\nrelated to art: writing critiques of artworks and reasoning about mental states\n(Theory of Mind, or ToM) in art-related situations. For the critique generation\npart, we built a system that combines Noel Carroll's evaluative framework with\na broad selection of art criticism theories. The model was prompted to first\nwrite a full-length critique and then shorter, more coherent versions using a\nstep-by-step prompting process. These AI-generated critiques were then compared\nwith those written by human experts in a Turing test-style evaluation. In many\ncases, human subjects had difficulty telling which was which, and the results\nsuggest that LLMs can produce critiques that are not only plausible in style\nbut also rich in interpretation, as long as they are carefully guided. In the\nsecond part, we introduced new simple ToM tasks based on situations involving\ninterpretation, emotion, and moral tension, which can appear in the context of\nart. These go beyond standard false-belief tests and allow for more complex,\nsocially embedded forms of reasoning. We tested 41 recent LLMs and found that\ntheir performance varied across tasks and models. In particular, tasks that\ninvolved affective or ambiguous situations tended to reveal clearer\ndifferences. Taken together, these results help clarify how LLMs respond to\ncomplex interpretative challenges, revealing both their cognitive limitations\nand potential. While our findings do not directly contradict the so-called\nGenerative AI Paradox--the idea that LLMs can produce expert-like output\nwithout genuine understanding--they suggest that, depending on how LLMs are\ninstructed, such as through carefully designed prompts, these models may begin\nto show behaviors that resemble understanding more closely than we might\nassume."}
{"id": "2504.12816", "pdf": "https://arxiv.org/pdf/2504.12816", "abs": "https://arxiv.org/abs/2504.12816", "authors": ["Xue Wen Tan", "Stanley Kok"], "title": "SMARTe: Slot-based Method for Accountable Relational Triple extraction", "categories": ["cs.CL"], "comment": null, "summary": "Relational Triple Extraction (RTE) is a fundamental task in Natural Language\nProcessing (NLP). However, prior research has primarily focused on optimizing\nmodel performance, with limited efforts to understand the internal mechanisms\ndriving these models. Many existing methods rely on complex preprocessing to\ninduce specific interactions, often resulting in opaque systems that may not\nfully align with their theoretical foundations. To address these limitations,\nwe propose SMARTe: a Slot-based Method for Accountable Relational Triple\nextraction. SMARTe introduces intrinsic interpretability through a slot\nattention mechanism and frames the task as a set prediction problem. Slot\nattention consolidates relevant information into distinct slots, ensuring all\npredictions can be explicitly traced to learned slot representations and the\ntokens contributing to each predicted relational triple. While emphasizing\ninterpretability, SMARTe achieves performance comparable to state-of-the-art\nmodels. Evaluations on the NYT and WebNLG datasets demonstrate that adding\ninterpretability does not compromise performance. Furthermore, we conducted\nqualitative assessments to showcase the explanations provided by SMARTe, using\nattention heatmaps that map to their respective tokens. We conclude with a\ndiscussion of our findings and propose directions for future research."}
{"id": "2504.12845", "pdf": "https://arxiv.org/pdf/2504.12845", "abs": "https://arxiv.org/abs/2504.12845", "authors": ["Amey Hengle", "Prasoon Bajpai", "Soham Dan", "Tanmoy Chakraborty"], "title": "Can LLMs reason over extended multilingual contexts? Towards long-context evaluation beyond retrieval and haystacks", "categories": ["cs.CL"], "comment": "33 Pages in Total - 23 (Main Manuscript) + 10 (Appendix)", "summary": "Existing multilingual long-context benchmarks, often based on the popular\nneedle-in-a-haystack test, primarily evaluate a model's ability to locate\nspecific information buried within irrelevant texts. However, such a\nretrieval-centric approach is myopic and inherently limited, as successful\nrecall alone does not indicate a model's capacity to reason over extended\ncontexts. Moreover, these benchmarks are susceptible to data leakage,\nshort-circuiting, and risk making the evaluation a priori identifiable. To\naddress these limitations, we introduce MLRBench, a new synthetic benchmark for\nmultilingual long-context reasoning. Unlike existing benchmarks, MLRBench goes\nbeyond surface-level retrieval by including tasks that assess multi-hop\ninference, aggregation, and epistemic reasoning. Spanning seven languages,\nMLRBench is designed to be parallel, resistant to leakage, and scalable to\narbitrary context lengths. Our extensive experiments with an open-weight large\nlanguage model (LLM) reveal a pronounced gap between high- and low-resource\nlanguages, particularly for tasks requiring the model to aggregate multiple\nfacts or predict the absence of information. We also find that, in multilingual\nsettings, LLMs effectively utilize less than 30% of their claimed context\nlength. Although off-the-shelf Retrieval Augmented Generation helps alleviate\nthis to a certain extent, it does not solve the long-context problem. We\nopen-source MLRBench to enable future research in improved evaluation and\ntraining of multilingual LLMs."}
{"id": "2504.12882", "pdf": "https://arxiv.org/pdf/2504.12882", "abs": "https://arxiv.org/abs/2504.12882", "authors": ["Patrick Giedemann", "Pius von Däniken", "Jan Deriu", "Alvaro Rodrigo", "Anselmo Peñas", "Mark Cieliebak"], "title": "ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection in Videos", "categories": ["cs.CL"], "comment": null, "summary": "The growing influence of video content as a medium for communication and\nmisinformation underscores the urgent need for effective tools to analyze\nclaims in multilingual and multi-topic settings. Existing efforts in\nmisinformation detection largely focus on written text, leaving a significant\ngap in addressing the complexity of spoken text in video transcripts. We\nintroduce ViClaim, a dataset of 1,798 annotated video transcripts across three\nlanguages (English, German, Spanish) and six topics. Each sentence in the\ntranscripts is labeled with three claim-related categories: fact-check-worthy,\nfact-non-check-worthy, or opinion. We developed a custom annotation tool to\nfacilitate the highly complex annotation process. Experiments with\nstate-of-the-art multilingual language models demonstrate strong performance in\ncross-validation (macro F1 up to 0.896) but reveal challenges in generalization\nto unseen topics, particularly for distinct domains. Our findings highlight the\ncomplexity of claim detection in video transcripts. ViClaim offers a robust\nfoundation for advancing misinformation detection in video-based communication,\naddressing a critical gap in multimodal analysis."}
{"id": "2504.12891", "pdf": "https://arxiv.org/pdf/2504.12891", "abs": "https://arxiv.org/abs/2504.12891", "authors": ["Vicent Briva-Iglesias"], "title": "Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "The rapid evolution of artificial intelligence (AI) has introduced AI agents\nas a disruptive paradigm across various industries, yet their application in\nmachine translation (MT) remains underexplored. This paper describes and\nanalyses the potential of single- and multi-agent systems for MT, reflecting on\nhow they could enhance multilingual digital communication. While single-agent\nsystems are well-suited for simpler translation tasks, multi-agent systems,\nwhich involve multiple specialized AI agents collaborating in a structured\nmanner, may offer a promising solution for complex scenarios requiring high\naccuracy, domain-specific knowledge, and contextual awareness. To demonstrate\nthe feasibility of multi-agent workflows in MT, we are conducting a pilot study\nin legal MT. The study employs a multi-agent system involving four specialized\nAI agents for (i) translation, (ii) adequacy review, (iii) fluency review, and\n(iv) final editing. Our findings suggest that multi-agent systems may have the\npotential to significantly improve domain-adaptability and contextual\nawareness, with superior translation quality to traditional MT or single-agent\nsystems. This paper also sets the stage for future research into multi-agent\napplications in MT, integration into professional translation workflows, and\nshares a demo of the system analyzed in the paper."}
{"id": "2504.12898", "pdf": "https://arxiv.org/pdf/2504.12898", "abs": "https://arxiv.org/abs/2504.12898", "authors": ["Zhouhao Sun", "Xiao Ding", "Li Du", "Yunpeng Xu", "Yixuan Ma", "Yang Zhao", "Bing Qin", "Ting Liu"], "title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (IGCIDB) framework. This\nframework first utilizes an information gain-guided causal intervention method\nto automatically and autonomously balance the distribution of\ninstruction-tuning dataset. Subsequently, it employs a standard supervised\nfine-tuning process to train LLMs on the debiased dataset. Experimental results\nshow that IGCIDB can effectively debias LLM to improve its generalizability\nacross different tasks."}
{"id": "2504.12911", "pdf": "https://arxiv.org/pdf/2504.12911", "abs": "https://arxiv.org/abs/2504.12911", "authors": ["Chengyi Ju", "Weijie Shi", "Chengzhong Liu", "Jiaming Ji", "Jipeng Zhang", "Ruiyuan Zhang", "Jia Zhu", "Jiajie Xu", "Yaodong Yang", "Sirui Han", "Yike Guo"], "title": "Benchmarking Multi-National Value Alignment for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Do Large Language Models (LLMs) hold positions that conflict with your\ncountry's values? Occasionally they do! However, existing works primarily focus\non ethical reviews, failing to capture the diversity of national values, which\nencompass broader policy, legal, and moral considerations. Furthermore, current\nbenchmarks that rely on spectrum tests using manually designed questionnaires\nare not easily scalable.\n  To address these limitations, we introduce NaVAB, a comprehensive benchmark\nto evaluate the alignment of LLMs with the values of five major nations: China,\nthe United States, the United Kingdom, France, and Germany. NaVAB implements a\nnational value extraction pipeline to efficiently construct value assessment\ndatasets. Specifically, we propose a modeling procedure with instruction\ntagging to process raw data sources, a screening process to filter\nvalue-related topics and a generation process with a Conflict Reduction\nmechanism to filter non-conflicting values.We conduct extensive experiments on\nvarious LLMs across countries, and the results provide insights into assisting\nin the identification of misaligned scenarios. Moreover, we demonstrate that\nNaVAB can be combined with alignment techniques to effectively reduce value\nconcerns by aligning LLMs' values with the target country."}
{"id": "2504.12913", "pdf": "https://arxiv.org/pdf/2504.12913", "abs": "https://arxiv.org/abs/2504.12913", "authors": ["Fanyi Yang", "Jianfeng Liu", "Xin Zhang", "Haoyu Liu", "Xixin Cao", "Yuefeng Zhan", "Hao Sun", "Weiwei Deng", "Feng Sun", "Qi Zhang"], "title": "MAIN: Mutual Alignment Is Necessary for instruction tuning", "categories": ["cs.CL"], "comment": null, "summary": "Instruction tuning has enabled large language models (LLMs) to achieve\nremarkable performance, but its success heavily depends on the availability of\nlarge-scale, high-quality instruction-response pairs. However, current methods\nfor scaling up data generation often overlook a crucial aspect: the alignment\nbetween instructions and responses. We hypothesize that high-quality\ninstruction-response pairs are not defined by the individual quality of each\ncomponent, but by the extent of their alignment with each other. To address\nthis, we propose a Mutual Alignment Framework (MAIN) that ensures coherence\nbetween the instruction and response through mutual constraints. Experiments\ndemonstrate that models such as LLaMA and Mistral, fine-tuned within this\nframework, outperform traditional methods across multiple benchmarks. This\napproach underscores the critical role of instruction-response alignment in\nenabling scalable and high-quality instruction tuning for LLMs."}
{"id": "2504.12915", "pdf": "https://arxiv.org/pdf/2504.12915", "abs": "https://arxiv.org/abs/2504.12915", "authors": ["Ebrahim Norouzi", "Sven Hertling", "Harald Sack"], "title": "ConExion: Concept Extraction with Large Language Models", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "In this paper, an approach for concept extraction from documents using\npre-trained large language models (LLMs) is presented. Compared with\nconventional methods that extract keyphrases summarizing the important\ninformation discussed in a document, our approach tackles a more challenging\ntask of extracting all present concepts related to the specific domain, not\njust the important ones. Through comprehensive evaluations of two widely used\nbenchmark datasets, we demonstrate that our method improves the F1 score\ncompared to state-of-the-art techniques. Additionally, we explore the potential\nof using prompts within these models for unsupervised concept extraction. The\nextracted concepts are intended to support domain coverage evaluation of\nontologies and facilitate ontology learning, highlighting the effectiveness of\nLLMs in concept extraction tasks. Our source code and datasets are publicly\navailable at https://github.com/ISE-FIZKarlsruhe/concept_extraction."}
{"id": "2504.12951", "pdf": "https://arxiv.org/pdf/2504.12951", "abs": "https://arxiv.org/abs/2504.12951", "authors": ["Nearchos Potamitis", "Akhil Arora"], "title": "Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages, 16 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2405.06691", "summary": "Recent advancements in large language models (LLMs) have catalyzed the\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\nperformance in complex reasoning tasks across various domains. This surge has\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\nrecent focus has been on iterative reasoning strategies that refine outputs\nthrough self-evaluation and verbalized feedback. However, these strategies\nrequire additional computational complexity to enable models to recognize and\ncorrect their mistakes, leading to a significant increase in their cost. In\nthis work, we introduce the concept of ``retrials without feedback'', an\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\nanswers. Unlike conventional iterative refinement methods, our method does not\nrequire explicit self-reflection or verbalized feedback, simplifying the\nrefinement process. Our findings indicate that simpler retrial-based approaches\noften outperform more sophisticated reasoning frameworks, suggesting that the\nbenefits of complex methods may not always justify their computational costs.\nBy challenging the prevailing assumption that more intricate reasoning\nstrategies inherently lead to better performance, our work offers new insights\ninto how simpler, more efficient approaches can achieve optimal results. So,\nare retrials all you need?"}
{"id": "2504.12972", "pdf": "https://arxiv.org/pdf/2504.12972", "abs": "https://arxiv.org/abs/2504.12972", "authors": ["Adithya Pratapa", "Teruko Mitamura"], "title": "Estimating Optimal Context Length for Hybrid Retrieval-augmented Multi-document Summarization", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in long-context reasoning abilities of language models led to\ninteresting applications in large-scale multi-document summarization. However,\nprior work has shown that these long-context models are not effective at their\nclaimed context windows. To this end, retrieval-augmented systems provide an\nefficient and effective alternative. However, their performance can be highly\nsensitive to the choice of retrieval context length. In this work, we present a\nhybrid method that combines retrieval-augmented systems with long-context\nwindows supported by recent language models. Our method first estimates the\noptimal retrieval length as a function of the retriever, summarizer, and\ndataset. On a randomly sampled subset of the dataset, we use a panel of LLMs to\ngenerate a pool of silver references. We use these silver references to\nestimate the optimal context length for a given RAG system configuration. Our\nresults on the multi-document summarization task showcase the effectiveness of\nour method across model classes and sizes. We compare against length estimates\nfrom strong long-context benchmarks such as RULER and HELMET. Our analysis also\nhighlights the effectiveness of our estimation method for very long-context LMs\nand its generalization to new classes of LMs."}
{"id": "2504.12976", "pdf": "https://arxiv.org/pdf/2504.12976", "abs": "https://arxiv.org/abs/2504.12976", "authors": ["Charles O'Neill", "Tirthankar Ghosal", "Roberta Răileanu", "Mike Walmsley", "Thang Bui", "Kevin Schawinski", "Ioana Ciucă"], "title": "Sparks of Science: Hypothesis Generation Using Structured Paper Data", "categories": ["cs.CL"], "comment": "9 pages, 2 figures. Comments welcome", "summary": "Generating novel and creative scientific hypotheses is a cornerstone in\nachieving Artificial General Intelligence. Large language and reasoning models\nhave the potential to aid in the systematic creation, selection, and validation\nof scientifically informed hypotheses. However, current foundation models often\nstruggle to produce scientific ideas that are both novel and feasible. One\nreason is the lack of a dedicated dataset that frames Scientific Hypothesis\nGeneration (SHG) as a Natural Language Generation (NLG) task. In this paper, we\nintroduce HypoGen, the first dataset of approximately 5500 structured\nproblem-hypothesis pairs extracted from top-tier computer science conferences\nstructured with a Bit-Flip-Spark schema, where the Bit is the conventional\nassumption, the Spark is the key insight or conceptual leap, and the Flip is\nthe resulting counterproposal. HypoGen uniquely integrates an explicit\nChain-of-Reasoning component that reflects the intellectual process from Bit to\nFlip. We demonstrate that framing hypothesis generation as conditional language\nmodelling, with the model fine-tuned on Bit-Flip-Spark and the\nChain-of-Reasoning (and where, at inference, we only provide the Bit), leads to\nimprovements in the overall quality of the hypotheses. Our evaluation employs\nautomated metrics and LLM judge rankings for overall quality assessment. We\nshow that by fine-tuning on our HypoGen dataset we improve the novelty,\nfeasibility, and overall quality of the generated hypotheses. The HypoGen\ndataset is publicly available at\nhuggingface.co/datasets/UniverseTBD/hypogen-dr1."}
{"id": "2504.12982", "pdf": "https://arxiv.org/pdf/2504.12982", "abs": "https://arxiv.org/abs/2504.12982", "authors": ["Jiatai Wang", "Zhiwei Xu", "Di Jin", "Xuewen Yang", "Tao Li"], "title": "Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The proliferation of large language models (LLMs) has significantly advanced\ninformation retrieval systems, particularly in response generation (RG).\nUnfortunately, LLMs often face knowledge conflicts between internal memory and\nretrievaled external information, arising from misinformation, biases, or\noutdated knowledge. These conflicts undermine response reliability and\nintroduce uncertainty in decision-making. In this work, we analyze how LLMs\nnavigate knowledge conflicts from an information-theoretic perspective and\nreveal that when conflicting and supplementary information exhibit significant\ndifferences, LLMs confidently resolve their preferences. However, when the\ndistinction is ambiguous, LLMs experience heightened uncertainty. Based on this\ninsight, we propose Swin-VIB, a novel framework that integrates a pipeline of\nvariational information bottleneck models into adaptive augmentation of\nretrieved information and guiding LLM preference in response generation.\nExtensive experiments on single-choice, open-ended question-answering (QA), and\nretrieval augmented generation (RAG) validate our theoretical findings and\ndemonstrate the efficacy of Swin-VIB. Notably, our method improves\nsingle-choice task accuracy by at least 7.54\\% over competitive baselines."}
{"id": "2504.12996", "pdf": "https://arxiv.org/pdf/2504.12996", "abs": "https://arxiv.org/abs/2504.12996", "authors": ["Saransh Agrawal", "Kuan-Hao Huang"], "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, In Proceedings of The 19th International Workshop on\n  Semantic Evaluation (SemEval), 2025", "summary": "Large language models (LLMs) frequently memorize sensitive information during\ntraining, posing risks when deploying publicly accessible models. Current\nmachine unlearning methods struggle to selectively remove specific data\nassociations without degrading overall model capabilities. This paper presents\nour solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a\ntwo-stage methodology that combines causal mediation analysis with\nlayer-specific optimization. Through systematic causal tracing experiments on\nOLMo architectures (1B and 7B parameters), we identify the critical role of the\nfirst few transformer layers (layers 0-5) in storing subject-attribute\nassociations within MLP modules. Building on this insight, we develop a\nconstrained optimization approach that freezes upper layers while applying a\nnovel joint loss function to lower layers-simultaneously maximizing forget set\nloss via output token cross-entropy penalties and minimizing retain set\ndeviation through adaptive regularization. Our method achieves 2nd place in the\n1B model track, demonstrating strong task performance while maintaining 88% of\nbaseline MMLU accuracy. These results establish causal-informed layer\noptimization as a promising paradigm for efficient, precise unlearning in LLMs,\noffering a significant step forward in addressing data privacy concerns in AI\nsystems."}
{"id": "2504.13023", "pdf": "https://arxiv.org/pdf/2504.13023", "abs": "https://arxiv.org/abs/2504.13023", "authors": ["Sangwook Kim", "Soonyoung Lee", "Jongseong Jang"], "title": "ChatEXAONEPath: An Expert-level Multimodal Large Language Model for Histopathology Using Whole Slide Images", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Recent studies have made significant progress in developing large language\nmodels (LLMs) in the medical domain, which can answer expert-level questions\nand demonstrate the potential to assist clinicians in real-world clinical\nscenarios. Studies have also witnessed the importance of integrating various\nmodalities with the existing LLMs for a better understanding of complex\nclinical contexts, which are innately multi-faceted by nature. Although studies\nhave demonstrated the ability of multimodal LLMs in histopathology to answer\nquestions from given images, they lack in understanding of thorough clinical\ncontext due to the patch-level data with limited information from public\ndatasets. Thus, developing WSI-level MLLMs is significant in terms of the\nscalability and applicability of MLLMs in histopathology. In this study, we\nintroduce an expert-level MLLM for histopathology using WSIs, dubbed as\nChatEXAONEPath. We present a retrieval-based data generation pipeline using\n10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas\n(TCGA). We also showcase an AI-based evaluation protocol for a comprehensive\nunderstanding of the medical context from given multimodal information and\nevaluate generated answers compared to the original histopathology reports. We\ndemonstrate the ability of diagnosing the given histopathology images using\nChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and\nreports. Our proposed model can understand pan-cancer WSIs and clinical context\nfrom various cancer types. We argue that our proposed model has the potential\nto assist clinicians by comprehensively understanding complex morphology of\nWSIs for cancer diagnosis through the integration of multiple modalities."}
{"id": "2504.13054", "pdf": "https://arxiv.org/pdf/2504.13054", "abs": "https://arxiv.org/abs/2504.13054", "authors": ["Yichao Feng", "Shuai Zhao", "Yueqiu Li", "Luwei Xiao", "Xiaobao Wu", "Anh Tuan Luu"], "title": "Aspect-Based Summarization with Self-Aspect Retrieval Enhanced Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aspect-based summarization aims to generate summaries tailored to specific\naspects, addressing the resource constraints and limited generalizability of\ntraditional summarization approaches. Recently, large language models have\nshown promise in this task without the need for training. However, they rely\nexcessively on prompt engineering and face token limits and hallucination\nchallenges, especially with in-context learning. To address these challenges,\nin this paper, we propose a novel framework for aspect-based summarization:\nSelf-Aspect Retrieval Enhanced Summary Generation. Rather than relying solely\non in-context learning, given an aspect, we employ an embedding-driven\nretrieval mechanism to identify its relevant text segments. This approach\nextracts the pertinent content while avoiding unnecessary details, thereby\nmitigating the challenge of token limits. Moreover, our framework optimizes\ntoken usage by deleting unrelated parts of the text and ensuring that the model\ngenerates output strictly based on the given aspect. With extensive experiments\non benchmark datasets, we demonstrate that our framework not only achieves\nsuperior performance but also effectively mitigates the token limitation\nproblem."}
{"id": "2504.13068", "pdf": "https://arxiv.org/pdf/2504.13068", "abs": "https://arxiv.org/abs/2504.13068", "authors": ["Sudesh Ramesh Bhagat", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study explores the relationship between deep learning (DL) model\naccuracy and expert agreement in the classification of crash narratives. We\nevaluate five DL models -- including BERT variants, the Universal Sentence\nEncoder (USE), and a zero-shot classifier -- against expert-labeled data and\nnarrative text. The analysis is further extended to four large language models\n(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive\ntrend: models with higher technical accuracy often exhibit lower agreement with\ndomain experts, whereas LLMs demonstrate greater expert alignment despite\nrelatively lower accuracy scores. To quantify and interpret model-expert\nagreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and\nSHAP-based explainability techniques. Findings indicate that expert-aligned\nmodels tend to rely more on contextual and temporal language cues, rather than\nlocation-specific keywords. These results underscore that accuracy alone is\ninsufficient for evaluating models in safety-critical NLP applications. We\nadvocate for incorporating expert agreement as a complementary metric in model\nevaluation frameworks and highlight the promise of LLMs as interpretable,\nscalable tools for crash analysis pipelines."}
{"id": "2504.13079", "pdf": "https://arxiv.org/pdf/2504.13079", "abs": "https://arxiv.org/abs/2504.13079", "authors": ["Han Wang", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "Retrieval-Augmented Generation with Conflicting Evidence", "categories": ["cs.CL", "cs.AI"], "comment": "Our data and code is available at:\n  https://github.com/HanNight/RAMDocs", "summary": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation."}
{"id": "2504.13125", "pdf": "https://arxiv.org/pdf/2504.13125", "abs": "https://arxiv.org/abs/2504.13125", "authors": ["Varun Rao", "Youran Sun", "Mahendra Kumar", "Tejas Mutneja", "Agastya Mukherjee", "Haizhao Yang"], "title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications."}
{"id": "2504.13134", "pdf": "https://arxiv.org/pdf/2504.13134", "abs": "https://arxiv.org/abs/2504.13134", "authors": ["Anamika Lochab", "Ruqi Zhang"], "title": "Energy-Based Reward Models for Robust Language Model Alignment", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM."}
{"id": "2504.13139", "pdf": "https://arxiv.org/pdf/2504.13139", "abs": "https://arxiv.org/abs/2504.13139", "authors": ["João Loula", "Benjamin LeBrun", "Li Du", "Ben Lipkin", "Clemente Pasti", "Gabriel Grand", "Tianyu Liu", "Yahya Emara", "Marjorie Freedman", "Jason Eisner", "Ryan Cotterel", "Vikash Mansinghka", "Alexander K. Lew", "Tim Vieira", "Timothy J. O'Donnell"], "title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "34 pages, 4 figures", "summary": "A wide range of LM applications require generating text that conforms to\nsyntactic or semantic constraints. Imposing such constraints can be naturally\nframed as probabilistic conditioning, but exact generation from the resulting\ndistribution -- which can differ substantially from the LM's base distribution\n-- is generally intractable. In this work, we develop an architecture for\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\nframework allows us to flexibly incorporate domain- and problem-specific\nconstraints at inference time, and efficiently reallocate computational\nresources in light of new information during the course of generation. By\ncomparing to a number of alternatives and ablations on four challenging domains\n-- Python code generation for data science, text-to-SQL, goal inference, and\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\nallows small open-source language models to outperform models over 8x larger,\nas well as closed-source, fine-tuned ones. In support of the probabilistic\nperspective, we show that these performance improvements are driven by better\napproximation to the posterior distribution. Our system builds on the framework\nof Lew et al. (2023) and integrates with its language model probabilistic\nprogramming language, giving users a simple, programmable way to apply SMC to a\nbroad variety of controlled generation problems."}
{"id": "2504.13161", "pdf": "https://arxiv.org/pdf/2504.13161", "abs": "https://arxiv.org/abs/2504.13161", "authors": ["Shizhe Diao", "Yu Yang", "Yonggan Fu", "Xin Dong", "Dan Su", "Markus Kliegl", "Zijia Chen", "Peter Belcak", "Yoshi Suhara", "Hongxu Yin", "Mostofa Patwary", "Yingyan", "Lin", "Jan Kautz", "Pavlo Molchanov"], "title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training", "categories": ["cs.CL"], "comment": "20 pages, 9 figures", "summary": "Pre-training datasets are typically collected from web content and lack\ninherent domain divisions. For instance, widely used datasets like Common Crawl\ndo not include explicit domain labels, while manually curating labeled datasets\nsuch as The Pile is labor-intensive. Consequently, identifying an optimal\npre-training data mixture remains a challenging problem, despite its\nsignificant benefits for pre-training performance. To address these challenges,\nwe propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an\nautomated framework that discovers, evaluates, and refines data mixtures in a\npre-training setting. Specifically, CLIMB embeds and clusters large-scale\ndatasets in a semantic space and then iteratively searches for optimal mixtures\nusing a smaller proxy model and a predictor. When continuously trained on 400B\ntokens with this mixture, our 1B model exceeds the state-of-the-art\nLlama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific\ndomain (e.g., Social Sciences) yields a 5% improvement over random sampling.\nFinally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20\nclusters as a research playground, and ClimbMix, a compact yet powerful\n400-billion-token dataset designed for efficient pre-training that delivers\nsuperior performance under an equal token budget. We analyze the final data\nmixture, elucidating the characteristics of an optimal data mixture. Our data\nis available at: https://research.nvidia.com/labs/lpr/climb/"}
{"id": "2504.12309", "pdf": "https://arxiv.org/pdf/2504.12309", "abs": "https://arxiv.org/abs/2504.12309", "authors": ["Yi-De Lin", "Guan-Ze Liao"], "title": "Large Language Model-Based Knowledge Graph System Construction for Sustainable Development Goals: An AI-Based Speculative Design Perspective", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "From 2000 to 2015, the UN's Millennium Development Goals guided global\npriorities. The subsequent Sustainable Development Goals (SDGs) adopted a more\ndynamic approach, with annual indicator updates. As 2030 nears and progress\nlags, innovative acceleration strategies are critical. This study develops an\nAI-powered knowledge graph system to analyze SDG interconnections, discover\npotential new goals, and visualize them online. Using official SDG texts,\nElsevier's keyword dataset, and 1,127 TED Talk transcripts (2020-2023), a pilot\non 269 talks from 2023 applies AI-speculative design, large language models,\nand retrieval-augmented generation. Key findings include: (1) Heatmap analysis\nreveals strong associations between Goal 10 and Goal 16, and minimal coverage\nof Goal 6. (2) In the knowledge graph, simulated dialogue over time reveals new\ncentral nodes, showing how richer data supports divergent thinking and goal\nclarity. (3) Six potential new goals are proposed, centered on equity,\nresilience, and technology-driven inclusion. This speculative-AI framework\noffers fresh insights for policymakers and lays groundwork for future\nmultimodal and cross-system SDG applications."}
{"id": "2504.12319", "pdf": "https://arxiv.org/pdf/2504.12319", "abs": "https://arxiv.org/abs/2504.12319", "authors": ["Duc Tuyen TA", "Wajdi Ben Saad", "Ji Young Oh"], "title": "Specialized text classification: an approach to classifying Open Banking transactions", "categories": ["cs.IR", "cs.AI", "cs.CL", "q-fin.CP"], "comment": null, "summary": "With the introduction of the PSD2 regulation in the EU which established the\nOpen Banking framework, a new window of opportunities has opened for banks and\nfintechs to explore and enrich Bank transaction descriptions with the aim of\nbuilding a better understanding of customer behavior, while using this\nunderstanding to prevent fraud, reduce risks and offer more competitive and\ntailored services.\n  And although the usage of natural language processing models and techniques\nhas seen an incredible progress in various applications and domains over the\npast few years, custom applications based on domain-specific text corpus remain\nunaddressed especially in the banking sector.\n  In this paper, we introduce a language-based Open Banking transaction\nclassification system with a focus on the french market and french language\ntext. The system encompasses data collection, labeling, preprocessing,\nmodeling, and evaluation stages. Unlike previous studies that focus on general\nclassification approaches, this system is specifically tailored to address the\nchallenges posed by training a language model with a specialized text corpus\n(Banking data in the French context). By incorporating language-specific\ntechniques and domain knowledge, the proposed system demonstrates enhanced\nperformance and efficiency compared to generic approaches."}
{"id": "2504.12408", "pdf": "https://arxiv.org/pdf/2504.12408", "abs": "https://arxiv.org/abs/2504.12408", "authors": ["Negar Arabzadeh", "Charles L. A . Clarke"], "title": "A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to automate relevance\njudgments for information retrieval (IR) tasks, often demonstrating agreement\nwith human labels that approaches inter-human agreement. To assess the\nrobustness and reliability of LLM-based relevance judgments, we systematically\ninvestigate impact of prompt sensitivity on the task. We collected prompts for\nrelevance assessment from 15 human experts and 15 LLMs across three tasks~ --\n~binary, graded, and pairwise~ -- ~yielding 90 prompts in total. After\nfiltering out unusable prompts from three humans and three LLMs, we employed\nthe remaining 72 prompts with three different LLMs as judges to label\ndocument/query pairs from two TREC Deep Learning Datasets (2020 and 2021). We\ncompare LLM-generated labels with TREC official human labels using Cohen's\n$\\kappa$ and pairwise agreement measures. In addition to investigating the\nimpact of prompt variations on agreement with human labels, we compare human-\nand LLM-generated prompts and analyze differences among different LLMs as\njudges. We also compare human- and LLM-generated prompts with the standard\nUMBRELA prompt used for relevance assessment by Bing and TREC 2024 Retrieval\nAugmented Generation (RAG) Track. To support future research in LLM-based\nevaluation, we release all data and prompts at\nhttps://github.com/Narabzad/prompt-sensitivity-relevance-judgements/."}
{"id": "2504.12477", "pdf": "https://arxiv.org/pdf/2504.12477", "abs": "https://arxiv.org/abs/2504.12477", "authors": ["George Fatouros", "Georgios Makridis", "George Kousiouris", "John Soldatos", "Anargyros Tsadimas", "Dimosthenis Kyriazis"], "title": "Towards Conversational AI for Human-Machine Collaborative MLOps", "categories": ["cs.AI", "cs.CL", "cs.HC", "68T50, 68T99, 68U35, 68N19", "I.2.1; H.5.2; D.2.11; I.2.7"], "comment": "8 pages, 5 figures", "summary": "This paper presents a Large Language Model (LLM) based conversational agent\nsystem designed to enhance human-machine collaboration in Machine Learning\nOperations (MLOps). We introduce the Swarm Agent, an extensible architecture\nthat integrates specialized agents to create and manage ML workflows through\nnatural language interactions. The system leverages a hierarchical, modular\ndesign incorporating a KubeFlow Pipelines (KFP) Agent for ML pipeline\norchestration, a MinIO Agent for data management, and a Retrieval-Augmented\nGeneration (RAG) Agent for domain-specific knowledge integration. Through\niterative reasoning loops and context-aware processing, the system enables\nusers with varying technical backgrounds to discover, execute, and monitor ML\npipelines; manage datasets and artifacts; and access relevant documentation,\nall via intuitive conversational interfaces. Our approach addresses the\naccessibility gap in complex MLOps platforms like Kubeflow, making advanced ML\ntools broadly accessible while maintaining the flexibility to extend to other\nplatforms. The paper describes the architecture, implementation details, and\ndemonstrates how this conversational MLOps assistant reduces complexity and\nlowers barriers to entry for users across diverse technical skill levels."}
{"id": "2504.12545", "pdf": "https://arxiv.org/pdf/2504.12545", "abs": "https://arxiv.org/abs/2504.12545", "authors": ["Benign John Ihugba", "Afsana Nasrin", "Ling Wu", "Lin Li", "Lijun Qian", "Xishuang Dong"], "title": "Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Mass-shooting events pose a significant challenge to public safety,\ngenerating large volumes of unstructured textual data that hinder effective\ninvestigations and the formulation of public policy. Despite the urgency, few\nprior studies have effectively automated the extraction of key information from\nthese events to support legal and investigative efforts. This paper presented\nthe first dataset designed for knowledge acquisition on mass-shooting events\nthrough the application of named entity recognition (NER) techniques. It\nfocuses on identifying key entities such as offenders, victims, locations, and\ncriminal instruments, that are vital for legal and investigative purposes. The\nNER process is powered by Large Language Models (LLMs) using few-shot\nprompting, facilitating the efficient extraction and organization of critical\ninformation from diverse sources, including news articles, police reports, and\nsocial media. Experimental results on real-world mass-shooting corpora\ndemonstrate that GPT-4o is the most effective model for mass-shooting NER,\nachieving the highest Micro Precision, Micro Recall, and Micro F1-scores.\nMeanwhile, o1-mini delivers competitive performance, making it a\nresource-efficient alternative for less complex NER tasks. It is also observed\nthat increasing the shot count enhances the performance of all models, but the\ngains are more substantial for GPT-4o and o1-mini, highlighting their superior\nadaptability to few-shot learning scenarios."}
{"id": "2504.12558", "pdf": "https://arxiv.org/pdf/2504.12558", "abs": "https://arxiv.org/abs/2504.12558", "authors": ["Negar Arabzadeh", "Charles L. A. Clarke"], "title": "Benchmarking LLM-based Relevance Judgment Methods", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in both academic and\nindustry settings to automate the evaluation of information seeking systems,\nparticularly by generating graded relevance judgments. Previous work on\nLLM-based relevance assessment has primarily focused on replicating graded\nhuman relevance judgments through various prompting strategies. However, there\nhas been limited exploration of alternative assessment methods or comprehensive\ncomparative studies. In this paper, we systematically compare multiple\nLLM-based relevance assessment methods, including binary relevance judgments,\ngraded relevance assessments, pairwise preference-based methods, and two\nnugget-based evaluation methods~--~document-agnostic and document-dependent. In\naddition to a traditional comparison based on system rankings using Kendall\ncorrelations, we also examine how well LLM judgments align with human\npreferences, as inferred from relevance grades. We conduct extensive\nexperiments on datasets from three TREC Deep Learning tracks 2019, 2020 and\n2021 as well as the ANTIQUE dataset, which focuses on non-factoid open-domain\nquestion answering. As part of our data release, we include relevance judgments\ngenerated by both an open-source (Llama3.2b) and a commercial (gpt-4o) model.\nOur goal is to \\textit{reproduce} various LLM-based relevance judgment methods\nto provide a comprehensive comparison. All code, data, and resources are\npublicly available in our GitHub Repository at\nhttps://github.com/Narabzad/llm-relevance-judgement-comparison."}
{"id": "2504.12562", "pdf": "https://arxiv.org/pdf/2504.12562", "abs": "https://arxiv.org/abs/2504.12562", "authors": ["Haidar Khan", "Hisham A. Alyahya", "Yazeed Alnumay", "M Saiful Bari", "Bülent Yener"], "title": "ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Evaluating the capabilities of Large Language Models (LLMs) has traditionally\nrelied on static benchmark datasets, human assessments, or model-based\nevaluations - methods that often suffer from overfitting, high costs, and\nbiases. ZeroSumEval is a novel competition-based evaluation protocol that\nleverages zero-sum games to assess LLMs with dynamic benchmarks that resist\nsaturation. ZeroSumEval encompasses a diverse suite of games, including\nsecurity challenges (PyJail), classic games (Chess, Liar's Dice, Poker),\nknowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These\ngames are designed to evaluate a range of AI capabilities such as strategic\nreasoning, planning, knowledge application, and creativity. Building upon\nrecent studies that highlight the effectiveness of game-based evaluations for\nLLMs, ZeroSumEval enhances these approaches by providing a standardized and\nextensible framework. To demonstrate this, we conduct extensive experiments\nwith >7000 simulations across 7 games and 13 models. Our results show that\nwhile frontier models from the GPT and Claude families can play common games\nand answer questions, they struggle to play games that require creating novel\nand challenging questions. We also observe that models cannot reliably\njailbreak each other and fail generally at tasks requiring creativity. We\nrelease our code at https://github.com/facebookresearch/ZeroSumEval."}
{"id": "2504.12579", "pdf": "https://arxiv.org/pdf/2504.12579", "abs": "https://arxiv.org/abs/2504.12579", "authors": ["Kaiyi Pang"], "title": "Provable Secure Steganography Based on Adaptive Dynamic Sampling", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "The security of private communication is increasingly at risk due to\nwidespread surveillance. Steganography, a technique for embedding secret\nmessages within innocuous carriers, enables covert communication over monitored\nchannels. Provably Secure Steganography (PSS) is state of the art for making\nstego carriers indistinguishable from normal ones by ensuring computational\nindistinguishability between stego and cover distributions. However, current\nPSS methods often require explicit access to the distribution of generative\nmodel for both sender and receiver, limiting their practicality in black box\nscenarios. In this paper, we propose a provably secure steganography scheme\nthat does not require access to explicit model distributions for both sender\nand receiver. Our method incorporates a dynamic sampling strategy, enabling\ngenerative models to embed secret messages within multiple sampling choices\nwithout disrupting the normal generation process of the model. Extensive\nevaluations of three real world datasets and three LLMs demonstrate that our\nblackbox method is comparable with existing white-box steganography methods in\nterms of efficiency and capacity while eliminating the degradation of\nsteganography in model generated outputs."}
{"id": "2504.12588", "pdf": "https://arxiv.org/pdf/2504.12588", "abs": "https://arxiv.org/abs/2504.12588", "authors": ["Liheng Ma", "Soumyasundar Pal", "Yingxue Zhang", "Philip H. S. Torr", "Mark Coates"], "title": "Simplifying Graph Transformers", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers have attained outstanding performance across various modalities,\nemploying scaled-dot-product (SDP) attention mechanisms. Researchers have\nattempted to migrate Transformers to graph learning, but most advanced Graph\nTransformers are designed with major architectural differences, either\nintegrating message-passing or incorporating sophisticated attention\nmechanisms. These complexities prevent the easy adoption of Transformer\ntraining advances. We propose three simple modifications to the plain\nTransformer to render it applicable to graphs without introducing major\narchitectural distortions. Specifically, we advocate for the use of (1)\nsimplified $L_2$ attention to measure the magnitude closeness of tokens; (2)\nadaptive root-mean-square normalization to preserve token magnitude\ninformation; and (3) a relative positional encoding bias with a shared encoder.\nSignificant performance gains across a variety of graph datasets justify the\neffectiveness of our proposed modifications. Furthermore, empirical evaluation\non the expressiveness benchmark reveals noteworthy realized expressiveness in\nthe graph isomorphism."}
{"id": "2504.12661", "pdf": "https://arxiv.org/pdf/2504.12661", "abs": "https://arxiv.org/abs/2504.12661", "authors": ["Menglan Chen", "Xianghe Pang", "Jingjing Dong", "WenHao Wang", "Yaxin Du", "Siheng Chen"], "title": "VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Aligning Vision-Language Models (VLMs) with safety standards is essential to\nmitigate risks arising from their multimodal complexity, where integrating\nvision and language unveils subtle threats beyond the reach of conventional\nsafeguards. Inspired by the insight that reasoning across modalities is key to\npreempting intricate vulnerabilities, we propose a novel direction for VLM\nsafety: multimodal reasoning-driven prompt rewriting. To this end, we introduce\nVLMGuard-R1, a proactive framework that refines user inputs through a\nreasoning-guided rewriter, dynamically interpreting text-image interactions to\ndeliver refined prompts that bolster safety across diverse VLM architectures\nwithout altering their core parameters. To achieve this, we devise a\nthree-stage reasoning pipeline to synthesize a dataset that trains the rewriter\nto infer subtle threats, enabling tailored, actionable responses over generic\nrefusals. Extensive experiments across three benchmarks with five VLMs reveal\nthat VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1\nachieves a remarkable 43.59\\% increase in average safety across five models on\nthe SIUO benchmark."}
{"id": "2504.12682", "pdf": "https://arxiv.org/pdf/2504.12682", "abs": "https://arxiv.org/abs/2504.12682", "authors": ["Arth Bohra", "Manvel Saroyan", "Danil Melkozerov", "Vahe Karufanyan", "Gabriel Maher", "Pascal Weinberger", "Artem Harutyunyan", "Giovanni Campagna"], "title": "WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Most recent web agent research has focused on navigation and transaction\ntasks, with little emphasis on extracting structured data at scale. We present\nWebLists, a benchmark of 200 data-extraction tasks across four common business\nand enterprise use-cases. Each task requires an agent to navigate to a webpage,\nconfigure it appropriately, and extract complete datasets with well-defined\nschemas. We show that both LLMs with search capabilities and SOTA web agents\nstruggle with these tasks, with a recall of 3% and 31%, respectively, despite\nhigher performance on question-answering tasks.\n  To address this challenge, we propose BardeenAgent, a novel framework that\nenables web agents to convert their execution into repeatable programs, and\nreplay them at scale across pages with similar structure. BardeenAgent is also\nthe first LLM agent to take advantage of the regular structure of HTML. In\nparticular BardeenAgent constructs a generalizable CSS selector to capture all\nrelevant items on the page, then fits the operations to extract the data.\n  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more\nthan doubling the performance of SOTA web agents, and reducing cost per output\nrow by 3x."}
{"id": "2504.12778", "pdf": "https://arxiv.org/pdf/2504.12778", "abs": "https://arxiv.org/abs/2504.12778", "authors": ["Yuxuan Zong", "Benjamin Piwowarski"], "title": "Towards Lossless Token Pruning in Late-Interaction Retrieval Models", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Accepted at SIGIR 2025 Full Paper Track", "summary": "Late interaction neural IR models like ColBERT offer a competitive\neffectiveness-efficiency trade-off across many benchmarks. However, they\nrequire a huge memory space to store the contextual representation for all the\ndocument tokens. Some works have proposed using either heuristics or\nstatistical-based techniques to prune tokens from each document. This however\ndoesn't guarantee that the removed tokens have no impact on the retrieval\nscore. Our work uses a principled approach to define how to prune tokens\nwithout impacting the score between a document and a query. We introduce three\nregularization losses, that induce a solution with high pruning ratios, as well\nas two pruning strategies. We study them experimentally (in and out-domain),\nshowing that we can preserve ColBERT's performance while using only 30\\% of the\ntokens."}
{"id": "2504.12867", "pdf": "https://arxiv.org/pdf/2504.12867", "abs": "https://arxiv.org/abs/2504.12867", "authors": ["Guanrou Yang", "Chen Yang", "Qian Chen", "Ziyang Ma", "Wenxi Chen", "Wen Wang", "Tianrui Wang", "Yifan Yang", "Zhikang Niu", "Wenrui Liu", "Fan Yu", "Zhihao Du", "Zhifu Gao", "ShiLiang Zhang", "Xie Chen"], "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and modality-of-thought (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released."}
{"id": "2504.12879", "pdf": "https://arxiv.org/pdf/2504.12879", "abs": "https://arxiv.org/abs/2504.12879", "authors": ["Grigory Kovalev", "Mikhail Tikhomirov", "Evgeny Kozhevnikov", "Max Kornilov", "Natalia Loukachevitch"], "title": "Building Russian Benchmark for Evaluation of Information Retrieval Models", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "We introduce RusBEIR, a comprehensive benchmark designed for zero-shot\nevaluation of information retrieval (IR) models in the Russian language.\nComprising 17 datasets from various domains, it integrates adapted, translated,\nand newly created datasets, enabling systematic comparison of lexical and\nneural models. Our study highlights the importance of preprocessing for lexical\nmodels in morphologically rich languages and confirms BM25 as a strong baseline\nfor full-document retrieval. Neural models, such as mE5-large and BGE-M3,\ndemonstrate superior performance on most datasets, but face challenges with\nlong-document retrieval due to input size constraints. RusBEIR offers a\nunified, open-source framework that promotes research in Russian-language\ninformation retrieval."}
{"id": "2504.12977", "pdf": "https://arxiv.org/pdf/2504.12977", "abs": "https://arxiv.org/abs/2504.12977", "authors": ["Maksim Vishnevskiy"], "title": "A Phenomenological Approach to Analyzing User Queries in IT Systems Using Heidegger's Fundamental Ontology", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.HC"], "comment": "12 pages, no figures", "summary": "This paper presents a novel research analytical IT system grounded in Martin\nHeidegger's Fundamental Ontology, distinguishing between beings (das Seiende)\nand Being (das Sein). The system employs two modally distinct, descriptively\ncomplete languages: a categorical language of beings for processing user inputs\nand an existential language of Being for internal analysis. These languages are\nbridged via a phenomenological reduction module, enabling the system to analyze\nuser queries (including questions, answers, and dialogues among IT\nspecialists), identify recursive and self-referential structures, and provide\nactionable insights in categorical terms. Unlike contemporary systems limited\nto categorical analysis, this approach leverages Heidegger's phenomenological\nexistential analysis to uncover deeper ontological patterns in query\nprocessing, aiding in resolving logical traps in complex interactions, such as\nmetaphor usage in IT contexts. The path to full realization involves\nformalizing the language of Being by a research team based on Heidegger's\nFundamental Ontology; given the existing completeness of the language of\nbeings, this reduces the system's computability to completeness, paving the way\nfor a universal query analysis tool. The paper presents the system's\narchitecture, operational principles, technical implementation, use\ncases--including a case based on real IT specialist dialogues--comparative\nevaluation with existing tools, and its advantages and limitations."}
{"id": "2504.13038", "pdf": "https://arxiv.org/pdf/2504.13038", "abs": "https://arxiv.org/abs/2504.13038", "authors": ["Leo Leppänen", "Lili Aunimo", "Arto Hellas", "Jukka K. Nurminen", "Linda Mannila"], "title": "How Large Language Models Are Changing MOOC Essay Answers: A Comparison of Pre- and Post-LLM Responses", "categories": ["cs.CY", "cs.CL", "K.3.1; I.2.7"], "comment": "10 pages, 4 figures", "summary": "The release of ChatGPT in late 2022 caused a flurry of activity and concern\nin the academic and educational communities. Some see the tool's ability to\ngenerate human-like text that passes at least cursory inspections for factual\naccuracy ``often enough'' a golden age of information retrieval and\ncomputer-assisted learning. Some, on the other hand, worry the tool may lead to\nunprecedented levels of academic dishonesty and cheating. In this work, we\nquantify some of the effects of the emergence of Large Language Models (LLMs)\non online education by analyzing a multi-year dataset of student essay\nresponses from a free university-level MOOC on AI ethics. Our dataset includes\nessays submitted both before and after ChatGPT's release. We find that the\nlaunch of ChatGPT coincided with significant changes in both the length and\nstyle of student essays, mirroring observations in other contexts such as\nacademic publishing. We also observe -- as expected based on related public\ndiscourse -- changes in prevalence of key content words related to AI and LLMs,\nbut not necessarily the general themes or topics discussed in the student\nessays as identified through (dynamic) topic modeling."}
{"id": "2504.13059", "pdf": "https://arxiv.org/pdf/2504.13059", "abs": "https://arxiv.org/abs/2504.13059", "authors": ["Yao Mu", "Tianxing Chen", "Zanxin Chen", "Shijia Peng", "Zhiqian Lan", "Zeyu Gao", "Zhixuan Liang", "Qiaojun Yu", "Yude Zou", "Mingkun Xu", "Lunkai Lin", "Zhiqiang Xie", "Mingyu Ding", "Ping Luo"], "title": "RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "CVPR 2025 Highlight. 22 pages. Project page:\n  https://robotwin-benchmark.github.io/", "summary": "In the rapidly advancing field of robotics, dual-arm coordination and complex\nobject manipulation are essential capabilities for developing advanced\nautonomous systems. However, the scarcity of diverse, high-quality\ndemonstration data and real-world-aligned evaluation benchmarks severely limits\nsuch development. To address this, we introduce RoboTwin, a generative digital\ntwin framework that uses 3D generative foundation models and large language\nmodels to produce diverse expert datasets and provide a real-world-aligned\nevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates\nvaried digital twins of objects from single 2D images, generating realistic and\ninteractive scenarios. It also introduces a spatial relation-aware code\ngeneration framework that combines object annotations with large language\nmodels to break down tasks, determine spatial constraints, and generate precise\nrobotic movement code. Our framework offers a comprehensive benchmark with both\nsimulated and real-world data, enabling standardized evaluation and better\nalignment between simulated training and real-world performance. We validated\nour approach using the open-source COBOT Magic Robot platform. Policies\npre-trained on RoboTwin-generated data and fine-tuned with limited real-world\nsamples demonstrate significant potential for enhancing dual-arm robotic\nmanipulation systems by improving success rates by over 70% for single-arm\ntasks and over 40% for dual-arm tasks compared to models trained solely on\nreal-world data."}
{"id": "2504.13085", "pdf": "https://arxiv.org/pdf/2504.13085", "abs": "https://arxiv.org/abs/2504.13085", "authors": ["Georgina Curto", "Svetlana Kiritchenko", "Muhammad Hammad Fahim Siddiqui", "Isar Nejadgholi", "Kathleen C. Fraser"], "title": "Tackling Social Bias against the Poor: A Dataset and Taxonomy on Aporophobia", "categories": ["cs.CY", "cs.CL"], "comment": "In Findings of the Association for Computational Linguistics: NAACL\n  2025", "summary": "Eradicating poverty is the first goal in the United Nations Sustainable\nDevelopment Goals. However, aporophobia -- the societal bias against people\nliving in poverty -- constitutes a major obstacle to designing, approving and\nimplementing poverty-mitigation policies. This work presents an initial step\ntowards operationalizing the concept of aporophobia to identify and track\nharmful beliefs and discriminative actions against poor people on social media.\nIn close collaboration with non-profits and governmental organizations, we\nconduct data collection and exploration. Then we manually annotate a corpus of\nEnglish tweets from five world regions for the presence of (1) direct\nexpressions of aporophobia, and (2) statements referring to or criticizing\naporophobic views or actions of others, to comprehensively characterize the\nsocial media discourse related to bias and discrimination against the poor.\nBased on the annotated data, we devise a taxonomy of categories of aporophobic\nattitudes and actions expressed through speech on social media. Finally, we\ntrain several classifiers and identify the main challenges for automatic\ndetection of aporophobia in social networks. This work paves the way towards\nidentifying, tracking, and mitigating aporophobic views on social media at\nscale."}
{"id": "2504.13120", "pdf": "https://arxiv.org/pdf/2504.13120", "abs": "https://arxiv.org/abs/2504.13120", "authors": ["Yongqian Peng", "Yuxi Ma", "Mengmeng Wang", "Yuxuan Wang", "Yizhou Wang", "Chi Zhang", "Yixin Zhu", "Zilong Zheng"], "title": "Probing and Inducing Combinational Creativity in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project page: https://ppyyqq.github.io/aicc/ The first two authors\n  contribute equally", "summary": "The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs."}
{"id": "2504.13128", "pdf": "https://arxiv.org/pdf/2504.13128", "abs": "https://arxiv.org/abs/2504.13128", "authors": ["Nandan Thakur", "Jimmy Lin", "Sam Havens", "Michael Carbin", "Omar Khattab", "Andrew Drozdov"], "title": "FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce FreshStack, a reusable framework for automatically building\ninformation retrieval (IR) evaluation benchmarks from community-asked questions\nand answers. FreshStack conducts the following steps: (1) automatic corpus\ncollection from code and technical documentation, (2) nugget generation from\ncommunity-asked questions and answers, and (3) nugget-level support, retrieving\ndocuments using a fusion of retrieval techniques and hybrid architectures. We\nuse FreshStack to build five datasets on fast-growing, recent, and niche topics\nto ensure the tasks are sufficiently challenging. On FreshStack, existing\nretrieval models, when applied out-of-the-box, significantly underperform\noracle approaches on all five topics, denoting plenty of headroom to improve IR\nquality. In addition, we identify cases where rerankers do not clearly improve\nfirst-stage retrieval accuracy (two out of five topics). We hope that\nFreshStack will facilitate future work toward constructing realistic, scalable,\nand uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are\navailable at: https://fresh-stack.github.io."}
{"id": "2504.13146", "pdf": "https://arxiv.org/pdf/2504.13146", "abs": "https://arxiv.org/abs/2504.13146", "authors": ["Yash Savani", "Asher Trockman", "Zhili Feng", "Avi Schwarzschild", "Alexander Robey", "Marc Finzi", "J. Zico Kolter"], "title": "Antidistillation Sampling", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\n\\emph{Antidistillation sampling} provides exactly this capability. By\nstrategically modifying a model's next-token probability distribution,\nantidistillation sampling poisons reasoning traces, rendering them\nsignificantly less effective for distillation while preserving the model's\npractical utility. For further details, see https://antidistillation.com."}
{"id": "2504.13151", "pdf": "https://arxiv.org/pdf/2504.13151", "abs": "https://arxiv.org/abs/2504.13151", "authors": ["Aaron Mueller", "Atticus Geiger", "Sarah Wiegreffe", "Dana Arad", "Iván Arcuschin", "Adam Belfki", "Yik Siu Chan", "Jaden Fiotto-Kaufman", "Tal Haklay", "Michael Hanna", "Jing Huang", "Rohan Gupta", "Yaniv Nikankin", "Hadas Orgad", "Nikhil Prakash", "Anja Reusch", "Aruna Sankaranarayanan", "Shun Shao", "Alessandro Stolfo", "Martin Tutek", "Amir Zur", "David Bau", "Yonatan Belinkov"], "title": "MIB: A Mechanistic Interpretability Benchmark", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "How can we know whether new mechanistic interpretability methods achieve real\nimprovements? In pursuit of meaningful and lasting evaluation standards, we\npropose MIB, a benchmark with two tracks spanning four tasks and five models.\nMIB favors methods that precisely and concisely recover relevant causal\npathways or specific causal variables in neural language models. The circuit\nlocalization track compares methods that locate the model components - and\nconnections between them - most important for performing a task (e.g.,\nattribution patching or information flow routes). The causal variable\nlocalization track compares methods that featurize a hidden vector, e.g.,\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and locate\nmodel features for a causal variable relevant to the task. Using MIB, we find\nthat attribution and mask optimization methods perform best on circuit\nlocalization. For causal variable localization, we find that the supervised DAS\nmethod performs best, while SAE features are not better than neurons, i.e.,\nstandard dimensions of hidden vectors. These findings illustrate that MIB\nenables meaningful comparisons of methods, and increases our confidence that\nthere has been real progress in the field."}
{"id": "2504.13171", "pdf": "https://arxiv.org/pdf/2504.13171", "abs": "https://arxiv.org/abs/2504.13171", "authors": ["Kevin Lin", "Charlie Snell", "Yu Wang", "Charles Packer", "Sarah Wooders", "Ion Stoica", "Joseph E. Gonzalez"], "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time", "categories": ["cs.AI", "cs.CL"], "comment": "Code and data released at:\n  https://github.com/letta-ai/sleep-time-compute", "summary": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task."}
{"id": "2504.13172", "pdf": "https://arxiv.org/pdf/2504.13172", "abs": "https://arxiv.org/abs/2504.13172", "authors": ["Haoxuan Li", "Yi Bin", "Yunshan Ma", "Guoqing Wang", "Yang Yang", "See-Kiong Ng", "Tat-Seng Chua"], "title": "SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework with MLLMs", "categories": ["cs.IR", "cs.CL", "cs.MM"], "comment": null, "summary": "Cross-modal retrieval (CMR) is a fundamental task in multimedia research,\nfocused on retrieving semantically relevant targets across different\nmodalities. While traditional CMR methods match text and image via\nembedding-based similarity calculations, recent advancements in pre-trained\ngenerative models have established generative retrieval as a promising\nalternative. This paradigm assigns each target a unique identifier and\nleverages a generative model to directly predict identifiers corresponding to\ninput queries without explicit indexing. Despite its great potential, current\ngenerative CMR approaches still face semantic information insufficiency in both\nidentifier construction and generation processes. To address these limitations,\nwe propose a novel unified Semantic-enhanced generative Cross-mOdal REtrieval\nframework (SemCORE), designed to unleash the semantic understanding\ncapabilities in generative cross-modal retrieval task. Specifically, we first\nconstruct a Structured natural language IDentifier (SID) that effectively\naligns target identifiers with generative models optimized for natural language\ncomprehension and generation. Furthermore, we introduce a Generative Semantic\nVerification (GSV) strategy enabling fine-grained target discrimination.\nAdditionally, to the best of our knowledge, SemCORE is the first framework to\nsimultaneously consider both text-to-image and image-to-text retrieval tasks\nwithin generative cross-modal retrieval. Extensive experiments demonstrate that\nour framework outperforms state-of-the-art generative cross-modal retrieval\nmethods. Notably, SemCORE achieves substantial improvements across benchmark\ndatasets, with an average increase of 8.65 points in Recall@1 for text-to-image\nretrieval."}
