{"id": "2504.13187", "pdf": "https://arxiv.org/pdf/2504.13187", "abs": "https://arxiv.org/abs/2504.13187", "authors": ["In Hak Moon"], "title": "Benchmarking Large Language Models for Calculus Problem-Solving: A Comparative Analysis", "categories": ["cs.CL"], "comment": null, "summary": "This study presents a comprehensive evaluation of five leading large language\nmodels (LLMs) - Chat GPT 4o, Copilot Pro, Gemini Advanced, Claude Pro, and Meta\nAI - on their performance in solving calculus differentiation problems. The\ninvestigation assessed these models across 13 fundamental problem types,\nemploying a systematic cross-evaluation framework where each model solved\nproblems generated by all models. Results revealed significant performance\ndisparities, with Chat GPT 4o achieving the highest success rate (94.71%),\nfollowed by Claude Pro (85.74%), Gemini Advanced (84.42%), Copilot Pro\n(76.30%), and Meta AI (56.75%). All models excelled at procedural\ndifferentiation tasks but showed varying limitations with conceptual\nunderstanding and algebraic manipulation. Notably, problems involving\nincreasing/decreasing intervals and optimization word problems proved most\nchallenging across all models. The cross-evaluation matrix revealed that Claude\nPro generated the most difficult problems, suggesting distinct capabilities\nbetween problem generation and problem-solving. These findings have significant\nimplications for educational applications, highlighting both the potential and\nlimitations of LLMs as calculus learning tools. While they demonstrate\nimpressive procedural capabilities, their conceptual understanding remains\nlimited compared to human mathematical reasoning, emphasizing the continued\nimportance of human instruction for developing deeper mathematical\ncomprehension."}
{"id": "2504.13189", "pdf": "https://arxiv.org/pdf/2504.13189", "abs": "https://arxiv.org/abs/2504.13189", "authors": ["Sohom Ghosh", "Sudip Kumar Naskar"], "title": "BASIR: Budget-Assisted Sectoral Impact Ranking -- A Dataset for Sector Identification and Performance Prediction Using Language Models", "categories": ["cs.CL", "q-fin.ST"], "comment": "The codes and the datasets can be accessed from\n  https://huggingface.co/datasets/sohomghosh/BASIR_Budget_Assisted_Sectoral_Impact_Ranking/tree/main/", "summary": "Government fiscal policies, particularly annual union budgets, exert\nsignificant influence on financial markets. However, real-time analysis of\nbudgetary impacts on sector-specific equity performance remains\nmethodologically challenging and largely unexplored. This study proposes a\nframework to systematically identify and rank sectors poised to benefit from\nIndia's Union Budget announcements. The framework addresses two core tasks: (1)\nmulti-label classification of excerpts from budget transcripts into 81\npredefined economic sectors, and (2) performance ranking of these sectors.\nLeveraging a comprehensive corpus of Indian Union Budget transcripts from 1947\nto 2025, we introduce BASIR (Budget-Assisted Sectoral Impact Ranking), an\nannotated dataset mapping excerpts from budgetary transcripts to sectoral\nimpacts. Our architecture incorporates fine-tuned embeddings for sector\nidentification, coupled with language models that rank sectors based on their\npredicted performances. Our results demonstrate 0.605 F1-score in sector\nclassification, and 0.997 NDCG score in predicting ranks of sectors based on\npost-budget performances. The methodology enables investors and policymakers to\nquantify fiscal policy impacts through structured, data-driven insights,\naddressing critical gaps in manual analysis. The annotated dataset has been\nreleased under CC-BY-NC-SA-4.0 license to advance computational economics\nresearch."}
{"id": "2504.13216", "pdf": "https://arxiv.org/pdf/2504.13216", "abs": "https://arxiv.org/abs/2504.13216", "authors": ["Bokwang Hwang", "Seonkyu Lim", "Taewoong Kim", "Yongjae Geun", "Sunghyun Bang", "Sohyun Park", "Jihyun Park", "Myeonggyu Lee", "Jinwoo Lee", "Yerin Kim", "Jinsun Yoo", "Jingyeong Hong", "Jina Park", "Yongchan Kim", "Suhyun Kim", "Younggyun Hahm", "Yiseul Lee", "Yejee Kang", "Chanhyuk Yoon", "Chansu Lee", "Heeyewon Jeong", "Jiyeon Lee", "Seonhye Gu", "Hyebin Kang", "Yousang Cho", "Hangyeol Yoo", "KyungTae Lim"], "title": "KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce KFinEval-Pilot, a benchmark suite specifically designed to\nevaluate large language models (LLMs) in the Korean financial domain.\nAddressing the limitations of existing English-centric benchmarks,\nKFinEval-Pilot comprises over 1,000 curated questions across three critical\nareas: financial knowledge, legal reasoning, and financial toxicity. The\nbenchmark is constructed through a semi-automated pipeline that combines\nGPT-4-generated prompts with expert validation to ensure domain relevance and\nfactual accuracy. We evaluate a range of representative LLMs and observe\nnotable performance differences across models, with trade-offs between task\naccuracy and output safety across different model families. These results\nhighlight persistent challenges in applying LLMs to high-stakes financial\napplications, particularly in reasoning and safety. Grounded in real-world\nfinancial use cases and aligned with the Korean regulatory and linguistic\ncontext, KFinEval-Pilot serves as an early diagnostic tool for developing safer\nand more reliable financial AI systems."}
{"id": "2504.13217", "pdf": "https://arxiv.org/pdf/2504.13217", "abs": "https://arxiv.org/abs/2504.13217", "authors": ["Jennifer Haase", "Finn Klessascheck", "Jan Mendling", "Sebastian Pokutta"], "title": "Sustainability via LLM Right-sizing", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 2 Figures, 6 Tables", "summary": "Large language models (LLMs) have become increasingly embedded in\norganizational workflows. This has raised concerns over their energy\nconsumption, financial costs, and data sovereignty. While performance\nbenchmarks often celebrate cutting-edge models, real-world deployment decisions\nrequire a broader perspective: when is a smaller, locally deployable model\n\"good enough\"? This study offers an empirical answer by evaluating eleven\nproprietary and open-weight LLMs across ten everyday occupational tasks,\nincluding summarizing texts, generating schedules, and drafting emails and\nproposals. Using a dual-LLM-based evaluation framework, we automated task\nexecution and standardized evaluation across ten criteria related to output\nquality, factual accuracy, and ethical responsibility. Results show that GPT-4o\ndelivers consistently superior performance but at a significantly higher cost\nand environmental footprint. Notably, smaller models like Gemma-3 and Phi-4\nachieved strong and reliable results on most tasks, suggesting their viability\nin contexts requiring cost-efficiency, local deployment, or privacy. A cluster\nanalysis revealed three model groups -- premium all-rounders, competent\ngeneralists, and limited but safe performers -- highlighting trade-offs between\nquality, control, and sustainability. Significantly, task type influenced model\neffectiveness: conceptual tasks challenged most models, while aggregation and\ntransformation tasks yielded better performances. We argue for a shift from\nperformance-maximizing benchmarks to task- and context-aware sufficiency\nassessments that better reflect organizational priorities. Our approach\ncontributes a scalable method to evaluate AI models through a sustainability\nlens and offers actionable guidance for responsible LLM deployment in practice."}
{"id": "2504.13227", "pdf": "https://arxiv.org/pdf/2504.13227", "abs": "https://arxiv.org/abs/2504.13227", "authors": ["Weijie Shi", "Jipeng Zhang", "Yaguang Wu", "Jingzhi Fang", "Ruiyuan Zhang", "Jiajie Xu", "Jia Zhu", "Hao Chen", "Yao Zhao", "Sirui Han", "Xiaofang Zhou"], "title": "DIDS: Domain Impact-aware Data Sampling for Large Language Model Training", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are commonly trained on multi-domain datasets,\nwhere domain sampling strategies significantly impact model performance due to\nvarying domain importance across downstream tasks. Existing approaches for\noptimizing domain-level sampling strategies struggle with maintaining\nintra-domain consistency and accurately measuring domain impact. In this paper,\nwe present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain\nconsistency, a gradient clustering algorithm is proposed to group training data\nbased on their learning effects, where a proxy language model and\ndimensionality reduction are employed to reduce computational overhead. To\naccurately measure domain impact, we develop a Fisher Information Matrix (FIM)\nguided metric that quantifies how domain-specific parameter updates affect the\nmodel's output distributions on downstream tasks, with theoretical guarantees.\nFurthermore, to determine optimal sampling ratios, DIDS combines both the\nFIM-guided domain impact assessment and loss learning trajectories that\nindicate domain-specific potential, while accounting for diminishing marginal\nreturns. Extensive experiments demonstrate that DIDS achieves 3.4% higher\naverage performance while maintaining comparable training efficiency."}
{"id": "2504.13237", "pdf": "https://arxiv.org/pdf/2504.13237", "abs": "https://arxiv.org/abs/2504.13237", "authors": ["Yan Yang", "Yixia Li", "Hongru Wang", "Xuetao Wei", "Jianqiao Yu", "Yun Chen", "Guanhua Chen"], "title": "ImPart: Importance-Aware Delta-Sparsification for Improved Model Compression and Merging in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "With the proliferation of task-specific large language models, delta\ncompression has emerged as a method to mitigate the resource challenges of\ndeploying numerous such models by effectively compressing the delta model\nparameters. Previous delta-sparsification methods either remove parameters\nrandomly or truncate singular vectors directly after singular value\ndecomposition (SVD). However, these methods either disregard parameter\nimportance entirely or evaluate it with too coarse a granularity. In this work,\nwe introduce ImPart, a novel importance-aware delta sparsification approach.\nLeveraging SVD, it dynamically adjusts sparsity ratios of different singular\nvectors based on their importance, effectively retaining crucial task-specific\nknowledge even at high sparsity ratios. Experiments show that ImPart achieves\nstate-of-the-art delta sparsification performance, demonstrating $2\\times$\nhigher compression ratio than baselines at the same performance level. When\nintegrated with existing methods, ImPart sets a new state-of-the-art on delta\nquantization and model merging."}
{"id": "2504.13261", "pdf": "https://arxiv.org/pdf/2504.13261", "abs": "https://arxiv.org/abs/2504.13261", "authors": ["Dong Wang"], "title": "CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.SI"], "comment": "12 pages, 1 figure, 3 tables", "summary": "Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT\nhas significantly impacted foreign language education, yet their pedagogical\ngrammar competence remains under-assessed. This paper introduces CPG-EVAL, the\nfirst dedicated benchmark specifically designed to evaluate LLMs' knowledge of\npedagogical grammar within the context of foreign language instruction.\nMethodology: The benchmark comprises five tasks designed to assess grammar\nrecognition, fine-grained grammatical distinction, categorical discrimination,\nand resistance to linguistic interference. Findings: Smaller-scale models can\nsucceed in single language instance tasks, but struggle with multiple instance\ntasks and interference from confusing instances. Larger-scale models show\nbetter resistance to interference but still have significant room for accuracy\nimprovement. The evaluation indicates the need for better instructional\nalignment and more rigorous benchmarks, to effectively guide the deployment of\nLLMs in educational contexts. Value: This study offers the first specialized,\ntheory-driven, multi-tiered benchmark framework for systematically evaluating\nLLMs' pedagogical grammar competence in Chinese language teaching contexts.\nCPG-EVAL not only provides empirical insights for educators, policymakers, and\nmodel developers to better gauge AI's current abilities in educational\nsettings, but also lays the groundwork for future research on improving model\nalignment, enhancing educational suitability, and ensuring informed\ndecision-making concerning LLM integration in foreign language instruction."}
{"id": "2504.13284", "pdf": "https://arxiv.org/pdf/2504.13284", "abs": "https://arxiv.org/abs/2504.13284", "authors": ["Derguene Mbaye", "Madoune Robert Seye", "Moussa Diallo", "Mamadou Lamine Ndiaye", "Djiby Sow", "Dimitri Samuel Adjanohoun", "Tatiana Mbengue", "Cheikh Samba Wade", "De Roulet Pablo", "Jean-Claude Baraka Munyaka", "Jerome Chenal"], "title": "Sentiment Analysis on the young people's perception about the mobile Internet costs in Senegal", "categories": ["cs.CL", "cs.SI"], "comment": "19 pages, 14 figures, 10th International Congress on Information and\n  Communication Technology (ICICT 2025)", "summary": "Internet penetration rates in Africa are rising steadily, and mobile Internet\nis getting an even bigger boost with the availability of smartphones. Young\npeople are increasingly using the Internet, especially social networks, and\nSenegal is no exception to this revolution. Social networks have become the\nmain means of expression for young people. Despite this evolution in Internet\naccess, there are few operators on the market, which limits the alternatives\navailable in terms of value for money. In this paper, we will look at how young\npeople feel about the price of mobile Internet in Senegal, in relation to the\nperceived quality of the service, through their comments on social networks. We\nscanned a set of Twitter and Facebook comments related to the subject and\napplied a sentiment analysis model to gather their general feelings."}
{"id": "2504.13367", "pdf": "https://arxiv.org/pdf/2504.13367", "abs": "https://arxiv.org/abs/2504.13367", "authors": ["Xiao Pu", "Michael Saxon", "Wenyue Hua", "William Yang Wang"], "title": "THOUGHTTERMINATOR: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning models have demonstrated impressive performance on difficult tasks\nthat traditional language models struggle at. However, many are plagued with\nthe problem of overthinking--generating large amounts of unnecessary tokens\nwhich don't improve accuracy on a question. We introduce approximate measures\nof problem-level difficulty and demonstrate that a clear relationship between\nproblem difficulty and optimal token spend exists, and evaluate how well\ncalibrated a variety of reasoning models are in terms of efficiently allocating\nthe optimal token count. We find that in general, reasoning models are poorly\ncalibrated, particularly on easy problems. To evaluate calibration on easy\nquestions we introduce DUMB500, a dataset of extremely easy math, reasoning,\ncode, and task problems, and jointly evaluate reasoning model on these simple\nexamples and extremely difficult examples from existing frontier benchmarks on\nthe same task domain. Finally, we introduce THOUGHTTERMINATOR, a training-free\nblack box decoding technique that significantly improves reasoning model\ncalibration."}
{"id": "2504.13425", "pdf": "https://arxiv.org/pdf/2504.13425", "abs": "https://arxiv.org/abs/2504.13425", "authors": ["Grace Byun", "Shinsun Lee", "Nayoung Choi", "Jinho Choi"], "title": "Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with Security Filtering", "categories": ["cs.CL"], "comment": null, "summary": "Existing Retrieval-Augmented Generation (RAG) systems face challenges in\nenterprise settings due to limited retrieval scope and data security risks.\nWhen relevant internal documents are unavailable, the system struggles to\ngenerate accurate and complete responses. Additionally, using closed-source\nLarge Language Models (LLMs) raises concerns about exposing proprietary\ninformation. To address these issues, we propose the Secure Multifaceted-RAG\n(SecMulti-RAG) framework, which retrieves not only from internal documents but\nalso from two supplementary sources: pre-generated expert knowledge for\nanticipated queries and on-demand external LLM-generated knowledge. To mitigate\nsecurity risks, we adopt a local open-source generator and selectively utilize\nexternal LLMs only when prompts are deemed safe by a filtering mechanism. This\napproach enhances completeness, prevents data leakage, and reduces costs. In\nour evaluation on a report generation task in the automotive industry,\nSecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9\npercent win rates across correctness, richness, and helpfulness in LLM-based\nevaluation, and 56.3 to 70.4 percent in human evaluation. This highlights\nSecMulti-RAG as a practical and secure solution for enterprise RAG."}
{"id": "2504.13439", "pdf": "https://arxiv.org/pdf/2504.13439", "abs": "https://arxiv.org/abs/2504.13439", "authors": ["Grace Byun", "Jinho Choi"], "title": "D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating generative models with open-ended generation is challenging due to\ninconsistencies in response formats. Multiple-choice (MC) evaluation mitigates\nthis issue, but generating high-quality distractors is time-consuming and\nlabor-intensive. We introduce D-GEN, the first open-source distractor generator\nmodel that transforms open-ended data into an MC format. To evaluate distractor\nquality, we propose two novel methods: (1) ranking alignment, ensuring\ngenerated distractors retain the discriminatory power of ground-truth\ndistractors, and (2) entropy analysis, comparing model confidence\ndistributions. Our results show that D-GEN preserves ranking consistency\n(Spearman's rho 0.99, Kendall's tau 0.94) and closely matches the entropy\ndistribution of ground-truth distractors. Human evaluation further confirms the\nfluency, coherence, distractiveness, and incorrectness. Our work advances\nrobust and efficient distractor generation with automated evaluation, setting a\nnew standard for MC evaluation."}
{"id": "2504.13471", "pdf": "https://arxiv.org/pdf/2504.13471", "abs": "https://arxiv.org/abs/2504.13471", "authors": ["Jiliang Ni", "Jiachen Pu", "Zhongyi Yang", "Kun Zhou", "Hui Wang", "Xiaoliang Xiao", "Dakui Wang", "Xin Li", "Jingfeng Luo", "Conggang Hu"], "title": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs", "categories": ["cs.CL"], "comment": null, "summary": "In recent years, Large Language Models (LLMs) have significantly advanced\nartificial intelligence by optimizing traditional Natural Language Processing\n(NLP) pipelines, improving performance and generalization. This has spurred\ntheir integration into various systems. Many NLP systems, including ours,\nemploy a \"one-stage\" pipeline directly incorporating LLMs. While effective,\nthis approach incurs substantial costs and latency due to the need for large\nmodel parameters to achieve satisfactory outcomes. This paper introduces a\nthree-stage cost-efficient end-to-end LLM deployment pipeline-including\nprototyping, knowledge transfer, and model compression-to tackle the\ncost-performance dilemma in LLM-based frameworks. Our approach yields a super\ntiny model optimized for cost and performance in online systems, simplifying\nthe system architecture. Initially, by transforming complex tasks into a\nfunction call-based LLM-driven pipeline, an optimal performance prototype\nsystem is constructed to produce high-quality data as a teacher model. The\nsecond stage combine techniques like rejection fine-tuning, reinforcement\nlearning and knowledge distillation to transfer knowledge to a smaller 0.5B\nstudent model, delivering effective performance at minimal cost. The final\nstage applies quantization and pruning to extremely compress model to 0.4B,\nachieving ultra-low latency and cost. The framework's modular design and\ncross-domain capabilities suggest potential applicability in other NLP areas."}
{"id": "2504.13475", "pdf": "https://arxiv.org/pdf/2504.13475", "abs": "https://arxiv.org/abs/2504.13475", "authors": ["Chenwei Yan", "Xiangling Fu", "Yuxuan Xiong", "Tianyi Wang", "Siu Cheung Hui", "Ji Wu", "Xien Liu"], "title": "LLM Sensitivity Evaluation Framework for Clinical Diagnosis", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance across\nvarious domains. However, for clinical diagnosis, higher expectations are\nrequired for LLM's reliability and sensitivity: thinking like physicians and\nremaining sensitive to key medical information that affects diagnostic\nreasoning, as subtle variations can lead to different diagnosis results. Yet,\nexisting works focus mainly on investigating the sensitivity of LLMs to\nirrelevant context and overlook the importance of key information. In this\npaper, we investigate the sensitivity of LLMs, i.e. GPT-3.5, GPT-4, Gemini,\nClaude3 and LLaMA2-7b, to key medical information by introducing different\nperturbation strategies. The evaluation results highlight the limitations of\ncurrent LLMs in remaining sensitive to key medical information for diagnostic\ndecision-making. The evolution of LLMs must focus on improving their\nreliability, enhancing their ability to be sensitive to key information, and\neffectively utilizing this information. These improvements will enhance human\ntrust in LLMs and facilitate their practical application in real-world\nscenarios. Our code and dataset are available at\nhttps://github.com/chenwei23333/DiagnosisQA."}
{"id": "2504.13500", "pdf": "https://arxiv.org/pdf/2504.13500", "abs": "https://arxiv.org/abs/2504.13500", "authors": ["Jianing Wang", "Jin Jiang", "Yang Liu", "Mengdi Zhang", "Xunliang Cai"], "title": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce a new \\emph{process prejudge} strategy in LLM\nreasoning to demonstrate that bootstrapping with process prejudge allows the\nLLM to adaptively anticipate the errors encountered when advancing the\nsubsequent reasoning steps, similar to people sometimes pausing to think about\nwhat mistakes may occur and how to avoid them, rather than relying solely on\ntrial and error. Specifically, we define a prejudge node in the rationale,\nwhich represents a reasoning step, with at least one step that follows the\nprejudge node that has no paths toward the correct answer. To synthesize the\nprejudge reasoning process, we present an automated reasoning framework with a\ndynamic tree-searching strategy. This framework requires only one LLM to\nperform answer judging, response critiquing, prejudge generation, and thought\ncompletion. Furthermore, we develop a two-phase training mechanism with\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to further enhance\nthe reasoning capabilities of LLMs. Experimental results from competition-level\ncomplex reasoning demonstrate that our method can teach the model to prejudge\nbefore thinking and significantly enhance the reasoning ability of LLMs. Code\nand data is released at https://github.com/wjn1996/Prejudge-Before-Think."}
{"id": "2504.13534", "pdf": "https://arxiv.org/pdf/2504.13534", "abs": "https://arxiv.org/abs/2504.13534", "authors": ["Feiyang Li", "Peng Fang", "Zhan Shi", "Arijit Khan", "Fang Wang", "Dan Feng", "Weihao Wang", "Xin Zhang", "Yongjian Cui"], "title": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While chain-of-thought (CoT) reasoning improves the performance of large\nlanguage models (LLMs) in complex tasks, it still has two main challenges: the\nlow reliability of relying solely on LLMs to generate reasoning chains and the\ninterference of natural language reasoning chains on the inference logic of\nLLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework\nwith three key designs: (i) Knowledge Graph-driven CoT Generation, featuring\nknowledge graphs to modulate reasoning chain generation of LLMs, thereby\nenhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which\nincorporates retrieval-augmented generation (RAG) into knowledge graphs to\nretrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable\ninformation; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to\nexecute reasoning tasks in pseudo-programs with greater logical rigor. We\nconduct a comprehensive evaluation on nine public datasets, covering three\nreasoning problems. Compared with the-state-of-the-art methods, CoT-RAG\nexhibits a significant accuracy improvement, ranging from 4.0% to 23.0%.\nFurthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable\naccuracy and efficient execution, highlighting its strong practical\napplicability and scalability."}
{"id": "2504.13545", "pdf": "https://arxiv.org/pdf/2504.13545", "abs": "https://arxiv.org/abs/2504.13545", "authors": ["Azmarah Rizvi", "Navojith Thamindu", "A. M. N. H. Adhikari", "W. P. U. Senevirathna", "Dharshana Kasthurirathna", "Lakmini Abeywardhana"], "title": "Enhancing Multilingual Sentiment Analysis with Explainability for Sinhala, English, and Code-Mixed Content", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 6 figures, 4 tables", "summary": "Sentiment analysis is crucial for brand reputation management in the banking\nsector, where customer feedback spans English, Sinhala, Singlish, and\ncode-mixed text. Existing models struggle with low-resource languages like\nSinhala and lack interpretability for practical use. This research develops a\nhybrid aspect-based sentiment analysis framework that enhances multilingual\ncapabilities with explainable outputs. Using cleaned banking customer reviews,\nwe fine-tune XLM-RoBERTa for Sinhala and code-mixed text, integrate\ndomain-specific lexicon correction, and employ BERT-base-uncased for English.\nThe system classifies sentiment (positive, neutral, negative) with confidence\nscores, while SHAP and LIME improve interpretability by providing real-time\nsentiment explanations. Experimental results show that our approaches\noutperform traditional transformer-based classifiers, achieving 92.3 percent\naccuracy and an F1-score of 0.89 in English and 88.4 percent in Sinhala and\ncode-mixed content. An explainability analysis reveals key sentiment drivers,\nimproving trust and transparency. A user-friendly interface delivers\naspect-wise sentiment insights, ensuring accessibility for businesses. This\nresearch contributes to robust, transparent sentiment analysis for financial\napplications by bridging gaps in multilingual, low-resource NLP and\nexplainability."}
{"id": "2504.13562", "pdf": "https://arxiv.org/pdf/2504.13562", "abs": "https://arxiv.org/abs/2504.13562", "authors": ["Yu Li", "Han Jiang", "Zhihua Wei"], "title": "DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification", "categories": ["cs.CL"], "comment": null, "summary": "With the widespread adoption of Large Language Models (LLMs), jailbreak\nattacks have become an increasingly pressing safety concern. While\nsafety-aligned LLMs can effectively defend against normal harmful queries, they\nremain vulnerable to such attacks. Existing defense methods primarily rely on\nfine-tuning or input modification, which often suffer from limited\ngeneralization and reduced utility. To address this, we introduce DETAM, a\nfinetuning-free defense approach that improves the defensive capabilities\nagainst jailbreak attacks of LLMs via targeted attention modification.\nSpecifically, we analyze the differences in attention scores between successful\nand unsuccessful defenses to identify the attention heads sensitive to\njailbreak attacks. During inference, we reallocate attention to emphasize the\nuser's core intention, minimizing interference from attack tokens. Our\nexperimental results demonstrate that DETAM outperforms various baselines in\njailbreak defense and exhibits robust generalization across different attacks\nand models, maintaining its effectiveness even on in-the-wild jailbreak data.\nFurthermore, in evaluating the model's utility, we incorporated over-defense\ndatasets, which further validate the superior performance of our approach. The\ncode will be released immediately upon acceptance."}
{"id": "2504.13592", "pdf": "https://arxiv.org/pdf/2504.13592", "abs": "https://arxiv.org/abs/2504.13592", "authors": ["Zihao Feng", "Xiaoxue Wang", "Ziwei Bai", "Donghang Su", "Bowen Wu", "Qun Yu", "Baoxun Wang"], "title": "Improving Generalization in Intent Detection: GRPO with Reward-Based Curriculum Sampling", "categories": ["cs.CL"], "comment": null, "summary": "Intent detection, a critical component in task-oriented dialogue (TOD)\nsystems, faces significant challenges in adapting to the rapid influx of\nintegrable tools with complex interrelationships. Existing approaches, such as\nzero-shot reformulations and LLM-based dynamic recognition, struggle with\nperformance degradation when encountering unseen intents, leading to erroneous\ntask routing. To enhance the model's generalization performance on unseen\ntasks, we employ Reinforcement Learning (RL) combined with a Reward-based\nCurriculum Sampling (RCS) during Group Relative Policy Optimization (GRPO)\ntraining in intent detection tasks. Experiments demonstrate that RL-trained\nmodels substantially outperform supervised fine-tuning (SFT) baselines in\ngeneralization. Besides, the introduction of the RCS, significantly bolsters\nthe effectiveness of RL in intent detection by focusing the model on\nchallenging cases during training. Moreover, incorporating Chain-of-Thought\n(COT) processes in RL notably improves generalization in complex intent\ndetection tasks, underscoring the importance of thought in challenging\nscenarios. This work advances the generalization of intent detection tasks,\noffering practical insights for deploying adaptable dialogue systems."}
{"id": "2504.13603", "pdf": "https://arxiv.org/pdf/2504.13603", "abs": "https://arxiv.org/abs/2504.13603", "authors": ["Pin-Er Chen", "Da-Chen Lian", "Shu-Kai Hsieh", "Sieh-Chuen Huang", "Hsuan-Lei Shao", "Jun-Wei Chiu", "Yang-Hsien Lin", "Zih-Ching Chen", "Cheng-Kuang", "Eddie TC Huang", "Simon See"], "title": "Continual Pre-Training is (not) What You Need in Domain Adaption", "categories": ["cs.CL"], "comment": "11 pages, 2 figures", "summary": "The recent advances in Legal Large Language Models (LLMs) have transformed\nthe landscape of legal research and practice by automating tasks, enhancing\nresearch precision, and supporting complex decision-making processes. However,\neffectively adapting LLMs to the legal domain remains challenging due to the\ncomplexity of legal reasoning, the need for precise interpretation of\nspecialized language, and the potential for hallucinations. This paper examines\nthe efficacy of Domain-Adaptive Continual Pre-Training (DACP) in improving the\nlegal reasoning capabilities of LLMs. Through a series of experiments on legal\nreasoning tasks within the Taiwanese legal framework, we demonstrate that while\nDACP enhances domain-specific knowledge, it does not uniformly improve\nperformance across all legal tasks. We discuss the trade-offs involved in DACP,\nparticularly its impact on model generalization and performance in prompt-based\ntasks, and propose directions for future research to optimize domain adaptation\nstrategies in legal AI."}
{"id": "2504.13615", "pdf": "https://arxiv.org/pdf/2504.13615", "abs": "https://arxiv.org/abs/2504.13615", "authors": ["Ritwik Mishra", "Rajiv Ratn Shah", "Ponnurangam Kumaraguru"], "title": "Long-context Non-factoid Question Answering in Indic Languages", "categories": ["cs.CL"], "comment": null, "summary": "Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA."}
{"id": "2504.13626", "pdf": "https://arxiv.org/pdf/2504.13626", "abs": "https://arxiv.org/abs/2504.13626", "authors": ["Yule Liu", "Jingyi Zheng", "Zhen Sun", "Zifan Peng", "Wenhan Dong", "Zeyang Sha", "Shiwen Cui", "Weiqiang Wang", "Xinlei He"], "title": "Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities in multiple tasks. However, LRMs typically suffer from\n\"overthinking\" problems, where models generate significantly redundant\nreasoning steps while bringing limited performance gains. Existing work relies\non fine-tuning to mitigate overthinking, which requires additional data,\nunconventional training setups, risky safety misalignment, and poor\ngeneralization.\n  Through empirical analysis, we reveal an important characteristic of LRM\nbehaviors that placing external CoTs generated by smaller models between the\nthinking token ($\\texttt{<think>}$ and $\\texttt{</think>)}$ can effectively\nmanipulate the model to generate fewer thoughts. Building on these insights, we\npropose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass\nunnecessary intermediate steps and reduce computational costs significantly. We\nconduct extensive experiments to validate the utility and efficiency of\nThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code\ndataset, ThoughtMani keeps the original performance and reduces output token\ncounts by approximately 30%, with little overhead from the CoT generator.\nFurthermore, we find that ThoughtMani enhances safety alignment by an average\nof 10%. Since model vendors typically serve models of different sizes\nsimultaneously, ThoughtMani provides an effective way to construct more\nefficient and accessible LRMs for real-world applications."}
{"id": "2504.13629", "pdf": "https://arxiv.org/pdf/2504.13629", "abs": "https://arxiv.org/abs/2504.13629", "authors": ["Cong William Lin", "Wu Zhu"], "title": "Divergent LLM Adoption and Heterogeneous Convergence Paths in Research Writing", "categories": ["cs.CL", "cs.AI", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, are reshaping content creation\nand academic writing. This study investigates the impact of AI-assisted\ngenerative revisions on research manuscripts, focusing on heterogeneous\nadoption patterns and their influence on writing convergence. Leveraging a\ndataset of over 627,000 academic papers from arXiv, we develop a novel\nclassification framework by fine-tuning prompt- and discipline-specific large\nlanguage models to detect the style of ChatGPT-revised texts. Our findings\nreveal substantial disparities in LLM adoption across academic disciplines,\ngender, native language status, and career stage, alongside a rapid evolution\nin scholarly writing styles. Moreover, LLM usage enhances clarity, conciseness,\nand adherence to formal writing conventions, with improvements varying by\nrevision type. Finally, a difference-in-differences analysis shows that while\nLLMs drive convergence in academic writing, early adopters, male researchers,\nnon-native speakers, and junior scholars exhibit the most pronounced stylistic\nshifts, aligning their writing more closely with that of established\nresearchers."}
{"id": "2504.13630", "pdf": "https://arxiv.org/pdf/2504.13630", "abs": "https://arxiv.org/abs/2504.13630", "authors": ["Shaomu Tan", "Christof Monz"], "title": "Remedy: Learning Machine Translation Evaluation from Human Preferences with Reward Modeling", "categories": ["cs.CL"], "comment": null, "summary": "A key challenge in MT evaluation is the inherent noise and inconsistency of\nhuman ratings. Regression-based neural metrics struggle with this noise, while\nprompting LLMs shows promise at system-level evaluation but performs poorly at\nsegment level. In this work, we propose ReMedy, a novel MT metric framework\nthat reformulates translation evaluation as a reward modeling task. Instead of\nregressing on imperfect human ratings directly, ReMedy learns relative\ntranslation quality using pairwise preference data, resulting in a more\nreliable evaluation. In extensive experiments across WMT22-24 shared tasks (39\nlanguage pairs, 111 MT systems), ReMedy achieves state-of-the-art performance\nat both segment- and system-level evaluation. Specifically, ReMedy-9B surpasses\nlarger WMT winners and massive closed LLMs such as MetricX-13B,\nXCOMET-Ensemble, GEMBA-GPT-4, PaLM-540B, and finetuned PaLM2. Further analyses\ndemonstrate that ReMedy delivers superior capability in detecting translation\nerrors and evaluating low-quality translations."}
{"id": "2504.13643", "pdf": "https://arxiv.org/pdf/2504.13643", "abs": "https://arxiv.org/abs/2504.13643", "authors": ["Tao He", "Lizi Liao", "Ming Liu", "Bing Qin"], "title": "Simulating Before Planning: Constructing Intrinsic User World Model for User-Tailored Dialogue Policy Planning", "categories": ["cs.CL"], "comment": "11 pages, 6 figures, SIGIR 2025", "summary": "Recent advancements in dialogue policy planning have emphasized optimizing\nsystem agent policies to achieve predefined goals, focusing on strategy design,\ntrajectory acquisition, and efficient training paradigms. However, these\napproaches often overlook the critical role of user characteristics, which are\nessential in real-world scenarios like conversational search and\nrecommendation, where interactions must adapt to individual user traits such as\npersonality, preferences, and goals. To address this gap, we first conduct a\ncomprehensive study utilizing task-specific user personas to systematically\nassess dialogue policy planning under diverse user behaviors. By leveraging\nrealistic user profiles for different tasks, our study reveals significant\nlimitations in existing approaches, highlighting the need for user-tailored\ndialogue policy planning. Building on this foundation, we present the\nUser-Tailored Dialogue Policy Planning (UDP) framework, which incorporates an\nIntrinsic User World Model to model user traits and feedback. UDP operates in\nthree stages: (1) User Persona Portraying, using a diffusion model to\ndynamically infer user profiles; (2) User Feedback Anticipating, leveraging a\nBrownian Bridge-inspired anticipator to predict user reactions; and (3)\nUser-Tailored Policy Planning, integrating these insights to optimize response\nstrategies. To ensure robust performance, we further propose an active learning\napproach that prioritizes challenging user personas during training.\nComprehensive experiments on benchmarks, including collaborative and\nnon-collaborative settings, demonstrate the effectiveness of UDP in learning\nuser-specific dialogue strategies. Results validate the protocol's utility and\nhighlight UDP's robustness, adaptability, and potential to advance user-centric\ndialogue systems."}
{"id": "2504.13653", "pdf": "https://arxiv.org/pdf/2504.13653", "abs": "https://arxiv.org/abs/2504.13653", "authors": ["Hesham Abdelmotaleb", "Craig McNeile", "Malgorzata Wojtys"], "title": "Word Embedding Techniques for Classification of Star Ratings", "categories": ["cs.CL", "stat.AP", "62P99"], "comment": "40 pages", "summary": "Telecom services are at the core of today's societies' everyday needs. The\navailability of numerous online forums and discussion platforms enables telecom\nproviders to improve their services by exploring the views of their customers\nto learn about common issues that the customers face. Natural Language\nProcessing (NLP) tools can be used to process the free text collected.\n  One way of working with such data is to represent text as numerical vectors\nusing one of many word embedding models based on neural networks. This research\nuses a novel dataset of telecom customers' reviews to perform an extensive\nstudy showing how different word embedding algorithms can affect the text\nclassification process. Several state-of-the-art word embedding techniques are\nconsidered, including BERT, Word2Vec and Doc2Vec, coupled with several\nclassification algorithms. The important issue of feature engineering and\ndimensionality reduction is addressed and several PCA-based approaches are\nexplored. Moreover, the energy consumption used by the different word\nembeddings is investigated. The findings show that some word embedding models\ncan lead to consistently better text classifiers in terms of precision, recall\nand F1-Score. In particular, for the more challenging classification tasks,\nBERT combined with PCA stood out with the highest performance metrics.\nMoreover, our proposed PCA approach of combining word vectors using the first\nprincipal component shows clear advantages in performance over the traditional\napproach of taking the average."}
{"id": "2504.13655", "pdf": "https://arxiv.org/pdf/2504.13655", "abs": "https://arxiv.org/abs/2504.13655", "authors": ["Jie Zou", "Cheng Lin", "Weikang Guo", "Zheng Wang", "Jiwei Wei", "Yang Yang", "Hengtao Shen"], "title": "Multi-Type Context-Aware Conversational Recommender Systems via Mixture-of-Experts", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "30 pages", "summary": "Conversational recommender systems enable natural language conversations and\nthus lead to a more engaging and effective recommendation scenario. As the\nconversations for recommender systems usually contain limited contextual\ninformation, many existing conversational recommender systems incorporate\nexternal sources to enrich the contextual information. However, how to combine\ndifferent types of contextual information is still a challenge. In this paper,\nwe propose a multi-type context-aware conversational recommender system, called\nMCCRS, effectively fusing multi-type contextual information via\nmixture-of-experts to improve conversational recommender systems. MCCRS\nincorporates both structured information and unstructured information,\nincluding the structured knowledge graph, unstructured conversation history,\nand unstructured item reviews. It consists of several experts, with each expert\nspecialized in a particular domain (i.e., one specific contextual information).\nMultiple experts are then coordinated by a ChairBot to generate the final\nresults. Our proposed MCCRS model takes advantage of different contextual\ninformation and the specialization of different experts followed by a ChairBot\nbreaks the model bottleneck on a single contextual information. Experimental\nresults demonstrate that our proposed MCCRS method achieves significantly\nhigher performance compared to existing baselines."}
{"id": "2504.13677", "pdf": "https://arxiv.org/pdf/2504.13677", "abs": "https://arxiv.org/abs/2504.13677", "authors": ["Andrea Santilli", "Adam Golinski", "Michael Kirchhof", "Federico Danieli", "Arno Blaas", "Miao Xiong", "Luca Zappella", "Sinead Williamson"], "title": "Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for\nimproving their safety and reliability. Evaluations often use performance\nmetrics like AUROC to assess how well UQ methods (e.g., negative sequence\nprobabilities) correlate with task correctness functions (e.g., ROUGE-L). In\nthis paper, we show that commonly used correctness functions bias UQ\nevaluations by inflating the performance of certain UQ methods. We evaluate 7\ncorrectness functions -- from lexical-based and embedding-based metrics to\nLLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our\nanalysis reveals that length biases in the errors of these correctness\nfunctions distort UQ assessments by interacting with length biases in UQ\nmethods. We identify LLM-as-a-judge approaches as among the least length-biased\nchoices and hence a potential solution to mitigate these biases."}
{"id": "2504.13685", "pdf": "https://arxiv.org/pdf/2504.13685", "abs": "https://arxiv.org/abs/2504.13685", "authors": ["Stefano M. Iacus", "Haodong Qi", "Jiyoung Han"], "title": "Deep literature reviews: an application of fine-tuned language models to migration research", "categories": ["cs.CL", "cs.LG", "stat.AP", "stat.CO"], "comment": null, "summary": "This paper presents a hybrid framework for literature reviews that augments\ntraditional bibliometric methods with large language models (LLMs). By\nfine-tuning open-source LLMs, our approach enables scalable extraction of\nqualitative insights from large volumes of research content, enhancing both the\nbreadth and depth of knowledge synthesis. To improve annotation efficiency and\nconsistency, we introduce an error-focused validation process in which LLMs\ngenerate initial labels and human reviewers correct misclassifications.\nApplying this framework to over 20000 scientific articles about human\nmigration, we demonstrate that a domain-adapted LLM can serve as a \"specialist\"\nmodel - capable of accurately selecting relevant studies, detecting emerging\ntrends, and identifying critical research gaps. Notably, the LLM-assisted\nreview reveals a growing scholarly interest in climate-induced migration.\nHowever, existing literature disproportionately centers on a narrow set of\nenvironmental hazards (e.g., floods, droughts, sea-level rise, and land\ndegradation), while overlooking others that more directly affect human health\nand well-being, such as air and water pollution or infectious diseases. This\nimbalance highlights the need for more comprehensive research that goes beyond\nphysical environmental changes to examine their ecological and societal\nconsequences, particularly in shaping migration as an adaptive response.\nOverall, our proposed framework demonstrates the potential of fine-tuned LLMs\nto conduct more efficient, consistent, and insightful literature reviews across\ndisciplines, ultimately accelerating knowledge synthesis and scientific\ndiscovery."}
{"id": "2504.13730", "pdf": "https://arxiv.org/pdf/2504.13730", "abs": "https://arxiv.org/abs/2504.13730", "authors": ["Paul K. Mandal", "Cole Leo", "Connor Hurley"], "title": "Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping Occupied Territory from Open Source Intelligence", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.8; H.3.1; K.4.1"], "comment": "7 pages, 1 figure, 1 table", "summary": "Open-source intelligence provides a stream of unstructured textual data that\ncan inform assessments of territorial control. We present CONTACT, a framework\nfor territorial control prediction using large language models (LLMs) and\nminimal supervision. We evaluate two approaches: SetFit, an embedding-based\nfew-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a\nmultilingual generative LLM. Our model is trained on a small hand-labeled\ndataset of news articles covering ISIS activity in Syria and Iraq, using\nprompt-conditioned extraction of control-relevant signals such as military\noperations, casualties, and location references. We show that the BLOOMZ-based\nmodel outperforms the SetFit baseline, and that prompt-based supervision\nimproves generalization in low-resource settings. CONTACT demonstrates that\nLLMs fine-tuned using few-shot methods can reduce annotation burdens and\nsupport structured inference from open-ended OSINT streams. Our code is\navailable at https://github.com/PaulKMandal/CONTACT/."}
{"id": "2504.13775", "pdf": "https://arxiv.org/pdf/2504.13775", "abs": "https://arxiv.org/abs/2504.13775", "authors": ["Zhengxian Wu", "Juan Wen", "Wanli Peng", "Ziwei Zhang", "Yinghan Zhou", "Yiming Xue"], "title": "BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models", "categories": ["cs.CL", "cs.CR"], "comment": "16 pages, 6 figures", "summary": "Previous insertion-based and paraphrase-based backdoors have achieved great\nsuccess in attack efficacy, but they ignore the text quality and semantic\nconsistency between poisoned and clean texts. Although recent studies introduce\nLLMs to generate poisoned texts and improve the stealthiness, semantic\nconsistency, and text quality, their hand-crafted prompts rely on expert\nexperiences, facing significant challenges in prompt adaptability and attack\nperformance after defenses. In this paper, we propose a novel backdoor attack\nbased on adaptive optimization mechanism of black-box large language models\n(BadApex), which leverages a black-box LLM to generate poisoned text through a\nrefined prompt. Specifically, an Adaptive Optimization Mechanism is designed to\nrefine an initial prompt iteratively using the generation and modification\nagents. The generation agent generates the poisoned text based on the initial\nprompt. Then the modification agent evaluates the quality of the poisoned text\nand refines a new prompt. After several iterations of the above process, the\nrefined prompt is used to generate poisoned texts through LLMs. We conduct\nextensive experiments on three dataset with six backdoor attacks and two\ndefenses. Extensive experimental results demonstrate that BadApex significantly\noutperforms state-of-the-art attacks. It improves prompt adaptability, semantic\nconsistency, and text quality. Furthermore, when two defense methods are\napplied, the average attack success rate (ASR) still up to 96.75%."}
{"id": "2504.13816", "pdf": "https://arxiv.org/pdf/2504.13816", "abs": "https://arxiv.org/abs/2504.13816", "authors": ["Chenghao Xiao", "Hou Pong Chan", "Hao Zhang", "Mahani Aljunied", "Lidong Bing", "Noura Al Moubayed", "Yu Rong"], "title": "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations", "categories": ["cs.CL"], "comment": null, "summary": "While understanding the knowledge boundaries of LLMs is crucial to prevent\nhallucination, research on knowledge boundaries of LLMs has predominantly\nfocused on English. In this work, we present the first study to analyze how\nLLMs recognize knowledge boundaries across different languages by probing their\ninternal representations when processing known and unknown questions in\nmultiple languages. Our empirical studies reveal three key findings: 1) LLMs'\nperceptions of knowledge boundaries are encoded in the middle to middle-upper\nlayers across different languages. 2) Language differences in knowledge\nboundary perception follow a linear structure, which motivates our proposal of\na training-free alignment method that effectively transfers knowledge boundary\nperception ability across languages, thereby helping reduce hallucination risk\nin low-resource languages; 3) Fine-tuning on bilingual question pair\ntranslation further enhances LLMs' recognition of knowledge boundaries across\nlanguages. Given the absence of standard testbeds for cross-lingual knowledge\nboundary analysis, we construct a multilingual evaluation suite comprising\nthree representative types of knowledge boundary data. Our code and datasets\nare publicly available at\nhttps://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries."}
{"id": "2504.13825", "pdf": "https://arxiv.org/pdf/2504.13825", "abs": "https://arxiv.org/abs/2504.13825", "authors": ["Junjie Yang", "Junhao Song", "Xudong Han", "Ziqian Bi", "Tianyang Wang", "Chia Xin Liang", "Xinyuan Song", "Yichao Zhang", "Qian Niu", "Benji Peng", "Keyu Chen", "Ming Liu"], "title": "Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Knowledge distillation (KD) is a technique for transferring knowledge from\ncomplex teacher models to simpler student models, significantly enhancing model\nefficiency and accuracy. It has demonstrated substantial advancements in\nvarious applications including image classification, object detection, language\nmodeling, text classification, and sentiment analysis. Recent innovations in KD\nmethods, such as attention-based approaches, block-wise logit distillation, and\ndecoupling distillation, have notably improved student model performance. These\ntechniques focus on stimulus complexity, attention mechanisms, and global\ninformation capture to optimize knowledge transfer. In addition, KD has proven\neffective in compressing large language models while preserving accuracy,\nreducing computational overhead, and improving inference speed. This survey\nsynthesizes the latest literature, highlighting key findings, contributions,\nand future directions in knowledge distillation to provide insights for\nresearchers and practitioners on its evolving role in artificial intelligence\nand machine learning."}
{"id": "2504.13828", "pdf": "https://arxiv.org/pdf/2504.13828", "abs": "https://arxiv.org/abs/2504.13828", "authors": ["Shijie Xia", "Yiwei Qin", "Xuefeng Li", "Yan Ma", "Run-Ze Fan", "Steffi Chern", "Haoyang Zou", "Fan Zhou", "Xiangkun Hu", "Jiahe Jin", "Yanheng He", "Yixin Ye", "Yixiu Liu", "Pengfei Liu"], "title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The first generation of Large Language Models - what might be called \"Act I\"\nof generative AI (2020-2023) - achieved remarkable success through massive\nparameter and data scaling, yet exhibited fundamental limitations in knowledge\nlatency, shallow reasoning, and constrained cognitive processes. During this\nera, prompt engineering emerged as our primary interface with AI, enabling\ndialogue-level communication through natural language. We now witness the\nemergence of \"Act II\" (2024-present), where models are transitioning from\nknowledge-retrieval systems (in latent space) to thought-construction engines\nthrough test-time scaling techniques. This new paradigm establishes a\nmind-level connection with AI through language-based thoughts. In this paper,\nwe clarify the conceptual foundations of cognition engineering and explain why\nthis moment is critical for its development. We systematically break down these\nadvanced approaches through comprehensive tutorials and optimized\nimplementations, democratizing access to cognition engineering and enabling\nevery practitioner to participate in AI's second act. We provide a regularly\nupdated collection of papers on test-time scaling in the GitHub Repository:\nhttps://github.com/GAIR-NLP/cognition-engineering"}
{"id": "2504.13834", "pdf": "https://arxiv.org/pdf/2504.13834", "abs": "https://arxiv.org/abs/2504.13834", "authors": ["Muhan Gao", "Jash Shah", "Weiqi Wang", "Daniel Khashabi"], "title": "Science Hierarchography: Hierarchical Organization of Science Literature", "categories": ["cs.CL"], "comment": null, "summary": "Scientific knowledge is growing rapidly, making it challenging to track\nprogress and high-level conceptual links across broad disciplines. While\nexisting tools like citation networks and search engines make it easy to access\na few related papers, they fundamentally lack the flexible abstraction needed\nto represent the density of activity in various scientific subfields. We\nmotivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature\ninto a high-quality hierarchical structure that allows for the categorization\nof scientific work across varying levels of abstraction, from very broad fields\nto very specific studies. Such a representation can provide insights into which\nfields are well-explored and which are under-explored. To achieve the goals of\nSCIENCE HIERARCHOGRAPHY, we develop a range of algorithms. Our primary approach\ncombines fast embedding-based clustering with LLM-based prompting to balance\nthe computational efficiency of embedding methods with the semantic precision\noffered by LLM prompting. We demonstrate that this approach offers the best\ntrade-off between quality and speed compared to methods that heavily rely on\nLLM prompting, such as iterative tree construction with LLMs. To better reflect\nthe interdisciplinary and multifaceted nature of research papers, our hierarchy\ncaptures multiple dimensions of categorization beyond simple topic labels. We\nevaluate the utility of our framework by assessing how effectively an LLM-based\nagent can locate target papers using the hierarchy. Results show that this\nstructured approach enhances interpretability, supports trend discovery, and\noffers an alternative pathway for exploring scientific literature beyond\ntraditional search methods. Code, data and demo:\n$\\href{https://github.com/JHU-CLSP/science-hierarchography}{https://github.com/JHU-CLSP/science-hierarchography}$"}
{"id": "2504.13835", "pdf": "https://arxiv.org/pdf/2504.13835", "abs": "https://arxiv.org/abs/2504.13835", "authors": ["Yicheng Chen", "Yining Li", "Kai Hu", "Zerun Ma", "Haochen Ye", "Kai Chen"], "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Data quality and diversity are key to the construction of effective\ninstruction-tuning datasets. % With the increasing availability of open-source\ninstruction-tuning datasets, it is advantageous to automatically select\nhigh-quality and diverse subsets from a vast amount of data. % Existing methods\ntypically prioritize instance quality and use heuristic rules to maintain\ndiversity. % However, this absence of a comprehensive view of the entire\ncollection often leads to suboptimal results. % Moreover, heuristic rules\ngenerally focus on distance or clustering within the embedding space, which\nfails to accurately capture the intent of complex instructions in the semantic\nspace. % To bridge this gap, we propose a unified method for quantifying the\ninformation content of datasets. This method models the semantic space by\nconstructing a label graph and quantifies diversity based on the distribution\nof information within the graph. % Based on such a measurement, we further\nintroduce an efficient sampling method that selects data samples iteratively to\n\\textbf{M}aximize the \\textbf{I}nformation \\textbf{G}ain (MIG) in semantic\nspace. % Experiments on various datasets and base models demonstrate that MIG\nconsistently outperforms state-of-the-art methods. % Notably, the model\nfine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance\nto the official SFT model trained on the full dataset, with improvements of\n+5.73\\% on AlpacaEval and +6.89\\% on Wildbench."}
{"id": "2504.13202", "pdf": "https://arxiv.org/pdf/2504.13202", "abs": "https://arxiv.org/abs/2504.13202", "authors": ["Timo Aukusti Laine"], "title": "The Quantum LLM: Modeling Semantic Spaces with Quantum Principles", "categories": ["cs.AI", "cs.CL", "quant-ph"], "comment": "16 pages, 6 figures", "summary": "In the previous article, we presented a quantum-inspired framework for\nmodeling semantic representation and processing in Large Language Models\n(LLMs), drawing upon mathematical tools and conceptual analogies from quantum\nmechanics to offer a new perspective on these complex systems. In this paper,\nwe clarify the core assumptions of this model, providing a detailed exposition\nof six key principles that govern semantic representation, interaction, and\ndynamics within LLMs. The goal is to justify that a quantum-inspired framework\nis a valid approach to studying semantic spaces. This framework offers valuable\ninsights into their information processing and response generation, and we\nfurther discuss the potential of leveraging quantum computing to develop\nsignificantly more powerful and efficient LLMs based on these principles."}
{"id": "2504.13203", "pdf": "https://arxiv.org/pdf/2504.13203", "abs": "https://arxiv.org/abs/2504.13203", "authors": ["Salman Rahman", "Liwei Jiang", "James Shiffer", "Genglin Liu", "Sheriff Issaka", "Md Rizwan Parvez", "Hamid Palangi", "Kai-Wei Chang", "Yejin Choi", "Saadia Gabriel"], "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "Multi-turn interactions with language models (LMs) pose critical safety\nrisks, as harmful intent can be strategically spread across exchanges. Yet, the\nvast majority of prior work has focused on single-turn safety, while\nadaptability and diversity remain among the key challenges of multi-turn\nred-teaming. To address these challenges, we present X-Teaming, a scalable\nframework that systematically explores how seemingly harmless interactions\nescalate into harmful outcomes and generates corresponding attack scenarios.\nX-Teaming employs collaborative agents for planning, attack optimization, and\nverification, achieving state-of-the-art multi-turn jailbreak effectiveness and\ndiversity with success rates up to 98.1% across representative leading\nopen-weight and closed-source models. In particular, X-Teaming achieves a 96.2%\nattack success rate against the latest Claude 3.7 Sonnet model, which has been\nconsidered nearly immune to single-turn attacks. Building on X-Teaming, we\nintroduce XGuard-Train, an open-source multi-turn safety training dataset that\nis 20x larger than the previous best resource, comprising 30K interactive\njailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our\nwork offers essential tools and insights for mitigating sophisticated\nconversational attacks, advancing the multi-turn safety of LMs."}
{"id": "2504.13277", "pdf": "https://arxiv.org/pdf/2504.13277", "abs": "https://arxiv.org/abs/2504.13277", "authors": ["Soorya Ram Shimgekar", "Violeta J. Rodriguez", "Paul A. Bloom", "Dong Whi Yoo", "Koustuv Saha"], "title": "Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "Suicide is a critical global public health issue, with millions experiencing\nsuicidal ideation (SI) each year. Online spaces enable individuals to express\nSI and seek peer support. While prior research has revealed the potential of\ndetecting SI using machine learning and natural language analysis, a key\nlimitation is the lack of a theoretical framework to understand the underlying\nfactors affecting high-risk suicidal intent. To bridge this gap, we adopted the\nInterpersonal Theory of Suicide (IPTS) as an analytic lens to analyze 59,607\nposts from Reddit's r/SuicideWatch, categorizing them into SI dimensions\n(Loneliness, Lack of Reciprocal Love, Self Hate, and Liability) and risk\nfactors (Thwarted Belongingness, Perceived Burdensomeness, and Acquired\nCapability of Suicide). We found that high-risk SI posts express planning and\nattempts, methods and tools, and weaknesses and pain. In addition, we also\nexamined the language of supportive responses through psycholinguistic and\ncontent analyses to find that individuals respond differently to different\nstages of Suicidal Ideation (SI) posts. Finally, we explored the role of AI\nchatbots in providing effective supportive responses to suicidal ideation\nposts. We found that although AI improved structural coherence, expert\nevaluations highlight persistent shortcomings in providing dynamic,\npersonalized, and deeply empathetic support. These findings underscore the need\nfor careful reflection and deeper understanding in both the development and\nconsideration of AI-driven interventions for effective mental health support."}
{"id": "2504.13308", "pdf": "https://arxiv.org/pdf/2504.13308", "abs": "https://arxiv.org/abs/2504.13308", "authors": ["Leena G Pillai", "D. Muhammad Noorul Mubarak"], "title": "Acoustic to Articulatory Inversion of Speech; Data Driven Approaches, Challenges, Applications, and Future Scope", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "This is a review paper about Acoustic to Articulatory inversion of\n  speech, presented in an international conference. This paper has 8 pages and\n  2 figures", "summary": "This review is focused on the data-driven approaches applied in different\napplications of Acoustic-to-Articulatory Inversion (AAI) of speech. This review\npaper considered the relevant works published in the last ten years\n(2011-2021). The selection criteria includes (a) type of AAI - Speaker\nDependent and Speaker Independent AAI, (b) objectives of the work -\nArticulatory approximation, Articulatory Feature space selection and Automatic\nSpeech Recognition (ASR), explore the correlation between acoustic and\narticulatory features, and framework for Computer-assisted language training,\n(c) Corpus - Simultaneously recorded speech (wav) and medical imaging models\nsuch as ElectroMagnetic Articulography (EMA), Electropalatography (EPG),\nLaryngography, Electroglottography (EGG), X-ray Cineradiography, Ultrasound,\nand real-time Magnetic Resonance Imaging (rtMRI), (d) Methods or models -\nrecent works are considered, and therefore all the works are based on machine\nlearning, (e) Evaluation - as AAI is a non-linear regression problem, the\nperformance evaluation is mostly done by Correlation Coefficient (CC), Root\nMean Square Error (RMSE), and also considered Mean Square Error (MSE), and Mean\nFormat Error (MFE). The practical application of the AAI model can provide a\nbetter and user-friendly interpretable image feedback system of articulatory\npositions, especially tongue movement. Such trajectory feedback system can be\nused to provide phonetic, language, and speech therapy for pathological\nsubjects."}
{"id": "2504.13359", "pdf": "https://arxiv.org/pdf/2504.13359", "abs": "https://arxiv.org/abs/2504.13359", "authors": ["Mehmet Hamza Erol", "Batu El", "Mirac Suzgun", "Mert Yuksekgonul", "James Zou"], "title": "Cost-of-Pass: An Economic Framework for Evaluating Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "Code is available at: https://github.com/mhamzaerol/Cost-of-Pass", "summary": "The widespread adoption of AI systems in the economy hinges on their ability\nto generate economic value that outweighs their inference costs. Evaluating\nthis tradeoff requires metrics that account for both performance and costs. We\npropose a framework grounded in production theory for evaluating language\nmodels by combining accuracy and inference cost. We introduce \"cost-of-pass\",\nthe expected monetary cost of generating a correct solution. We then define the\n\"frontier cost-of-pass\" as the minimum cost-of-pass achievable across available\nmodels or the \"human-expert, using the approximate cost of hiring an expert.\nOur analysis reveals distinct economic insights. First, lightweight models are\nmost cost-effective for basic quantitative tasks, large models for\nknowledge-intensive ones, and reasoning models for complex quantitative\nproblems, despite higher per-token costs. Second, tracking this frontier\ncost-of-pass over the past year reveals significant progress, particularly for\ncomplex quantitative tasks where the cost has roughly halved every few months.\nThird, to trace key innovations driving this progress, we examine\ncounterfactual frontiers: estimates of cost-efficiency without specific model\nclasses. We find that innovations in lightweight, large, and reasoning models\nhave been essential for pushing the frontier in basic quantitative,\nknowledge-intensive, and complex quantitative tasks, respectively. Finally, we\nassess the cost-reductions afforded by common inference-time techniques like\nmajority voting and self-refinement, finding that their marginal accuracy gains\nrarely justify their costs. Our findings underscore that complementary\nmodel-level innovations are the primary drivers of cost-efficiency, and our\neconomic framework provides a principled tool for measuring this progress and\nguiding deployment."}
{"id": "2504.13388", "pdf": "https://arxiv.org/pdf/2504.13388", "abs": "https://arxiv.org/abs/2504.13388", "authors": ["Yegor Klochkov"], "title": "A mean teacher algorithm for unlearning of language models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "One of the goals of language model unlearning is to reduce memorization of\nselected text instances while retaining the model's general abilities. Despite\nvarious proposed methods, reducing memorization of large datasets without\nnoticeable degradation in model utility remains challenging. In this paper, we\ninvestigate the mean teacher algorithm (Tarvainen & Valpola, 2017), a simple\nproximal optimization method from continual learning literature that gradually\nmodifies the teacher model. We show that the mean teacher can approximate a\ntrajectory of a slow natural gradient descent (NGD), which inherently seeks\nlow-curvature updates that are less likely to degrade the model utility. While\nslow NGD can suffer from vanishing gradients, we introduce a new unlearning\nloss called \"negative log-unlikelihood\" (NLUL) that avoids this problem. We\nshow that the combination of mean teacher and NLUL improves some metrics on the\nMUSE benchmarks (Shi et al., 2024)."}
{"id": "2504.13406", "pdf": "https://arxiv.org/pdf/2504.13406", "abs": "https://arxiv.org/abs/2504.13406", "authors": ["Xiangbo Gao", "Yuheng Wu", "Rujia Wang", "Chenxi Liu", "Yang Zhou", "Zhengzhong Tu"], "title": "LangCoop: Collaborative Driving with Language", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multi-agent collaboration holds great promise for enhancing the safety,\nreliability, and mobility of autonomous driving systems by enabling information\nsharing among multiple connected agents. However, existing multi-agent\ncommunication approaches are hindered by limitations of existing communication\nmedia, including high bandwidth demands, agent heterogeneity, and information\nloss. To address these challenges, we introduce LangCoop, a new paradigm for\ncollaborative autonomous driving that leverages natural language as a compact\nyet expressive medium for inter-agent communication. LangCoop features two key\ninnovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured\nzero-shot vision-language reasoning and Natural Language Information Packaging\n(LangPack) for efficiently packaging information into concise, language-based\nmessages. Through extensive experiments conducted in the CARLA simulations, we\ndemonstrate that LangCoop achieves a remarkable 96\\% reduction in communication\nbandwidth (< 2KB per message) compared to image-based communication, while\nmaintaining competitive driving performance in the closed-loop evaluation."}
{"id": "2504.13416", "pdf": "https://arxiv.org/pdf/2504.13416", "abs": "https://arxiv.org/abs/2504.13416", "authors": ["Saksham Rastogi", "Pratyush Maini", "Danish Pruthi"], "title": "STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": "Accepted at DATA-FM, WMark @ ICLR 2025. Project page at see\n  https://codeboy5.github.io/stamp", "summary": "Given how large parts of publicly available text are crawled to pretrain\nlarge language models (LLMs), data creators increasingly worry about the\ninclusion of their proprietary data for model training without attribution or\nlicensing. Their concerns are also shared by benchmark curators whose test-sets\nmight be compromised. In this paper, we present STAMP, a framework for\ndetecting dataset membership-i.e., determining the inclusion of a dataset in\nthe pretraining corpora of LLMs. Given an original piece of content, our\nproposal involves first generating multiple rephrases, each embedding a\nwatermark with a unique secret key. One version is to be released publicly,\nwhile others are to be kept private. Subsequently, creators can compare model\nlikelihoods between public and private versions using paired statistical tests\nto prove membership. We show that our framework can successfully detect\ncontamination across four benchmarks which appear only once in the training\ndata and constitute less than 0.001% of the total tokens, outperforming several\ncontamination detection and dataset inference baselines. We verify that STAMP\npreserves both the semantic meaning and the utility of the original data in\ncomparing different models. We apply STAMP to two real-world scenarios to\nconfirm the inclusion of paper abstracts and blog articles in the pretraining\ncorpora."}
{"id": "2504.13472", "pdf": "https://arxiv.org/pdf/2504.13472", "abs": "https://arxiv.org/abs/2504.13472", "authors": ["Xinchen Wang", "Pengfei Gao", "Chao Peng", "Ruida Hu", "Cuiyun Gao"], "title": "CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong capabilities in code\ngeneration, underscoring the critical need for rigorous and comprehensive\nevaluation. Existing evaluation approaches fall into three categories,\nincluding human-centered, metric-based, and LLM-based. Considering that\nhuman-centered approaches are labour-intensive and metric-based ones overly\nrely on reference answers, LLM-based approaches are gaining increasing\nattention due to their stronger contextual understanding capabilities and\nsuperior efficiency. However, the performance of LLM-based approaches remains\nlimited due to: (1) lack of multisource domain knowledge, and (2) insufficient\ncomprehension of complex code.\n  To mitigate the limitations, we propose CodeVisionary, the first LLM-based\nagent framework for evaluating LLMs in code generation. CodeVisionary consists\nof two stages: (1) Multiscore knowledge analysis stage, which aims to gather\nmultisource and comprehensive domain knowledge by formulating and executing a\nstepwise evaluation plan. (2) Negotiation-based scoring stage, which involves\nmultiple judges engaging in discussions to better comprehend the complex code\nand reach a consensus on the evaluation score. Extensive experiments\ndemonstrate that CodeVisionary achieves the best performance for evaluating\nLLMs in code generation, outperforming the best baseline methods with average\nimprovements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau\ncoefficients, respectively. Besides, CodeVisionary provides detailed evaluation\nreports, which assist developers in identifying shortcomings and making\nimprovements. The resources of CodeVisionary are available at\nhttps://anonymous.4open.science/r/CodeVisionary."}
{"id": "2504.13480", "pdf": "https://arxiv.org/pdf/2504.13480", "abs": "https://arxiv.org/abs/2504.13480", "authors": ["Minsu Koh", "Beom-Chul Park", "Heejo Kong", "Seong-Whan Lee"], "title": "Integrating Locality-Aware Attention with Transformers for General Geometry PDEs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by IJCNN 2025", "summary": "Neural operators have emerged as promising frameworks for learning mappings\ngoverned by partial differential equations (PDEs), serving as data-driven\nalternatives to traditional numerical methods. While methods such as the\nFourier neural operator (FNO) have demonstrated notable performance, their\nreliance on uniform grids restricts their applicability to complex geometries\nand irregular meshes. Recently, Transformer-based neural operators with linear\nattention mechanisms have shown potential in overcoming these limitations for\nlarge-scale PDE simulations. However, these approaches predominantly emphasize\nglobal feature aggregation, often overlooking fine-scale dynamics and localized\nPDE behaviors essential for accurate solutions. To address these challenges, we\npropose the Locality-Aware Attention Transformer (LA2Former), which leverages\nK-nearest neighbors for dynamic patchifying and integrates global-local\nattention for enhanced PDE modeling. By combining linear attention for\nefficient global context encoding with pairwise attention for capturing\nintricate local interactions, LA2Former achieves an optimal balance between\ncomputational efficiency and predictive accuracy. Extensive evaluations across\nsix benchmark datasets demonstrate that LA2Former improves predictive accuracy\nby over 50% relative to existing linear attention methods, while also\noutperforming full pairwise attention under optimal conditions. This work\nunderscores the critical importance of localized feature learning in advancing\nTransformer-based neural operators for solving PDEs on complex and irregular\ndomains."}
{"id": "2504.13551", "pdf": "https://arxiv.org/pdf/2504.13551", "abs": "https://arxiv.org/abs/2504.13551", "authors": ["CheolWon Na", "YunSeok Choi", "Jee-Hyong Lee"], "title": "Q-FAKER: Query-free Hard Black-box Attack via Controlled Generation", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "NAACL 2025 Findings", "summary": "Many adversarial attack approaches are proposed to verify the vulnerability\nof language models. However, they require numerous queries and the information\non the target model. Even black-box attack methods also require the target\nmodel's output information. They are not applicable in real-world scenarios, as\nin hard black-box settings where the target model is closed and inaccessible.\nEven the recently proposed hard black-box attacks still require many queries\nand demand extremely high costs for training adversarial generators. To address\nthese challenges, we propose Q-faker (Query-free Hard Black-box Attacker), a\nnovel and efficient method that generates adversarial examples without\naccessing the target model. To avoid accessing the target model, we use a\nsurrogate model instead. The surrogate model generates adversarial sentences\nfor a target-agnostic attack. During this process, we leverage controlled\ngeneration techniques. We evaluate our proposed method on eight datasets.\nExperimental results demonstrate our method's effectiveness including high\ntransferability and the high quality of the generated adversarial examples, and\nprove its practical in hard black-box settings."}
{"id": "2504.13644", "pdf": "https://arxiv.org/pdf/2504.13644", "abs": "https://arxiv.org/abs/2504.13644", "authors": ["Gabriel Freedman", "Francesca Toni"], "title": "Exploring the Potential for Large Language Models to Demonstrate Rational Probabilistic Beliefs", "categories": ["cs.AI", "cs.CL"], "comment": "8 pages, 4 figures", "summary": "Advances in the general capabilities of large language models (LLMs) have led\nto their use for information retrieval, and as components in automated decision\nsystems. A faithful representation of probabilistic reasoning in these models\nmay be essential to ensure trustworthy, explainable and effective performance\nin these tasks. Despite previous work suggesting that LLMs can perform complex\nreasoning and well-calibrated uncertainty quantification, we find that current\nversions of this class of model lack the ability to provide rational and\ncoherent representations of probabilistic beliefs. To demonstrate this, we\nintroduce a novel dataset of claims with indeterminate truth values and apply a\nnumber of well-established techniques for uncertainty quantification to measure\nthe ability of LLM's to adhere to fundamental properties of probabilistic\nreasoning."}
{"id": "2504.13667", "pdf": "https://arxiv.org/pdf/2504.13667", "abs": "https://arxiv.org/abs/2504.13667", "authors": ["Russell Beale"], "title": "Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "Accepted for IDC 2025. Citation: Russell Beale. 2025. Large Language\n  Models Will Change The Way Children Think About Technology And Impact Every\n  Interaction Paradigm. In Proceedings of Interaction Design and Children\n  Conference (IDC2025). ACM, New York, NY, USA", "summary": "This paper presents a hopeful perspective on the potentially dramatic impacts\nof Large Language Models on how we children learn and how they will expect to\ninteract with technology. We review the effects of LLMs on education so far,\nand make the case that these effects are minor compared to the upcoming changes\nthat are occurring. We present a small scenario and self-ethnographic study\ndemonstrating the effects of these changes, and define five significant\nconsiderations that interactive systems designers will have to accommodate in\nthe future."}
{"id": "2504.13707", "pdf": "https://arxiv.org/pdf/2504.13707", "abs": "https://arxiv.org/abs/2504.13707", "authors": ["Yichen Wu", "Xudong Pan", "Geng Hong", "Min Yang"], "title": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As the general capabilities of large language models (LLMs) improve and agent\napplications become more widespread, the underlying deception risks urgently\nrequire systematic evaluation and effective oversight. Unlike existing\nevaluation which uses simulated games or presents limited choices, we introduce\nOpenDeception, a novel deception evaluation framework with an open-ended\nscenario dataset. OpenDeception jointly evaluates both the deception intention\nand capabilities of LLM-based agents by inspecting their internal reasoning\nprocess. Specifically, we construct five types of common use cases where LLMs\nintensively interact with the user, each consisting of ten diverse, concrete\nscenarios from the real world. To avoid ethical concerns and costs of high-risk\ndeceptive interactions with human testers, we propose to simulate the\nmulti-turn dialogue via agent simulation. Extensive evaluation of eleven\nmainstream LLMs on OpenDeception highlights the urgent need to address\ndeception risks and security concerns in LLM-based agents: the deception\nintention ratio across the models exceeds 80%, while the deception success rate\nsurpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do\nexhibit a higher risk of deception, which calls for more alignment efforts on\ninhibiting deceptive behaviors."}
{"id": "2504.13752", "pdf": "https://arxiv.org/pdf/2504.13752", "abs": "https://arxiv.org/abs/2504.13752", "authors": ["Benjamin Cohen-Wang", "Yung-Sung Chuang", "Aleksander Madry"], "title": "Learning to Attribute with Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Given a sequence of tokens generated by a language model, we may want to\nidentify the preceding tokens that influence the model to generate this\nsequence. Performing such token attribution is expensive; a common approach is\nto ablate preceding tokens and directly measure their effects. To reduce the\ncost of token attribution, we revisit attention weights as a heuristic for how\na language model uses previous tokens. Naive approaches to attribute model\nbehavior with attention (e.g., averaging attention weights across attention\nheads to estimate a token's influence) have been found to be unreliable. To\nattain faithful attributions, we propose treating the attention weights of\ndifferent attention heads as features. This way, we can learn how to\neffectively leverage attention weights for attribution (using signal from\nablations). Our resulting method, Attribution with Attention (AT2), reliably\nperforms on par with approaches that involve many ablations, while being\nsignificantly more efficient. To showcase the utility of AT2, we use it to\nprune less important parts of a provided context in a question answering\nsetting, improving answer quality. We provide code for AT2 at\nhttps://github.com/MadryLab/AT2 ."}
{"id": "2504.13756", "pdf": "https://arxiv.org/pdf/2504.13756", "abs": "https://arxiv.org/abs/2504.13756", "authors": ["Dmitrii Kharlapenko", "Stepan Shabalin", "Fazl Barez", "Arthur Conmy", "Neel Nanda"], "title": "Scaling sparse feature circuit finding for in-context learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse autoencoders (SAEs) are a popular tool for interpreting large language\nmodel activations, but their utility in addressing open questions in\ninterpretability remains unclear. In this work, we demonstrate their\neffectiveness by using SAEs to deepen our understanding of the mechanism behind\nin-context learning (ICL). We identify abstract SAE features that (i) encode\nthe model's knowledge of which task to execute and (ii) whose latent vectors\ncausally induce the task zero-shot. This aligns with prior work showing that\nICL is mediated by task vectors. We further demonstrate that these task vectors\nare well approximated by a sparse sum of SAE latents, including these\ntask-execution features. To explore the ICL mechanism, we adapt the sparse\nfeature circuits methodology of Marks et al. (2024) to work for the much larger\nGemma-1 2B model, with 30 times as many parameters, and to the more complex\ntask of ICL. Through circuit finding, we discover task-detecting features with\ncorresponding SAE latents that activate earlier in the prompt, that detect when\ntasks have been performed. They are causally linked with task-execution\nfeatures through the attention and MLP sublayers."}
{"id": "2504.13818", "pdf": "https://arxiv.org/pdf/2504.13818", "abs": "https://arxiv.org/abs/2504.13818", "authors": ["Yixuan Even Xu", "Yash Savani", "Fei Fang", "Zico Kolter"], "title": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages, 1 figure", "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing\nreasoning capabilities in large language models, but faces a fundamental\nasymmetry in computation and memory requirements: inference is embarrassingly\nparallel with a minimal memory footprint, while policy updates require\nextensive synchronization and are memory-intensive. To address this asymmetry,\nwe introduce PODS (Policy Optimization with Down-Sampling), a framework that\nstrategically decouples these phases by generating numerous rollouts in\nparallel but updating only on an informative subset. Within this framework, we\ndevelop max-variance down-sampling, a theoretically motivated method that\nselects rollouts with maximally diverse reward signals. We prove that this\napproach has an efficient algorithmic solution, and empirically demonstrate\nthat GRPO with PODS using max-variance down-sampling achieves superior\nperformance over standard GRPO on the GSM8K benchmark."}
{"id": "2504.13837", "pdf": "https://arxiv.org/pdf/2504.13837", "abs": "https://arxiv.org/abs/2504.13837", "authors": ["Yang Yue", "Zhiqi Chen", "Rui Lu", "Andrew Zhao", "Zhaokai Wang", "Yang Yue", "Shiji Song", "Gao Huang"], "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "24 pages, 19 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@\\textit{k} metric with large values of \\textit{k} to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does \\emph{not}, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of $k$ (\\eg, $k$=1), base models can achieve a comparable or\neven higher pass@$k$ score compared to their RL counterparts at large $k$\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io"}
