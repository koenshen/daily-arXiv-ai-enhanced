{"id": "2504.16956", "pdf": "https://arxiv.org/pdf/2504.16956", "abs": "https://arxiv.org/abs/2504.16956", "authors": ["Cong Qi", "Hanzhang Fang", "Tianxing Hu", "Siqi Jiang", "Wei Zhi"], "title": "Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity", "categories": ["cs.CL", "cs.LG", "q-bio.GN"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of\ncellular heterogeneity, but its complexity, which is marked by high\ndimensionality, sparsity, and batch effects, which poses major computational\nchallenges. Transformer-based models have made significant advances in this\ndomain but are often limited by their quadratic complexity and suboptimal\nhandling of long-range dependencies. In this work, we introduce GeneMamba, a\nscalable and efficient foundation model for single-cell transcriptomics built\non state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba\ncaptures bidirectional gene context with linear-time complexity, offering\nsubstantial computational gains over transformer baselines. The model is\npretrained on nearly 30 million cells and incorporates biologically informed\nobjectives, including pathway-aware contrastive loss and rank-based gene\nencoding. We evaluate GeneMamba across diverse tasks, including multi-batch\nintegration, cell type annotation, and gene-gene correlation, demonstrating\nstrong performance, interpretability, and robustness. These results position\nGeneMamba as a practical and powerful alternative to transformer-based methods,\nadvancing the development of biologically grounded, scalable tools for\nlarge-scale single-cell data analysis."}
{"id": "2504.16977", "pdf": "https://arxiv.org/pdf/2504.16977", "abs": "https://arxiv.org/abs/2504.16977", "authors": ["Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Amit Agarwal"], "title": "Tokenization Matters: Improving Zero-Shot NER for Indic Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is a critical component of Natural Language Processing (NLP),\nespecially for low resource languages, where subword segmentation influences\nvocabulary structure and downstream task accuracy. Although Byte Pair Encoding\n(BPE) is a standard tokenization method in multilingual language models, its\nsuitability for Named Entity Recognition (NER) in low resource Indic languages\nremains underexplored due to its limitations in handling morphological\ncomplexity. In this work, we systematically compare BPE, SentencePiece, and\nCharacter Level tokenization strategies using IndicBERT for NER tasks in low\nresource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as\nextremely low resource Indic languages like Santali, Manipuri, and Sindhi. We\nassess both intrinsic linguistic properties tokenization efficiency, out of\nvocabulary (OOV) rates, and morphological preservation as well as extrinsic\ndownstream performance, including fine tuning and zero shot cross lingual\ntransfer.\n  Our experiments show that SentencePiece is a consistently better performing\napproach than BPE for NER in low resource Indic Languages, particularly in zero\nshot cross lingual settings, as it better preserves entity consistency. While\nBPE provides the most compact tokenization form, it is not capable of\ngeneralization because it misclassifies or even fails to recognize entity\nlabels when tested on unseen languages. In contrast, SentencePiece constitutes\na better linguistic structural preservation model, benefiting extremely low\nresource and morphologically rich Indic languages, such as Santali and\nManipuri, for superior entity recognition, as well as high generalization\nacross scripts, such as Sindhi, written in Arabic. The results point to\nSentencePiece as the more effective tokenization strategy for NER within\nmultilingual and low resource Indic NLP applications."}
{"id": "2504.17025", "pdf": "https://arxiv.org/pdf/2504.17025", "abs": "https://arxiv.org/abs/2504.17025", "authors": ["Luca Moroni", "Giovanni Puccetti", "Pere-Lluis Huguet Cabot", "Andrei Stefan Bejgu", "Edoardo Barba", "Alessio Miaschi", "Felice Dell'Orletta", "Andrea Esuli", "Roberto Navigli"], "title": "Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "The number of pretrained Large Language Models (LLMs) is increasing steadily,\nthough the majority are designed predominantly for the English language. While\nstate-of-the-art LLMs can handle other languages, due to language contamination\nor some degree of multilingual pretraining data, they are not optimized for\nnon-English languages, leading to inefficient encoding (high token \"fertility\")\nand slower inference speed. In this work, we thoroughly compare a variety of\nvocabulary adaptation techniques for optimizing English LLMs for the Italian\nlanguage, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a\nnovel method that leverages neural mapping for vocabulary substitution. SAVA\nachieves competitive performance across multiple downstream tasks, enhancing\ngrounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing\ntoken fertility by 25\\%, and Llama-3.1-8B, optimizing the vocabulary and\nreducing the number of parameters by 1 billion. We show that, following the\nadaptation of the vocabulary, these models can recover their performance with a\nrelatively limited stage of continual training on the target language. Finally,\nwe test the capabilities of the adapted models on various multi-choice and\ngenerative tasks."}
{"id": "2504.17052", "pdf": "https://arxiv.org/pdf/2504.17052", "abs": "https://arxiv.org/abs/2504.17052", "authors": ["Shariar Kabir", "Kevin Esterling", "Yue Dong"], "title": "Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models", "categories": ["cs.CL"], "comment": "20 pages, 9 figures", "summary": "Large Language Models (LLMs) are increasingly shaping political discourse,\nyet their responses often display inconsistency when subjected to scrutiny.\nWhile prior research has primarily categorized LLM outputs as left- or\nright-leaning to assess their political stances, a critical question remains:\nDo these responses reflect genuine internal beliefs or merely surface-level\nalignment with training data? To address this, we propose a novel framework for\nevaluating belief depth by analyzing (1) argumentative consistency and (2)\nuncertainty quantification. We evaluate 12 LLMs on 19 economic policies from\nthe Political Compass Test, challenging their belief stability with both\nsupportive and opposing arguments. Our analysis reveals that LLMs exhibit\ntopic-specific belief stability rather than a uniform ideological stance.\nNotably, up to 95% of left-leaning models' responses and 89% of right-leaning\nmodels' responses remain consistent under the challenge, enabling semantic\nentropy to achieve high accuracy (AUROC=0.78), effectively distinguishing\nbetween surface-level alignment from genuine belief. These findings call into\nquestion the assumption that LLMs maintain stable, human-like political\nideologies, emphasizing the importance of conducting topic-specific reliability\nassessments for real-world applications."}
{"id": "2504.17075", "pdf": "https://arxiv.org/pdf/2504.17075", "abs": "https://arxiv.org/abs/2504.17075", "authors": ["Arjun Subramonian", "Vagrant Gautam", "Preethi Seshadri", "Dietrich Klakow", "Kai-Wei Chang", "Yizhou Sun"], "title": "Agree to Disagree? A Meta-Evaluation of LLM Misgendering", "categories": ["cs.CL", "cs.CY"], "comment": "Work in progress", "summary": "Numerous methods have been proposed to measure LLM misgendering, including\nprobability-based evaluations (e.g., automatically with templatic sentences)\nand generation-based evaluations (e.g., with automatic heuristics or human\nvalidation). However, it has gone unexamined whether these evaluation methods\nhave convergent validity, that is, whether their results align. Therefore, we\nconduct a systematic meta-evaluation of these methods across three existing\ndatasets for LLM misgendering. We propose a method to transform each dataset to\nenable parallel probability- and generation-based evaluation. Then, by\nautomatically evaluating a suite of 6 models from 3 families, we find that\nthese methods can disagree with each other at the instance, dataset, and model\nlevels, conflicting on 20.2% of evaluation instances. Finally, with a human\nevaluation of 2400 LLM generations, we show that misgendering behaviour is\ncomplex and goes far beyond pronouns, which automatic evaluations are not\ncurrently designed to capture, suggesting essential disagreement with human\nevaluations. Based on our findings, we provide recommendations for future\nevaluations of LLM misgendering. Our results are also more widely relevant, as\nthey call into question broader methodological conventions in LLM evaluation,\nwhich often assume that different evaluation methods agree."}
{"id": "2504.17083", "pdf": "https://arxiv.org/pdf/2504.17083", "abs": "https://arxiv.org/abs/2504.17083", "authors": ["Rendi Chevi", "Kentaro Inui", "Thamar Solorio", "Alham Fikri Aji"], "title": "How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study", "categories": ["cs.CL"], "comment": "Accepted at GenAICHI 2025 @ ACM CHI 2025", "summary": "What makes an interaction with the LLM more preferable for the user? While it\nis intuitive to assume that information accuracy in the LLM's responses would\nbe one of the influential variables, recent studies have found that inaccurate\nLLM's responses could still be preferable when they are perceived to be more\nauthoritative, certain, well-articulated, or simply verbose. These variables\ninterestingly fall under the broader category of language style, implying that\nthe style in the LLM's responses might meaningfully influence users'\npreferences. This hypothesized dynamic could have double-edged consequences:\nenhancing the overall user experience while simultaneously increasing their\nsusceptibility to risks such as LLM's misinformation or hallucinations. In this\nshort paper, we present our preliminary studies in exploring this subject.\nThrough a series of exploratory and experimental user studies, we found that\nLLM's language style does indeed influence user's preferences, but how and\nwhich language styles influence the preference varied across different user\npopulations, and more interestingly, moderated by the user's very own\nindividual traits. As a preliminary work, the findings in our studies should be\ninterpreted with caution, particularly given the limitations in our samples,\nwhich still need wider demographic diversity and larger sample sizes. Our\nfuture directions will first aim to address these limitations, which would\nenable a more comprehensive joint effect analysis between the language style,\nindividual traits, and preferences, and further investigate the potential\ncausal relationship between and beyond these variables."}
{"id": "2504.17091", "pdf": "https://arxiv.org/pdf/2504.17091", "abs": "https://arxiv.org/abs/2504.17091", "authors": ["Seunghyun Yoo"], "title": "Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning", "categories": ["cs.CL", "68T05"], "comment": "5 page", "summary": "Due to the proliferation of short-form content and the rapid adoption of AI,\nopportunities for deep, reflective thinking have significantly diminished,\nundermining users' critical thinking and reducing engagement with the reasoning\nbehind AI-generated outputs. To address this issue, we propose an Interactive\nChain-of-Thought (CoT) Framework that enhances human-centered explainability\nand responsible AI usage by making the model's inference process transparent,\nmodular, and user-editable. The framework decomposes reasoning into clearly\ndefined blocks that users can inspect, modify, and re-execute, encouraging\nactive cognitive engagement rather than passive consumption. It further\nintegrates a lightweight edit-adaptation mechanism inspired by preference\nlearning, allowing the system to align with diverse cognitive styles and user\nintentions. Ethical transparency is ensured through explicit metadata\ndisclosure, built-in bias checkpoint functionality, and privacy-preserving\nsafeguards. This work outlines the design principles and architecture necessary\nto promote critical engagement, responsible interaction, and inclusive\nadaptation in AI systems aimed at addressing complex societal challenges."}
{"id": "2504.17119", "pdf": "https://arxiv.org/pdf/2504.17119", "abs": "https://arxiv.org/abs/2504.17119", "authors": ["Muskan Garg", "Shaina Raza", "Shebuti Rayana", "Xingyi Liu", "Sunghwan Sohn"], "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 7 tables, 5 figures", "summary": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github"}
{"id": "2504.17130", "pdf": "https://arxiv.org/pdf/2504.17130", "abs": "https://arxiv.org/abs/2504.17130", "authors": ["Hannah Cyberey", "David Evans"], "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control", "categories": ["cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have transformed the way we access information.\nThese models are often tuned to refuse to comply with requests that are\nconsidered harmful and to produce responses that better align with the\npreferences of those who control the models. To understand how this\n\"censorship\" works. We use representation engineering techniques to study\nopen-weights safety-tuned models. We present a method for finding a\nrefusal--compliance vector that detects and controls the level of censorship in\nmodel outputs. We also analyze recent reasoning LLMs, distilled from\nDeepSeek-R1, and uncover an additional dimension of censorship through \"thought\nsuppression\". We show a similar approach can be used to find a vector that\nsuppresses the model's reasoning process, allowing us to remove censorship by\napplying the negative multiples of this vector"}
{"id": "2504.17137", "pdf": "https://arxiv.org/pdf/2504.17137", "abs": "https://arxiv.org/abs/2504.17137", "authors": ["Chanhee Park", "Hyeonseok Moon", "Chanjun Park", "Heuiseok Lim"], "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL2025 Findings", "summary": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective\nmethod for enhancing the generative capabilities of Large Language Models\n(LLMs) through the incorporation of external knowledge. However, the evaluation\nof RAG systems remains a challenge, due to the intricate interplay between\nretrieval and generation components. This limitation has resulted in a scarcity\nof benchmarks that facilitate a detailed, component-specific assessment. In\nthis work, we present MIRAGE, a Question Answering dataset specifically\ndesigned for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped\nto a retrieval pool of 37,800 entries, enabling an efficient and precise\nevaluation of both retrieval and generation tasks. We also introduce novel\nevaluation metrics aimed at measuring RAG adaptability, encompassing dimensions\nsuch as noise vulnerability, context acceptability, context insensitivity, and\ncontext misinterpretation. Through comprehensive experiments across various\nretriever-LLM configurations, we provide new insights into the optimal\nalignment of model pairs and the nuanced dynamics within RAG systems. The\ndataset and evaluation code are publicly available, allowing for seamless\nintegration and customization in diverse research settings\\footnote{The MIRAGE\ncode and data are available at https://github.com/nlpai-lab/MIRAGE."}
{"id": "2504.17192", "pdf": "https://arxiv.org/pdf/2504.17192", "abs": "https://arxiv.org/abs/2504.17192", "authors": ["Minju Seo", "Jinheon Baek", "Seongyun Lee", "Sung Ju Hwang"], "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning", "categories": ["cs.CL"], "comment": null, "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins."}
{"id": "2504.17200", "pdf": "https://arxiv.org/pdf/2504.17200", "abs": "https://arxiv.org/abs/2504.17200", "authors": ["Yangxinyu Xie", "Bowen Jiang", "Tanwi Mallick", "Joshua David Bergerson", "John K. Hutchison", "Duane R. Verner", "Jordan Branham", "M. Ross Alexander", "Robert B. Ross", "Yan Feng", "Leslie-Anne Levy", "Weijie Su", "Camillo J. Taylor"], "title": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are a transformational capability at the\nfrontier of artificial intelligence and machine learning that can support\ndecision-makers in addressing pressing societal challenges such as extreme\nnatural hazard events. As generalized models, LLMs often struggle to provide\ncontext-specific information, particularly in areas requiring specialized\nknowledge. In this work we propose a retrieval-augmented generation (RAG)-based\nmulti-agent LLM system to support analysis and decision-making in the context\nof natural hazards and extreme weather events. As a proof of concept, we\npresent WildfireGPT, a specialized system focused on wildfire hazards. The\narchitecture employs a user-centered, multi-agent design to deliver tailored\nrisk insights across diverse stakeholder groups. By integrating natural hazard\nand extreme weather projection data, observational datasets, and scientific\nliterature through an RAG framework, the system ensures both the accuracy and\ncontextual relevance of the information it provides. Evaluation across ten\nexpert-led case studies demonstrates that WildfireGPT significantly outperforms\nexisting LLM-based solutions for decision support."}
{"id": "2504.17220", "pdf": "https://arxiv.org/pdf/2504.17220", "abs": "https://arxiv.org/abs/2504.17220", "authors": ["Kaidong Feng", "Zhu Sun", "Jie Yang", "Hui Fang", "Xinghua Qu", "Wenyuan Liu"], "title": "Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "LLMs are increasingly explored for bundle generation, thanks to their\nreasoning capabilities and knowledge. However, deploying large-scale LLMs\nintroduces significant efficiency challenges, primarily high computational\ncosts during fine-tuning and inference due to their massive parameterization.\nKnowledge distillation (KD) offers a promising solution, transferring expertise\nfrom large teacher models to compact student models. This study systematically\ninvestigates knowledge distillation approaches for bundle generation, aiming to\nminimize computational demands while preserving performance. We explore three\ncritical research questions: (1) how does the format of KD impact bundle\ngeneration performance? (2) to what extent does the quantity of distilled\nknowledge influence performance? and (3) how do different ways of utilizing the\ndistilled knowledge affect performance? We propose a comprehensive KD framework\nthat (i) progressively extracts knowledge (patterns, rules, deep thoughts);\n(ii) captures varying quantities of distilled knowledge through different\nstrategies; and (iii) exploits complementary LLM adaptation techniques\n(in-context learning, supervised fine-tuning, combination) to leverage\ndistilled knowledge in small student models for domain-specific adaptation and\nenhanced efficiency. Extensive experiments provide valuable insights into how\nknowledge format, quantity, and utilization methodologies collectively shape\nLLM-based bundle generation performance, exhibiting KD's significant potential\nfor more efficient yet effective LLM-based bundle generation."}
{"id": "2504.17238", "pdf": "https://arxiv.org/pdf/2504.17238", "abs": "https://arxiv.org/abs/2504.17238", "authors": ["Jinfeng Zhou", "Yuxuan Chen", "Jianing Yin", "Yongkang Huang", "Yihan Shi", "Xikun Zhang", "Libiao Peng", "Rongsheng Zhang", "Tangjie Lv", "Zhipeng Hu", "Hongning Wang", "Minlie Huang"], "title": "Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Cognitive Restructuring (CR) is a psychotherapeutic process aimed at\nidentifying and restructuring an individual's negative thoughts, arising from\nmental health challenges, into more helpful and positive ones via multi-turn\ndialogues. Clinician shortage and stigma urge the development of human-LLM\ninteractive psychotherapy for CR. Yet, existing efforts implement CR via simple\ntext rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to\nalign with the psychotherapeutic process for effective CR. To address this gap,\nwe propose CRDial, a novel framework for CR, which creates multi-turn dialogues\nwith specifically designed identification and restructuring stages of negative\nthoughts, integrates sentence-level supportive conversation strategies, and\nadopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we\ndistill Crisp, a large-scale and high-quality bilingual dialogue dataset, from\nLLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and\n14B scales. Extensive human studies show the superiority of Crispers in\npointwise, pairwise, and intervention evaluations."}
{"id": "2504.17252", "pdf": "https://arxiv.org/pdf/2504.17252", "abs": "https://arxiv.org/abs/2504.17252", "authors": ["Ocheme Anthony Ekle", "Biswarup Das"], "title": "Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo", "categories": ["cs.CL", "cs.LG", "68T50, 68T01", "I.2.7; I.2.1"], "comment": "25 pages, 14 combined figures (19 total), includes horizontal\n  layouts. Submitted to arXiv for open access", "summary": "In this study, we develop Neural Machine Translation (NMT) and\nTransformer-based transfer learning models for English-to-Igbo translation - a\nlow-resource African language spoken by over 40 million people across Nigeria\nand West Africa. Our models are trained on a curated and benchmarked dataset\ncompiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,\nall verified by native language experts. We leverage Recurrent Neural Network\n(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated\nRecurrent Units (GRU), enhanced with attention mechanisms to improve\ntranslation accuracy. To further enhance performance, we apply transfer\nlearning using MarianNMT pre-trained models within the SimpleTransformers\nframework. Our RNN-based system achieves competitive results, closely matching\nexisting English-Igbo benchmarks. With transfer learning, we observe a\nperformance gain of +4.83 BLEU points, reaching an estimated translation\naccuracy of 70%. These findings highlight the effectiveness of combining RNNs\nwith transfer learning to address the performance gap in low-resource language\ntranslation tasks."}
{"id": "2504.17264", "pdf": "https://arxiv.org/pdf/2504.17264", "abs": "https://arxiv.org/abs/2504.17264", "authors": ["Zhaolu Kang", "Hongtian Cai", "Xiangyang Ji", "Jinzhe Li", "Nanfei Gu"], "title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively."}
{"id": "2504.17279", "pdf": "https://arxiv.org/pdf/2504.17279", "abs": "https://arxiv.org/abs/2504.17279", "authors": ["Xiuying Chen", "Tairan Wang", "Juexiao Zhou", "Zirui Song", "Xin Gao", "Xiangliang Zhang"], "title": "Evaluating and Mitigating Bias in AI-Based Medical Text Generation", "categories": ["cs.CL"], "comment": "12 pages, 8 figures, published in Nature Computational Science", "summary": "Artificial intelligence (AI) systems, particularly those based on deep\nlearning models, have increasingly achieved expert-level performance in medical\napplications. However, there is growing concern that such AI systems may\nreflect and amplify human bias, and reduce the quality of their performance in\nhistorically under-served populations. The fairness issue has attracted\nconsiderable research interest in the medical imaging classification field, yet\nit remains understudied in the text generation domain. In this study, we\ninvestigate the fairness problem in text generation within the medical field\nand observe significant performance discrepancies across different races,\nsexes, and age groups, including intersectional groups, various model scales,\nand different evaluation metrics. To mitigate this fairness issue, we propose\nan algorithm that selectively optimizes those underperformed groups to reduce\nbias. The selection rules take into account not only word-level accuracy but\nalso the pathology accuracy to the target reference, while ensuring that the\nentire process remains fully differentiable for effective model training. Our\nevaluations across multiple backbones, datasets, and modalities demonstrate\nthat our proposed algorithm enhances fairness in text generation without\ncompromising overall performance. Specifically, the disparities among various\ngroups across different metrics were diminished by more than 30% with our\nalgorithm, while the relative change in text generation accuracy was typically\nwithin 2%. By reducing the bias generated by deep learning models, our proposed\napproach can potentially alleviate concerns about the fairness and reliability\nof text generation diagnosis in medical domain.\n  Our code is publicly available to facilitate further research at\nhttps://github.com/iriscxy/GenFair."}
{"id": "2504.17309", "pdf": "https://arxiv.org/pdf/2504.17309", "abs": "https://arxiv.org/abs/2504.17309", "authors": ["Junyan Zhang", "Shuliang Liu", "Aiwei Liu", "Yubo Gao", "Jungang Li", "Xiaojie Gu", "Xuming Hu"], "title": "CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality", "categories": ["cs.CL"], "comment": "Published at the 1st workshop on GenAI Watermarking, collocated with\n  ICLR 2025", "summary": "Watermarking technology is a method used to trace the usage of content\ngenerated by large language models. Sentence-level watermarking aids in\npreserving the semantic integrity within individual sentences while maintaining\ngreater robustness. However, many existing sentence-level watermarking\ntechniques depend on arbitrary segmentation or generation processes to embed\nwatermarks, which can limit the availability of appropriate sentences. This\nlimitation, in turn, compromises the quality of the generated response. To\naddress the challenge of balancing high text quality with robust watermark\ndetection, we propose CoheMark, an advanced sentence-level watermarking\ntechnique that exploits the cohesive relationships between sentences for better\nlogical fluency. The core methodology of CoheMark involves selecting sentences\nthrough trained fuzzy c-means clustering and applying specific next sentence\nselection criteria. Experimental evaluations demonstrate that CoheMark achieves\nstrong watermark strength while exerting minimal impact on text quality."}
{"id": "2504.17311", "pdf": "https://arxiv.org/pdf/2504.17311", "abs": "https://arxiv.org/abs/2504.17311", "authors": ["Yulia Otmakhova", "Hung Thinh Truong", "Rahmad Mahendra", "Zenan Zhai", "Rongxin Zhu", "Daniel Beck", "Jey Han Lau"], "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors."}
{"id": "2504.17332", "pdf": "https://arxiv.org/pdf/2504.17332", "abs": "https://arxiv.org/abs/2504.17332", "authors": ["Zihan Wang", "Lu Yuan", "Zhengxuan Zhang", "Qing Zhao"], "title": "Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection", "categories": ["cs.CL"], "comment": null, "summary": "In the digital era, social media has become a major conduit for information\ndissemination, yet it also facilitates the rapid spread of misinformation.\nTraditional misinformation detection methods primarily focus on surface-level\nfeatures, overlooking the crucial roles of human empathy in the propagation\nprocess. To address this gap, we propose the Dual-Aspect Empathy Framework\n(DAE), which integrates cognitive and emotional empathy to analyze\nmisinformation from both the creator and reader perspectives. By examining\ncreators' cognitive strategies and emotional appeals, as well as simulating\nreaders' cognitive judgments and emotional responses using Large Language\nModels (LLMs), DAE offers a more comprehensive and human-centric approach to\nmisinformation detection. Moreover, we further introduce an empathy-aware\nfiltering mechanism to enhance response authenticity and diversity.\nExperimental results on benchmark datasets demonstrate that DAE outperforms\nexisting methods, providing a novel paradigm for multimodal misinformation\ndetection."}
{"id": "2504.17353", "pdf": "https://arxiv.org/pdf/2504.17353", "abs": "https://arxiv.org/abs/2504.17353", "authors": ["Chengguang Gan", "Sunbowen Lee", "Zhixi Cai", "Yanbin Wei", "Lei Zheng", "Yunhao Liang", "Shiwen Ni", "Tatsunori Mori"], "title": "M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction", "categories": ["cs.CL", "cs.CV", "cs.MM"], "comment": null, "summary": "Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection\nof information extraction and model interpretability. MRE aims to leverage the\nmutual understanding between tasks of different granularities, enhancing the\nperformance of both coarse-grained and fine-grained tasks through joint\nmodeling. While MRE has been explored and validated in the textual domain, its\napplicability to visual and multimodal domains remains unexplored. In this\nwork, we extend MRE to the multimodal information extraction domain for the\nfirst time. Specifically, we introduce a new task: Multimodal Mutual\nReinforcement Effect (M-MRE), and construct a corresponding dataset to support\nthis task. To address the challenges posed by M-MRE, we further propose a\nPrompt Format Adapter (PFA) that is fully compatible with various Large\nVision-Language Models (LVLMs). Experimental results demonstrate that MRE can\nalso be observed in the M-MRE task, a multimodal text-image understanding\nscenario. This provides strong evidence that MRE facilitates mutual gains\nacross three interrelated tasks, confirming its generalizability beyond the\ntextual domain."}
{"id": "2504.17360", "pdf": "https://arxiv.org/pdf/2504.17360", "abs": "https://arxiv.org/abs/2504.17360", "authors": ["Jose G. Moreno", "Jesus Lovon", "M'Rick Robin-Charlet", "Christine Damase-Michel", "Lynda Tamine"], "title": "PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning of Large Language Models (LLMs) has become the default practice\nfor improving model performance on a given task. However, performance\nimprovement comes at the cost of training on vast amounts of annotated data\nwhich could be sensitive leading to significant data privacy concerns. In\nparticular, the healthcare domain is one of the most sensitive domains exposed\nto data privacy issues. In this paper, we present PatientDx, a framework of\nmodel merging that allows the design of effective LLMs for health-predictive\ntasks without requiring fine-tuning nor adaptation on patient data. Our\nproposal is based on recently proposed techniques known as merging of LLMs and\naims to optimize a building block merging strategy. PatientDx uses a pivotal\nmodel adapted to numerical reasoning and tunes hyperparameters on examples\nbased on a performance metric but without training of the LLM on these data.\nExperiments using the mortality tasks of the MIMIC-IV dataset show improvements\nup to 7% in terms of AUROC when compared to initial models. Additionally, we\nconfirm that when compared to fine-tuned models, our proposal is less prone to\ndata leak problems without hurting performance. Finally, we qualitatively show\nthe capabilities of our proposal through a case study. Our best model is\npublicly available at https://huggingface.co/ Jgmorenof/mistral\\_merged\\_0\\_4."}
{"id": "2504.17366", "pdf": "https://arxiv.org/pdf/2504.17366", "abs": "https://arxiv.org/abs/2504.17366", "authors": ["Yongxuan Wu", "Runyu Chen", "Peiyu Liu", "Hongjin Qian"], "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench."}
{"id": "2504.17390", "pdf": "https://arxiv.org/pdf/2504.17390", "abs": "https://arxiv.org/abs/2504.17390", "authors": ["Jihyun Lee", "Yejin Jeon", "Seungyeon Seo", "Gary Geunbae Lee"], "title": "PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona", "categories": ["cs.CL"], "comment": "Accepted in NAACL 2025 main", "summary": "Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests\nthrough natural language interactions, yet existing systems often produce\ngeneric, monotonic responses that lack individuality and fail to adapt to\nusers' personal attributes. To address this, we introduce PicPersona-TOD, a\nnovel dataset that incorporates user images as part of the persona, enabling\npersonalized responses tailored to user-specific factors such as age or\nemotional context. This is facilitated by first impressions, dialogue\npolicy-guided prompting, and the use of external knowledge to reduce\nhallucinations. Human evaluations confirm that our dataset enhances user\nexperience, with personalized responses contributing to a more engaging\ninteraction. Additionally, we introduce a new NLG model, Pictor, which not only\npersonalizes responses, but also demonstrates robust performance across unseen\ndomains https://github.com/JihyunLee1/PicPersona."}
{"id": "2504.17445", "pdf": "https://arxiv.org/pdf/2504.17445", "abs": "https://arxiv.org/abs/2504.17445", "authors": ["Anna Lieb", "Maneesh Arora", "Eni Mustafaraj"], "title": "Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation", "categories": ["cs.CL"], "comment": "Presented at IC2S2 2024 in Philadelphia, USA", "summary": "Unsupervised machine learning techniques, such as topic modeling and\nclustering, are often used to identify latent patterns in unstructured text\ndata in fields such as political science and sociology. These methods overcome\ncommon concerns about reproducibility and costliness involved in the\nlabor-intensive process of human qualitative analysis. However, two major\nlimitations of topic models are their interpretability and their practicality\nfor answering targeted, domain-specific social science research questions. In\nthis work, we investigate opportunities for using LLM-generated text\naugmentation to improve the usefulness of topic modeling output. We use a\npolitical science case study to evaluate our results in a domain-specific\napplication, and find that topic modeling using GPT-4 augmentations creates\nhighly interpretable categories that can be used to investigate domain-specific\nresearch questions with minimal human guidance."}
{"id": "2504.17480", "pdf": "https://arxiv.org/pdf/2504.17480", "abs": "https://arxiv.org/abs/2504.17480", "authors": ["Xin Yi", "Shunfan Zhengc", "Linlin Wanga", "Xiaoling Wang", "Liang He"], "title": "Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable."}
{"id": "2504.17550", "pdf": "https://arxiv.org/pdf/2504.17550", "abs": "https://arxiv.org/abs/2504.17550", "authors": ["Yejin Bang", "Ziwei Ji", "Alan Schelten", "Anthony Hartshorn", "Tara Fowler", "Cheng Zhang", "Nicola Cancedda", "Pascale Fung"], "title": "HalluLens: LLM Hallucination Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": "42 pages", "summary": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations."}
{"id": "2504.17562", "pdf": "https://arxiv.org/pdf/2504.17562", "abs": "https://arxiv.org/abs/2504.17562", "authors": ["Rei Higuchi", "Ryotaro Kawata", "Naoki Nishikawa", "Kazusato Oko", "Shoichiro Yamaguchi", "Sosuke Kobayashi", "Seiya Tokui", "Kohei Hayashi", "Daisuke Okanohara", "Taiji Suzuki"], "title": "When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The ability to acquire latent semantics is one of the key properties that\ndetermines the performance of language models. One convenient approach to\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\nthe beginning of texts in the pre-training data, making it easier for the model\nto access latent semantics before observing the entire text. Previous studies\nhave reported that this technique actually improves the performance of trained\nmodels in downstream tasks; however, this improvement has been observed only in\nspecific downstream tasks, without consistent enhancement in average next-token\nprediction loss. To understand this phenomenon, we closely investigate how\nprepending metadata during pre-training affects model performance by examining\nits behavior using artificial data. Interestingly, we found that this approach\nproduces both positive and negative effects on the downstream tasks. We\ndemonstrate that the effectiveness of the approach depends on whether latent\nsemantics can be inferred from the downstream task's prompt. Specifically,\nthrough investigations using data generated by probabilistic context-free\ngrammars, we show that training with metadata helps improve model's performance\nwhen the given context is long enough to infer the latent semantics. In\ncontrast, the technique negatively impacts performance when the context lacks\nthe necessary information to make an accurate posterior inference."}
{"id": "2504.17565", "pdf": "https://arxiv.org/pdf/2504.17565", "abs": "https://arxiv.org/abs/2504.17565", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Yunjie Ji", "Han Zhao", "Xiangang Li"], "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training", "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M"}
{"id": "2504.17574", "pdf": "https://arxiv.org/pdf/2504.17574", "abs": "https://arxiv.org/abs/2504.17574", "authors": ["Zhenkai Qin", "Guifang Yang", "Dongze Wu"], "title": "RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "As false information continues to proliferate across social media platforms,\neffective rumor detection has emerged as a pressing challenge in natural\nlanguage processing. This paper proposes RAGAT-Mind, a multi-granular modeling\napproach for Chinese rumor detection, built upon the MindSpore deep learning\nframework. The model integrates TextCNN for local semantic extraction,\nbidirectional GRU for sequential context learning, Multi-Head Self-Attention\nfor global dependency focusing, and Bidirectional Graph Convolutional Networks\n(BiGCN) for structural representation of word co-occurrence graphs. Experiments\non the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior\nclassification performance, attaining 99.2% accuracy and a macro-F1 score of\n0.9919. The results validate the effectiveness of combining hierarchical\nlinguistic features with graph-based semantic structures. Furthermore, the\nmodel exhibits strong generalization and interpretability, highlighting its\npractical value for real-world rumor detection applications."}
{"id": "2504.17653", "pdf": "https://arxiv.org/pdf/2504.17653", "abs": "https://arxiv.org/abs/2504.17653", "authors": ["Samaneh Hosseini Moghaddam", "Kelly Lyons", "Cheryl Regehr", "Vivek Goel", "Kaitlyn Regehr"], "title": "Towards a comprehensive taxonomy of online abusive language informed by machine leaning", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of abusive language in online communications has posed\nsignificant risks to the health and wellbeing of individuals and communities.\nThe growing concern regarding online abuse and its consequences necessitates\nmethods for identifying and mitigating harmful content and facilitating\ncontinuous monitoring, moderation, and early intervention. This paper presents\na taxonomy for distinguishing key characteristics of abusive language within\nonline text. Our approach uses a systematic method for taxonomy development,\nintegrating classification systems of 18 existing multi-label datasets to\ncapture key characteristics relevant to online abusive language classification.\nThe resulting taxonomy is hierarchical and faceted, comprising 5 categories and\n17 dimensions. It classifies various facets of online abuse, including context,\ntarget, intensity, directness, and theme of abuse. This shared understanding\ncan lead to more cohesive efforts, facilitate knowledge exchange, and\naccelerate progress in the field of online abuse detection and mitigation among\nresearchers, policy makers, online platform owners, and other stakeholders."}
{"id": "2504.17665", "pdf": "https://arxiv.org/pdf/2504.17665", "abs": "https://arxiv.org/abs/2504.17665", "authors": ["Zena Al-Khalili", "Nick Howell", "Dietrich Klakow"], "title": "Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics", "categories": ["cs.CL"], "comment": null, "summary": "Assisting LLMs with code generation improved their performance on\nmathematical reasoning tasks. However, the evaluation of code-assisted LLMs is\ngenerally restricted to execution correctness, lacking a rigorous evaluation of\ntheir generated programs. In this work, we bridge this gap by conducting an\nin-depth analysis of code-assisted LLMs' generated programs in response to math\nreasoning tasks. Our evaluation focuses on the extent to which LLMs ground\ntheir programs to math rules, and how that affects their end performance. For\nthis purpose, we assess the generations of five different LLMs, on two\ndifferent math datasets, both manually and automatically. Our results reveal\nthat the distribution of grounding depends on LLMs' capabilities and the\ndifficulty of math problems. Furthermore, mathematical grounding is more\neffective for closed-source models, while open-source models fail to employ\nmath rules in their solutions correctly. On MATH500, the percentage of grounded\nprograms decreased to half, while the ungrounded generations doubled in\ncomparison to ASDiv grade-school problems. Our work highlights the need for\nin-depth evaluation beyond execution accuracy metrics, toward a better\nunderstanding of code-assisted LLMs' capabilities and limits in the math\ndomain."}
{"id": "2504.17671", "pdf": "https://arxiv.org/pdf/2504.17671", "abs": "https://arxiv.org/abs/2504.17671", "authors": ["Yuanchang Ye", "Weiyan Wen"], "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making."}
{"id": "2504.17674", "pdf": "https://arxiv.org/pdf/2504.17674", "abs": "https://arxiv.org/abs/2504.17674", "authors": ["Jared Fernandez", "Clara Na", "Vashisth Tiwari", "Yonatan Bisk", "Sasha Luccioni", "Emma Strubell"], "title": "Energy Considerations of Large Language Model Inference and Efficiency Optimizations", "categories": ["cs.CL", "cs.LG"], "comment": "16 pages", "summary": "As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure."}
{"id": "2504.17685", "pdf": "https://arxiv.org/pdf/2504.17685", "abs": "https://arxiv.org/abs/2504.17685", "authors": ["Haru-Tada Sato", "Fuka Matsuzaki", "Jun-ichiro Takahashi"], "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 2 figures", "summary": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach."}
{"id": "2504.17704", "pdf": "https://arxiv.org/pdf/2504.17704", "abs": "https://arxiv.org/abs/2504.17704", "authors": ["Cheng Wang", "Yue Liu", "Baolong Li", "Duzhen Zhang", "Zhongzhi Li", "Junfeng Fang"], "title": "Safety in Large Reasoning Models: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks\nlike mathematics and coding, leveraging their advanced reasoning capabilities.\nNevertheless, as these capabilities progress, significant concerns regarding\ntheir vulnerabilities and safety have arisen, which can pose challenges to\ntheir deployment and application in real-world settings. This paper presents a\ncomprehensive survey of LRMs, meticulously exploring and summarizing the newly\nemerged safety risks, attacks, and defense strategies. By organizing these\nelements into a detailed taxonomy, this work aims to offer a clear and\nstructured understanding of the current safety landscape of LRMs, facilitating\nfuture research and development to enhance the security and reliability of\nthese powerful models."}
{"id": "2504.17720", "pdf": "https://arxiv.org/pdf/2504.17720", "abs": "https://arxiv.org/abs/2504.17720", "authors": ["Vansh Gupta", "Sankalan Pal Chowdhury", "Vilém Zouhar", "Donya Rooein", "Mrinmaya Sachan"], "title": "Multilingual Performance Biases of Large Language Models in Education", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment."}
{"id": "2504.17753", "pdf": "https://arxiv.org/pdf/2504.17753", "abs": "https://arxiv.org/abs/2504.17753", "authors": ["Anuja Tayal", "Devika Salunke", "Barbara Di Eugenio", "Paula Allen-Meares", "Eulalia Puig Abril", "Olga Garcia", "Carolyn Dickens", "Andrew Boyd"], "title": "Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT", "categories": ["cs.CL"], "comment": null, "summary": "Conversational assistants are becoming more and more popular, including in\nhealthcare, partly because of the availability and capabilities of Large\nLanguage Models. There is a need for controlled, probing evaluations with real\nstakeholders which can highlight advantages and disadvantages of more\ntraditional architectures and those based on generative AI. We present a\nwithin-group user study to compare two versions of a conversational assistant\nthat allows heart failure patients to ask about salt content in food. One\nversion of the system was developed in-house with a neurosymbolic architecture,\nand one is based on ChatGPT. The evaluation shows that the in-house system is\nmore accurate, completes more tasks and is less verbose than the one based on\nChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors\nand requires fewer clarifications to complete the task. Patients show no\npreference for one over the other."}
{"id": "2504.17768", "pdf": "https://arxiv.org/pdf/2504.17768", "abs": "https://arxiv.org/abs/2504.17768", "authors": ["Piotr Nawrot", "Robert Li", "Renjie Huang", "Sebastian Ruder", "Kelly Marchisio", "Edoardo M. Ponti"], "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications."}
{"id": "2504.16939", "pdf": "https://arxiv.org/pdf/2504.16939", "abs": "https://arxiv.org/abs/2504.16939", "authors": ["Emre Can Acikgoz", "Cheng Qian", "Hongru Wang", "Vardhan Dongre", "Xiusi Chen", "Heng Ji", "Dilek Hakkani-Tür", "Gokhan Tur"], "title": "A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have propelled conversational\nAI from traditional dialogue systems into sophisticated agents capable of\nautonomous actions, contextual awareness, and multi-turn interactions with\nusers. Yet, fundamental questions about their capabilities, limitations, and\npaths forward remain open. This survey paper presents a desideratum for\nnext-generation Conversational Agents - what has been achieved, what challenges\npersist, and what must be done for more scalable systems that approach\nhuman-level intelligence. To that end, we systematically analyze LLM-driven\nConversational Agents by organizing their capabilities into three primary\ndimensions: (i) Reasoning - logical, systematic thinking inspired by human\nintelligence for decision making, (ii) Monitor - encompassing self-awareness\nand user interaction monitoring, and (iii) Control - focusing on tool\nutilization and policy following. Building upon this, we introduce a novel\ntaxonomy by classifying recent work on Conversational Agents around our\nproposed desideratum. We identify critical research gaps and outline key\ndirections, including realistic evaluations, long-term multi-turn reasoning\nskills, self-evolution capabilities, collaborative and multi-agent task\ncompletion, personalization, and proactivity. This work aims to provide a\nstructured foundation, highlight existing limitations, and offer insights into\npotential future research directions for Conversational Agents, ultimately\nadvancing progress toward Artificial General Intelligence (AGI). We maintain a\ncurated repository of papers at:\nhttps://github.com/emrecanacikgoz/awesome-conversational-agents."}
{"id": "2504.17004", "pdf": "https://arxiv.org/pdf/2504.17004", "abs": "https://arxiv.org/abs/2504.17004", "authors": ["Amin Karbasi", "Omar Montasser", "John Sous", "Grigoris Velegkas"], "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment."}
{"id": "2504.17038", "pdf": "https://arxiv.org/pdf/2504.17038", "abs": "https://arxiv.org/abs/2504.17038", "authors": ["Christian D. Newman", "Brandon Scholten", "Sophia Testa", "Joshua A. C. Behler", "Syreen Banabilah", "Michael L. Collard", "Michael J. Decker", "Mohamed Wiem Mkaouer", "Marcos Zampieri", "Eman Abdullah AlOmar", "Reem Alsuhaibani", "Anthony Peruma", "Jonathan I. Maletic"], "title": "SCALAR: A Part-of-speech Tagger for Identifiers", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "The paper presents the Source Code Analysis and Lexical Annotation Runtime\n(SCALAR), a tool specialized for mapping (annotating) source code identifier\nnames to their corresponding part-of-speech tag sequence (grammar pattern).\nSCALAR's internal model is trained using scikit-learn's\nGradientBoostingClassifier in conjunction with a manually-curated oracle of\nidentifier names and their grammar patterns. This specializes the tagger to\nrecognize the unique structure of the natural language used by developers to\ncreate all types of identifiers (e.g., function names, variable names etc.).\nSCALAR's output is compared with a previous version of the tagger, as well as a\nmodern off-the-shelf part-of-speech tagger to show how it improves upon other\ntaggers' output for annotating identifiers. The code is available on Github"}
{"id": "2504.17365", "pdf": "https://arxiv.org/pdf/2504.17365", "abs": "https://arxiv.org/abs/2504.17365", "authors": ["Ling You", "Wenxuan Huang", "Xinni Xie", "Xiangyi Wei", "Bangyan Li", "Shaohui Lin", "Yang Li", "Changbo Wang"], "title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Soccer is a globally popular sporting event, typically characterized by long\nmatches and distinctive highlight moments. Recent advances in Multimodal Large\nLanguage Models (MLLMs) offer promising capabilities in temporal grounding and\nvideo understanding, soccer commentary generation often requires precise\ntemporal localization and semantically rich descriptions over long-form video.\nHowever, existing soccer MLLMs often rely on the temporal a priori for caption\ngeneration, so they cannot process the soccer video end-to-end. While some\ntraditional approaches follow a two-step paradigm that is complex and fails to\ncapture the global context to achieve suboptimal performance. To solve the\nabove issues, we present TimeSoccer, the first end-to-end soccer MLLM for\nSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.\nTimeSoccer jointly predicts timestamps and generates captions in a single pass,\nenabling global context modeling across 45-minute matches. To support long\nvideo understanding of soccer matches, we introduce MoFA-Select, a\ntraining-free, motion-aware frame compression module that adaptively selects\nrepresentative frames via a coarse-to-fine strategy, and incorporates\ncomplementary training paradigms to strengthen the model's ability to handle\nlong temporal sequences. Extensive experiments demonstrate that our TimeSoccer\nachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end\nform, generating high-quality commentary with accurate temporal alignment and\nstrong semantic relevance."}
{"id": "2504.17449", "pdf": "https://arxiv.org/pdf/2504.17449", "abs": "https://arxiv.org/abs/2504.17449", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Lidan Shou", "Ke Chen", "Gang Chen", "Qin Xie", "Guiming Xie", "Xuejian Gong"], "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by VLDBJ 2025", "summary": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy."}
