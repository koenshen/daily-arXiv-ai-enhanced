{"id": "2504.18560", "pdf": "https://arxiv.org/pdf/2504.18560", "abs": "https://arxiv.org/abs/2504.18560", "authors": ["Alessio Buscemi", "Cédric Lothritz", "Sergio Morales", "Marcos Gomez-Vazquez", "Robert Clarisó", "Jordi Cabot", "German Castignani"], "title": "Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have exhibited impressive natural language\nprocessing capabilities but often perpetuate social biases inherent in their\ntraining data. To address this, we introduce MultiLingual Augmented Bias\nTesting (MLA-BiTe), a framework that improves prior bias evaluation methods by\nenabling systematic multilingual bias testing. MLA-BiTe leverages automated\ntranslation and paraphrasing techniques to support comprehensive assessments\nacross diverse linguistic settings. In this study, we evaluate the\neffectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six\nlanguages -- including two low-resource languages -- focusing on seven\nsensitive categories of discrimination."}
{"id": "2504.18639", "pdf": "https://arxiv.org/pdf/2504.18639", "abs": "https://arxiv.org/abs/2504.18639", "authors": ["Passant Elchafei", "Mervet Abu-Elkheir"], "title": "Span-Level Hallucination Detection for LLM-Generated Answers", "categories": ["cs.CL"], "comment": null, "summary": "Detecting spans of hallucination in LLM-generated answers is crucial for\nimproving factual consistency. This paper presents a span-level hallucination\ndetection framework for the SemEval-2025 Shared Task, focusing on English and\nArabic texts. Our approach integrates Semantic Role Labeling (SRL) to decompose\nthe answer into atomic roles, which are then compared with a retrieved\nreference context obtained via question-based LLM prompting. Using a\nDeBERTa-based textual entailment model, we evaluate each role semantic\nalignment with the retrieved context. The entailment scores are further refined\nthrough token-level confidence measures derived from output logits, and the\ncombined scores are used to detect hallucinated spans. Experiments on the\nMu-SHROOM dataset demonstrate competitive performance. Additionally,\nhallucinated spans have been verified through fact-checking by prompting GPT-4\nand LLaMA. Our findings contribute to improving hallucination detection in\nLLM-generated responses."}
{"id": "2504.18673", "pdf": "https://arxiv.org/pdf/2504.18673", "abs": "https://arxiv.org/abs/2504.18673", "authors": ["Jiayi Li", "Yingfan Zhou", "Pranav Narayanan Venkit", "Halima Binte Islam", "Sneha Arya", "Shomir Wilson", "Sarah Rajtmajer"], "title": "Can Third-parties Read Our Emotions?", "categories": ["cs.CL"], "comment": null, "summary": "Natural Language Processing tasks that aim to infer an author's private\nstates, e.g., emotions and opinions, from their written text, typically rely on\ndatasets annotated by third-party annotators. However, the assumption that\nthird-party annotators can accurately capture authors' private states remains\nlargely unexamined. In this study, we present human subjects experiments on\nemotion recognition tasks that directly compare third-party annotations with\nfirst-party (author-provided) emotion labels. Our findings reveal significant\nlimitations in third-party annotations-whether provided by human annotators or\nlarge language models (LLMs)-in faithfully representing authors' private\nstates. However, LLMs outperform human annotators nearly across the board. We\nfurther explore methods to improve third-party annotation quality. We find that\ndemographic similarity between first-party authors and third-party human\nannotators enhances annotation performance. While incorporating first-party\ndemographic information into prompts leads to a marginal but statistically\nsignificant improvement in LLMs' performance. We introduce a framework for\nevaluating the limitations of third-party annotations and call for refined\nannotation practices to accurately represent and model authors' private states."}
{"id": "2504.18715", "pdf": "https://arxiv.org/pdf/2504.18715", "abs": "https://arxiv.org/abs/2504.18715", "authors": ["Tuochao Chen", "Qirui Wang", "Runlin He", "Shyam Gollakota"], "title": "Spatial Speech Translation: Translating Across Space With Binaural Hearables", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by CHI2025", "summary": "Imagine being in a crowded space where people speak a different language and\nhaving hearables that transform the auditory space into your native language,\nwhile preserving the spatial cues for all speakers. We introduce spatial speech\ntranslation, a novel concept for hearables that translate speakers in the\nwearer's environment, while maintaining the direction and unique voice\ncharacteristics of each speaker in the binaural output. To achieve this, we\ntackle several technical challenges spanning blind source separation,\nlocalization, real-time expressive translation, and binaural rendering to\npreserve the speaker directions in the translated audio, while achieving\nreal-time inference on the Apple M2 silicon. Our proof-of-concept evaluation\nwith a prototype binaural headset shows that, unlike existing models, which\nfail in the presence of interference, we achieve a BLEU score of up to 22.01\nwhen translating between languages, despite strong interference from other\nspeakers in the environment. User studies further confirm the system's\neffectiveness in spatially rendering the translated speech in previously unseen\nreal-world reverberant environments. Taking a step back, this work marks the\nfirst step towards integrating spatial perception into speech translation."}
{"id": "2504.18718", "pdf": "https://arxiv.org/pdf/2504.18718", "abs": "https://arxiv.org/abs/2504.18718", "authors": ["Lauren Levine", "Junghyun Min", "Amir Zeldes"], "title": "Building UD Cairo for Old English in the Classroom", "categories": ["cs.CL"], "comment": "7 pages, 2 figures", "summary": "In this paper we present a sample treebank for Old English based on the UD\nCairo sentences, collected and annotated as part of a classroom curriculum in\nHistorical Linguistics. To collect the data, a sample of 20 sentences\nillustrating a range of syntactic constructions in the world's languages, we\nemploy a combination of LLM prompting and searches in authentic Old English\ndata. For annotation we assigned sentences to multiple students with limited\nprior exposure to UD, whose annotations we compare and adjudicate. Our results\nsuggest that while current LLM outputs in Old English do not reflect authentic\nsyntax, this can be mitigated by post-editing, and that although beginner\nannotators do not possess enough background to complete the task perfectly,\ntaken together they can produce good results and learn from the experience. We\nalso conduct preliminary parsing experiments using Modern English training\ndata, and find that although performance on Old English is poor, parsing on\nannotated features (lemma, hyperlemma, gloss) leads to improved performance."}
{"id": "2504.18736", "pdf": "https://arxiv.org/pdf/2504.18736", "abs": "https://arxiv.org/abs/2504.18736", "authors": ["Jianyou Wang", "Weili Cao", "Kaicheng Wang", "Xiaoyue Wang", "Ashish Dalvi", "Gino Prasad", "Qishan Liang", "Hsuan-lin Her", "Ming Wang", "Qin Yang", "Gene W. Yeo", "David E. Neal", "Maxim Khan", "Christopher D. Rosin", "Ramamohan Paturi", "Leon Bergen"], "title": "EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers", "categories": ["cs.CL"], "comment": null, "summary": "We study the task of automatically finding evidence relevant to hypotheses in\nbiomedical papers. Finding relevant evidence is an important step when\nresearchers investigate scientific hypotheses. We introduce EvidenceBench to\nmeasure models performance on this task, which is created by a novel pipeline\nthat consists of hypothesis generation and sentence-by-sentence annotation of\nbiomedical papers for relevant evidence, completely guided by and faithfully\nfollowing existing human experts judgment. We demonstrate the pipeline's\nvalidity and accuracy with multiple sets of human-expert annotations. We\nevaluated a diverse set of language models and retrieval systems on the\nbenchmark and found that model performances still fall significantly short of\nthe expert level on this task. To show the scalability of our proposed\npipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated\npapers with hypotheses to facilitate model training and development. Both\ndatasets are available at https://github.com/EvidenceBench/EvidenceBench"}
{"id": "2504.18762", "pdf": "https://arxiv.org/pdf/2504.18762", "abs": "https://arxiv.org/abs/2504.18762", "authors": ["Ojasw Upadhyay", "Abishek Saravankumar", "Ayman Ismail"], "title": "SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures, 4 tables", "summary": "Large Language Models (LLMs) are powerful but often require extensive\nfine-tuning and large datasets for specialized domains like law.\nGeneral-purpose pre-training may not capture legal nuances, and acquiring\nsufficient legal data is challenging. We introduce SynLexLM, a novel approach\nto efficiently pre-train a legal LLM. Our method employs curriculum learning,\nprogressing from simple to complex legal texts and queries, combined with\nsynthetic data augmentation using models like Gemini Pro to address data\nscarcity. We aim to achieve improved performance on legal benchmarks\n(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned\nversions. Preliminary work involves generating synthetic QA pairs reflecting\nlegal reasoning. This work aims to enhance legal document analysis and research\ntools, potentially democratizing access to advanced legal AI."}
{"id": "2504.18805", "pdf": "https://arxiv.org/pdf/2504.18805", "abs": "https://arxiv.org/abs/2504.18805", "authors": ["Jong Inn Park", "Maanas Taneja", "Qianwen Wang", "Dongyeop Kang"], "title": "Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page: https://minnesotanlp.github.io/scitalk-project-page/", "summary": "Generating engaging, accurate short-form videos from scientific papers is\nchallenging due to content complexity and the gap between expert authors and\nreaders. Existing end-to-end methods often suffer from factual inaccuracies and\nvisual artifacts, limiting their utility for scientific dissemination. To\naddress these issues, we propose SciTalk, a novel multi-LLM agentic framework,\ngrounding videos in various sources, such as text, figures, visual styles, and\navatars. Inspired by content creators' workflows, SciTalk uses specialized\nagents for content summarization, visual scene planning, and text and layout\nediting, and incorporates an iterative feedback mechanism where video agents\nsimulate user roles to give feedback on generated videos from previous\niterations and refine generation prompts. Experimental evaluations show that\nSciTalk outperforms simple prompting methods in generating scientifically\naccurate and engaging content over the refined loop of video generation.\nAlthough preliminary results are still not yet matching human creators'\nquality, our framework provides valuable insights into the challenges and\nbenefits of feedback-driven video generation. Our code, data, and generated\nvideos will be publicly available."}
{"id": "2504.18838", "pdf": "https://arxiv.org/pdf/2504.18838", "abs": "https://arxiv.org/abs/2504.18838", "authors": ["Yixin Cao", "Shibo Hong", "Xinze Li", "Jiahao Ying", "Yubo Ma", "Haiyuan Liang", "Yantao Liu", "Zijun Yao", "Xiaozhi Wang", "Dan Huang", "Wenxuan Zhang", "Lifu Huang", "Muhao Chen", "Lei Hou", "Qianru Sun", "Xingjun Ma", "Zuxuan Wu", "Min-Yen Kan", "David Lo", "Qi Zhang", "Heng Ji", "Jing Jiang", "Juanzi Li", "Aixin Sun", "Xuanjing Huang", "Tat-Seng Chua", "Yu-Gang Jiang"], "title": "Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are advancing at an amazing speed and have\nbecome indispensable across academia, industry, and daily applications. To keep\npace with the status quo, this survey probes the core challenges that the rise\nof LLMs poses for evaluation. We identify and analyze two pivotal transitions:\n(i) from task-specific to capability-based evaluation, which reorganizes\nbenchmarks around core competencies such as knowledge, reasoning, instruction\nfollowing, multi-modal understanding, and safety; and (ii) from manual to\nautomated evaluation, encompassing dynamic dataset curation and\n\"LLM-as-a-judge\" scoring.\n  Yet, even with these transitions, a crucial obstacle persists: the evaluation\ngeneralization issue. Bounded test sets cannot scale alongside models whose\nabilities grow seemingly without limit. We will dissect this issue, along with\nthe core challenges of the above two transitions, from the perspectives of\nmethods, datasets, evaluators, and metrics. Due to the fast evolving of this\nfield, we will maintain a living GitHub repository (links are in each section)\nto crowd-source updates and corrections, and warmly invite contributors and\ncollaborators."}
{"id": "2504.18839", "pdf": "https://arxiv.org/pdf/2504.18839", "abs": "https://arxiv.org/abs/2504.18839", "authors": ["Abdellah Ghassel", "Xianzhi Li", "Xiaodan Zhu"], "title": "Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are rapidly changing various domains. However,\ntheir capabilities in handling conversational breakdowns still require an\nin-depth exploration. This paper addresses the challenge of detecting and\nmitigating dialogue breakdowns within LLM-driven conversational systems. While\npowerful models from OpenAI and Anthropic excel in many dialogue tasks, they\ncan still produce incoherent or contradictory responses, commonly referred to\nas breakdowns, which undermine user trust. To tackle this, we propose an\napproach that combines specialized fine-tuning with advanced prompting\nstrategies, including few-shot learning, chain-of-thought reasoning, and\nanalogical prompting. In particular, we fine-tune a small 8B model and\ndemonstrate its robust classification and calibration capabilities in English\nand Japanese dialogue. We also validate its generalization on the BETOLD\ndataset, achieving a 7\\% accuracy improvement over its base model. Furthermore,\nwe introduce a real-time deployment architecture that selectively escalates\nsuspicious responses to more resource-intensive frontier models only when\nbreakdowns are detected, significantly cutting operational expenses and energy\nconsumption. Experimental results show our method surpasses prior\nstate-of-the-art specialized classifiers while also narrowing performance gaps\nbetween smaller open-source models and large proprietary ones. Our approach\noffers a scalable solution for robust conversational AI in high-impact domains\nby combining efficiency, interpretability, and reliability."}
{"id": "2504.18851", "pdf": "https://arxiv.org/pdf/2504.18851", "abs": "https://arxiv.org/abs/2504.18851", "authors": ["Hayley Ross", "Ameya Sunil Mahabaleshwarkar", "Yoshi Suhara"], "title": "When2Call: When (not) to Call Tools", "categories": ["cs.CL"], "comment": "NAACL 2025", "summary": "Leveraging external tools is a key feature for modern Language Models (LMs)\nto expand their capabilities and integrate them into existing systems. However,\nexisting benchmarks primarily focus on the accuracy of tool calling -- whether\nthe correct tool is called with the correct parameters -- and less on\nevaluating when LMs should (not) call tools. We develop a new benchmark,\nWhen2Call, which evaluates tool-calling decision-making: when to generate a\ntool call, when to ask follow-up questions and when to admit the question can't\nbe answered with the tools provided. We find that state-of-the-art tool-calling\nLMs show significant room for improvement on When2Call, indicating the\nimportance of this benchmark. We also develop a training set for When2Call and\nleverage the multiple-choice nature of the benchmark to develop a preference\noptimization training regime, which shows considerably more improvement than\ntraditional fine-tuning. We release the benchmark and training data as well as\nevaluation scripts at https://github.com/NVIDIA/When2Call."}
{"id": "2504.18857", "pdf": "https://arxiv.org/pdf/2504.18857", "abs": "https://arxiv.org/abs/2504.18857", "authors": ["Yi Lu", "Wanxu Zhao", "Xin Zhou", "Chenxin An", "Chenglong Wang", "Shuo Li", "Yuming Yang", "Jun Zhao", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle to process and generate coherent\ncontext when the number of input tokens exceeds the pre-trained length. Recent\nadvancements in long-context extension have significantly expanded the context\nwindow of LLMs but require expensive overhead to train the large-scale models\nwith longer context. In this work, we propose Dimension-Wise Positional\nEmbeddings Manipulation (DPE), a training-free framework to extrapolate the\ncontext window of LLMs by diving into RoPE's different hidden dimensions.\nInstead of manipulating all dimensions equally, DPE detects the effective\nlength for every dimension and finds the key dimensions for context extension.\nWe reuse the original position indices with their embeddings from the\npre-trained model and manipulate the key dimensions' position indices to their\nmost effective lengths. In this way, DPE adjusts the pre-trained models with\nminimal modifications while ensuring that each dimension reaches its optimal\nstate for extrapolation. DPE significantly surpasses well-known baselines such\nas YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of\n128k tokens without continual training and integrates seamlessly with Flash\nAttention 2. In addition to its impressive extrapolation capability, DPE also\ndramatically improves the models' performance within training length, such as\nLlama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When\ncompared with commercial models, Llama 3.1 70B with DPE even achieves better\nperformance than GPT-4-128K."}
{"id": "2504.18872", "pdf": "https://arxiv.org/pdf/2504.18872", "abs": "https://arxiv.org/abs/2504.18872", "authors": ["Alexandra Abbas", "Nora Petrova", "Helios Ael Lyons", "Natalia Perez-Campanero"], "title": "Latent Adversarial Training Improves the Representation of Refusal", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent work has shown that language models' refusal behavior is primarily\nencoded in a single direction in their latent space, making it vulnerable to\ntargeted attacks. Although Latent Adversarial Training (LAT) attempts to\nimprove robustness by introducing noise during training, a key question\nremains: How does this noise-based training affect the underlying\nrepresentation of refusal behavior? Understanding this encoding is crucial for\nevaluating LAT's effectiveness and limitations, just as the discovery of linear\nrefusal directions revealed vulnerabilities in traditional supervised safety\nfine-tuning (SSFT).\n  Through the analysis of Llama 2 7B, we examine how LAT reorganizes the\nrefusal behavior in the model's latent space compared to SSFT and embedding\nspace adversarial training (AT). By computing activation differences between\nharmful and harmless instruction pairs and applying Singular Value\nDecomposition (SVD), we find that LAT significantly alters the refusal\nrepresentation, concentrating it in the first two SVD components which explain\napproximately 75 percent of the activation differences variance - significantly\nhigher than in reference models. This concentrated representation leads to more\neffective and transferable refusal vectors for ablation attacks: LAT models\nshow improved robustness when attacked with vectors from reference models but\nbecome more vulnerable to self-generated vectors compared to SSFT and AT. Our\nfindings suggest that LAT's training perturbations enable a more comprehensive\nrepresentation of refusal behavior, highlighting both its potential strengths\nand vulnerabilities for improving model safety."}
{"id": "2504.18884", "pdf": "https://arxiv.org/pdf/2504.18884", "abs": "https://arxiv.org/abs/2504.18884", "authors": ["Junichiro Niimi"], "title": "A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": "This manuscript has been accepted for the 30th International\n  Conference on Natural Language & Information Systems (NLDB 2025). The final\n  version will appear in the Springer LNCS proceedings. arXiv admin note: text\n  overlap with arXiv:2407.13069", "summary": "With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%."}
{"id": "2504.18938", "pdf": "https://arxiv.org/pdf/2504.18938", "abs": "https://arxiv.org/abs/2504.18938", "authors": ["Junhong Liang", "Yu Zhou"], "title": "MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction", "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens\nin sentences. While Large Language Models (LLMs) have shown remarkable success\nin identifying and rectifying potential errors, they often struggle with\nmaintaining consistent output lengths and adapting to domain-specific\ncorrections. Furthermore, existing CSC task impose rigid constraints requiring\ninput and output lengths to be identical, limiting their applicability. In this\nwork, we extend traditional CSC to variable-length correction scenarios,\nincluding Chinese Splitting Error Correction (CSEC) and ASR N-best Error\nCorrection. To address domain adaptation and length consistency, we propose\nMTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection\nmechanism. Our approach constructs a retrieval database from domain-specific\ntraining data and dictionaries, fine-tuning retrievers to optimize performance\nfor error-containing inputs. Additionally, we introduce a multi-source\ncombination strategy with iterative length reflection to ensure output length\nfidelity. Experiments across diverse domain datasets demonstrate that our\nmethod significantly outperforms current approaches in correction quality,\nparticularly in handling domain-specific and variable-length error correction\ntasks."}
{"id": "2504.18942", "pdf": "https://arxiv.org/pdf/2504.18942", "abs": "https://arxiv.org/abs/2504.18942", "authors": ["Debarati Das", "Khanh Chi Le", "Ritik Sachin Parkar", "Karin De Langis", "Brendan Madson", "Chad M. Berryman", "Robin M. Willis", "Daniel H. Moses", "Brett McDonnell", "Daniel Schwarcz", "Dongyeop Kang"], "title": "LawFlow : Collecting and Simulating Lawyers' Thought Processes", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "submitted to COLM 2025", "summary": "Legal practitioners, particularly those early in their careers, face complex,\nhigh-stakes tasks that require adaptive, context-sensitive reasoning. While AI\nholds promise in supporting legal work, current datasets and models are\nnarrowly focused on isolated subtasks and fail to capture the end-to-end\ndecision-making required in real-world practice. To address this gap, we\nintroduce LawFlow, a dataset of complete end-to-end legal workflows collected\nfrom trained law students, grounded in real-world business entity formation\nscenarios. Unlike prior datasets focused on input-output pairs or linear chains\nof thought, LawFlow captures dynamic, modular, and iterative reasoning\nprocesses that reflect the ambiguity, revision, and client-adaptive strategies\nof legal practice. Using LawFlow, we compare human and LLM-generated workflows,\nrevealing systematic differences in structure, reasoning flexibility, and plan\nexecution. Human workflows tend to be modular and adaptive, while LLM workflows\nare more sequential, exhaustive, and less sensitive to downstream implications.\nOur findings also suggest that legal professionals prefer AI to carry out\nsupportive roles, such as brainstorming, identifying blind spots, and surfacing\nalternatives, rather than executing complex workflows end-to-end. Building on\nthese findings, we propose a set of design suggestions, rooted in empirical\nobservations, that align AI assistance with human goals of clarity,\ncompleteness, creativity, and efficiency, through hybrid planning, adaptive\nexecution, and decision-point support. Our results highlight both the current\nlimitations of LLMs in supporting complex legal workflows and opportunities for\ndeveloping more collaborative, reasoning-aware legal AI systems. All data and\ncode are available on our project page\n(https://minnesotanlp.github.io/LawFlow-website/)."}
{"id": "2504.18992", "pdf": "https://arxiv.org/pdf/2504.18992", "abs": "https://arxiv.org/abs/2504.18992", "authors": ["Sanwoo Lee", "Jiahao Liu", "Qifan Wang", "Jingang Wang", "Xunliang Cai", "Yunfang Wu"], "title": "Dynamic Fisher-weighted Model Merging via Bayesian Optimization", "categories": ["cs.CL"], "comment": null, "summary": "The fine-tuning of pre-trained language models has resulted in the widespread\navailability of task-specific models. Model merging offers an efficient way to\ncreate multi-task models by combining these fine-tuned models at the parameter\nlevel, without the need for training data or joint training on multiple\ndatasets. Existing merging approaches typically involve scaling the parameters\nmodel-wise or integrating parameter importance parameter-wise. Both approaches\nexhibit their own weaknesses, leading to a notable performance gap compared to\nmulti-task fine-tuning. In this paper, we unify these seemingly distinct\nstrategies into a more general merging framework, and introduce Dynamic\nFisher-weighted Merging (DF-Merge). Specifically, candidate models are\nassociated with a set of coefficients that linearly scale their fine-tuned\nparameters. Bayesian optimization is applied to dynamically adjust these\ncoefficients, aiming to maximize overall performance on validation sets. Each\niteration of this process integrates parameter importance based on the Fisher\ninformation conditioned by the coefficients. Experimental results show that\nDF-Merge outperforms strong baselines across models of different sizes and a\nvariety of tasks. Our analysis shows that the effectiveness of DF-Merge arises\nfrom the unified view of merging and that near-optimal performance is\nachievable in a few iterations, even with minimal validation data."}
{"id": "2504.19019", "pdf": "https://arxiv.org/pdf/2504.19019", "abs": "https://arxiv.org/abs/2504.19019", "authors": ["Mohammad Akbar-Tajari", "Mohammad Taher Pilehvar", "Mohammad Mahmoody"], "title": "Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "19 pages, 1 figure, 6 tables", "summary": "The challenge of ensuring Large Language Models (LLMs) align with societal\nstandards is of increasing interest, as these models are still prone to\nadversarial jailbreaks that bypass their safety mechanisms. Identifying these\nvulnerabilities is crucial for enhancing the robustness of LLMs against such\nexploits. We propose Graph of ATtacks (GoAT), a method for generating\nadversarial prompts to test the robustness of LLM alignment using the Graph of\nThoughts framework [Besta et al., 2024]. GoAT excels at generating highly\neffective jailbreak prompts with fewer queries to the victim model than\nstate-of-the-art attacks, achieving up to five times better jailbreak success\nrate against robust models like Llama. Notably, GoAT creates high-quality,\nhuman-readable prompts without requiring access to the targeted model's\nparameters, making it a black-box attack. Unlike approaches constrained by\ntree-based reasoning, GoAT's reasoning is based on a more intricate graph\nstructure. By making simultaneous attack paths aware of each other's progress,\nthis dynamic framework allows a deeper integration and refinement of reasoning\npaths, significantly enhancing the collaborative exploration of adversarial\nvulnerabilities in LLMs. At a technical level, GoAT starts with a graph\nstructure and iteratively refines it by combining and improving thoughts,\nenabling synergy between different thought paths. The code for our\nimplementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks."}
{"id": "2504.19021", "pdf": "https://arxiv.org/pdf/2504.19021", "abs": "https://arxiv.org/abs/2504.19021", "authors": ["Zhyar Rzgar K Rostam", "Gábor Kertész"], "title": "Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure, 8 tables", "summary": "Efficient text classification is essential for handling the increasing volume\nof academic publications. This study explores the use of pre-trained language\nmodels (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on\nthe Web of Science (WoS-46985) dataset for scientific text classification. To\nenhance performance, we augment the dataset by executing seven targeted queries\nin the WoS database, retrieving 1,000 articles per category aligned with\nWoS-46985's main classes. PLMs predict labels for this unlabeled data, and a\nhard-voting strategy combines predictions for improved accuracy and confidence.\nFine-tuning on the expanded dataset with dynamic learning rates and early\nstopping significantly boosts classification accuracy, especially in\nspecialized domains. Domain-specific models like SciBERT and BioBERT\nconsistently outperform general-purpose models such as BERT. These findings\nunderscore the efficacy of dataset augmentation, inference-driven label\nprediction, hard-voting, and fine-tuning techniques in creating robust and\nscalable solutions for automated academic text classification."}
{"id": "2504.19024", "pdf": "https://arxiv.org/pdf/2504.19024", "abs": "https://arxiv.org/abs/2504.19024", "authors": ["Jiabin Fan", "Guoqing Luo", "Michael Bowling", "Lili Mou"], "title": "KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation", "categories": ["cs.CL"], "comment": null, "summary": "We propose a novel k-step return estimation method (called KETCHUP) for\nReinforcement Learning(RL)-based knowledge distillation (KD) in text generation\ntasks. Our idea is to induce a K-step return by using the Bellman Optimality\nEquation for multiple steps. Theoretical analysis shows that this K-step\nformulation reduces the variance of the gradient estimates, thus leading to\nimproved RL optimization especially when the student model size is large.\nEmpirical evaluation on three text generation tasks demonstrates that our\napproach yields superior performance in both standard task metrics and large\nlanguage model (LLM)-based evaluation. These results suggest that our K-step\nreturn induction offers a promising direction for enhancing RL-based KD in LLM\nresearch."}
{"id": "2504.19044", "pdf": "https://arxiv.org/pdf/2504.19044", "abs": "https://arxiv.org/abs/2504.19044", "authors": ["Di Wu", "Yibin Lei", "Christof Monz"], "title": "Calibrating Translation Decoding with Quality Estimation on LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Neural machine translation (NMT) systems typically employ maximum a\nposteriori (MAP) decoding to select the highest-scoring translation from the\ndistribution mass. However, recent evidence highlights the inadequacy of MAP\ndecoding, often resulting in low-quality or even pathological hypotheses -- the\ndecoding objective is not aligned with real-world translation quality. This\npaper proposes calibrating hypothesis likelihoods with translation quality from\na distribution view by directly optimizing their Pearson correlation -- thereby\nenhancing the effectiveness of translation decoding. With our method,\ntranslation on large language models (LLMs) improves substantially after\nlimited training (2K instances per direction). This improvement is orthogonal\nto those achieved through supervised fine-tuning, leading to substantial gains\nacross a broad range of metrics and human evaluations -- even when applied to\ntop-performing translation-specialized LLMs fine-tuned on high-quality\ntranslation data, such as Tower, or when compared to recent preference\noptimization methods, like CPO. Moreover, the calibrated translation likelihood\ncan directly serve as a strong proxy for translation quality, closely\napproximating or even surpassing some state-of-the-art translation quality\nestimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates\nthat calibration enhances the effectiveness of MAP decoding, thereby enabling\ngreater efficiency in real-world deployment. The resulting state-of-the-art\ntranslation model, which covers 10 languages, along with the accompanying code\nand human evaluation data, has been released to the community:\nhttps://github.com/moore3930/calibrating-llm-mt."}
{"id": "2504.19061", "pdf": "https://arxiv.org/pdf/2504.19061", "abs": "https://arxiv.org/abs/2504.19061", "authors": ["Anindya Bijoy Das", "Shibbir Ahmed", "Shahnewaz Karim Sakib"], "title": "Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, such as reasons for hospital admission, significant\nin-hospital events, and critical follow-up actions. In addition, we also assess\nthe prevalence of various types of hallucinations in the summaries produced by\nthese models. Detecting hallucinations is vital as it directly influences the\nreliability of the information, potentially affecting patient care and\ntreatment outcomes. We conduct comprehensive numerical simulations to\nrigorously evaluate the performance of these models, further probing the\naccuracy and fidelity of the extracted content in clinical summarization."}
{"id": "2504.19066", "pdf": "https://arxiv.org/pdf/2504.19066", "abs": "https://arxiv.org/abs/2504.19066", "authors": ["Deeksha Varshney", "Keane Ong", "Rui Mao", "Erik Cambria", "Gianmarco Mengaldo"], "title": "ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics", "categories": ["cs.CL", "cs.AI", "cs.LG", "physics.ao-ph"], "comment": null, "summary": "Accurate assessments of extreme weather events are vital for research and\npolicy, yet localized and granular data remain scarce in many parts of the\nworld. This data gap limits our ability to analyze potential outcomes and\nimplications of extreme weather events, hindering effective decision-making.\nLarge Language Models (LLMs) can process vast amounts of unstructured text\ndata, extract meaningful insights, and generate detailed assessments by\nsynthesizing information from multiple sources. Furthermore, LLMs can\nseamlessly transfer their general language understanding to smaller models,\nenabling these models to retain key knowledge while being fine-tuned for\nspecific tasks. In this paper, we propose Extreme Weather Reasoning-Aware\nAlignment (EWRA), a method that enhances small language models (SLMs) by\nincorporating structured reasoning paths derived from LLMs, and\nExtremeWeatherNews, a large dataset of extreme weather event-related news\narticles. EWRA and ExtremeWeatherNews together form the overall framework,\nClimaEmpact, that focuses on addressing three critical extreme-weather tasks:\ncategorization of tangible vulnerabilities/impacts, topic labeling, and emotion\nanalysis. By aligning SLMs with advanced reasoning strategies on\nExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for\nSLM alignment), EWRA improves the SLMs' ability to generate well-grounded and\ndomain-specific responses for extreme weather analytics. Our results show that\nthe approach proposed guides SLMs to output domain-aligned responses,\nsurpassing the performance of task-specific models and offering enhanced\nreal-world applicability for extreme weather analytics."}
{"id": "2504.19070", "pdf": "https://arxiv.org/pdf/2504.19070", "abs": "https://arxiv.org/abs/2504.19070", "authors": ["Sakshi Singh", "Abhinav Prakash", "Aakriti Shah", "Chaitanya Sachdeva", "Sanjana Dumpala"], "title": "Sample-Efficient Language Model for Hinglish Conversational AI", "categories": ["cs.CL", "I.2.7; I.2.6; H.5.2"], "comment": "5 pages, 2 tables, 2 figures", "summary": "This paper presents our process for developing a sample-efficient language\nmodel for a conversational Hinglish chatbot. Hinglish, a code-mixed language\nthat combines Hindi and English, presents a unique computational challenge due\nto inconsistent spelling, lack of standardization, and limited quality of\nconversational data. This work evaluates multiple pre-trained cross-lingual\nlanguage models, including Gemma3-4B and Qwen2.5-7B, and employs fine-tuning\ntechniques to improve performance on Hinglish conversational tasks. The\nproposed approach integrates synthetically generated dialogues with insights\nfrom existing Hinglish datasets to address data scarcity. Experimental results\ndemonstrate that models with fewer parameters, when appropriately fine-tuned on\nhigh-quality code-mixed data, can achieve competitive performance for Hinglish\nconversation generation while maintaining computational efficiency."}
{"id": "2504.19095", "pdf": "https://arxiv.org/pdf/2504.19095", "abs": "https://arxiv.org/abs/2504.19095", "authors": ["Jikai Wang", "Juntao Li", "Lijun Wu", "Min Zhang"], "title": "Efficient Reasoning for LLMs through Speculative Chain-of-Thought", "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have\nrecently attracted widespread attention due to their impressive task-solving\nabilities. However, the enormous model size and the generation of lengthy\nthought chains introduce significant reasoning costs and response latency.\nExisting methods for efficient reasoning mainly focus on reducing the number of\nmodel parameters or shortening the chain-of-thought length. In this paper, we\nintroduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency\nfrom another perspective by accelerated average reasoning speed through large\nand small model collaboration. SCoT conducts thought-level drafting using a\nlightweight draft model. Then it selects the best CoT draft and corrects the\nerror cases with the target model. The proposed thinking behavior alignment\nimproves the efficiency of drafting and the draft selection strategy maintains\nthe prediction accuracy for complex problems. Experimental results on GSM8K,\nMATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces\nreasoning latency by 48\\%$\\sim$66\\% for Deepseek-R1-Distill-Qwen-32B while\nachieving near-target-model-level performance. Our code is available at\nhttps://github.com/Jikai0Wang/Speculative_CoT."}
{"id": "2504.19101", "pdf": "https://arxiv.org/pdf/2504.19101", "abs": "https://arxiv.org/abs/2504.19101", "authors": ["Qianren Mao", "Qili Zhang", "Hanwen Hao", "Zhentao Han", "Runhua Xu", "Weifeng Jiang", "Qi Hu", "Zhijun Chen", "Tyler Zhou", "Bo Li", "Yangqiu Song", "Jin Dong", "Jianxin Li", "Philip S. Yu"], "title": "Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising\nsolution for enhancing the accuracy and credibility of Large Language Models\n(LLMs), particularly in Question & Answer tasks. This is achieved by\nincorporating proprietary and private data from integrated databases. However,\nprivate RAG systems face significant challenges due to the scarcity of private\ndomain data and critical data privacy issues. These obstacles impede the\ndeployment of private RAG systems, as developing privacy-preserving RAG systems\nrequires a delicate balance between data security and data availability. To\naddress these challenges, we regard federated learning (FL) as a highly\npromising technology for privacy-preserving RAG services. We propose a novel\nframework called Federated Retrieval-Augmented Generation (FedE4RAG). This\nframework facilitates collaborative training of client-side RAG retrieval\nmodels. The parameters of these models are aggregated and distributed on a\ncentral-server, ensuring data privacy without direct sharing of raw data. In\nFedE4RAG, knowledge distillation is employed for communication between the\nserver and client models. This technique improves the generalization of local\nRAG retrievers during the federated learning process. Additionally, we apply\nhomomorphic encryption within federated learning to safeguard model parameters\nand mitigate concerns related to data leakage. Extensive experiments conducted\non the real-world dataset have validated the effectiveness of FedE4RAG. The\nresults demonstrate that our proposed framework can markedly enhance the\nperformance of private RAG systems while maintaining robust data privacy\nprotection."}
{"id": "2504.19110", "pdf": "https://arxiv.org/pdf/2504.19110", "abs": "https://arxiv.org/abs/2504.19110", "authors": ["Huajian Xin", "Luming Li", "Xiaoran Jin", "Jacques Fleuriot", "Wenda Li"], "title": "APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries", "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has shown promise in formal\ntheorem proving, yet existing benchmarks remain limited to isolated, static\nproof tasks, failing to capture the iterative, engineering-intensive workflows\nof real-world formal mathematics libraries. Motivated by analogous advances in\nsoftware engineering, we introduce the paradigm of Automated Proof Engineering\n(APE), which aims to automate proof engineering tasks such as feature addition,\nproof refactoring, and bug fixing using LLMs. To facilitate research in this\ndirection, we present APE-Bench I, the first realistic benchmark built from\nreal-world commit histories of Mathlib4, featuring diverse file-level tasks\ndescribed in natural language and verified via a hybrid approach combining the\nLean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable\nparallel verification infrastructure optimized for proof checking across\nmultiple versions of Mathlib. Empirical results on state-of-the-art LLMs\ndemonstrate strong performance on localized edits but substantial degradation\non handling complex proof engineering. This work lays the foundation for\ndeveloping agentic workflows in proof engineering, with future benchmarks\ntargeting multi-file coordination, project-scale verification, and autonomous\nagents capable of planning, editing, and repairing formal libraries."}
{"id": "2504.19162", "pdf": "https://arxiv.org/pdf/2504.19162", "abs": "https://arxiv.org/abs/2504.19162", "authors": ["Jiaqi Chen", "Bang Zhang", "Ruotian Ma", "Peisong Wang", "Xiaodan Liang", "Zhaopeng Tu", "Xiaolong Li", "Kwan-Yee K. Wong"], "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project: https://chen-judge.github.io/SPC/", "summary": "Evaluating the step-by-step reliability of large language model (LLM)\nreasoning, such as Chain-of-Thought, remains challenging due to the difficulty\nand cost of obtaining high-quality step-level supervision. In this paper, we\nintroduce Self-Play Critic (SPC), a novel approach where a critic model evolves\nits ability to assess reasoning steps through adversarial self-play games,\neliminating the need for manual step-level annotation. SPC involves fine-tuning\ntwo copies of a base model to play two roles, namely a \"sneaky generator\" that\ndeliberately produces erroneous steps designed to be difficult to detect, and a\n\"critic\" that analyzes the correctness of reasoning steps. These two models\nengage in an adversarial game in which the generator aims to fool the critic,\nwhile the critic model seeks to identify the generator's errors. Using\nreinforcement learning based on the game outcomes, the models iteratively\nimprove; the winner of each confrontation receives a positive reward and the\nloser receives a negative reward, driving continuous self-evolution.\nExperiments on three reasoning process benchmarks (ProcessBench, PRM800K,\nDeltaBench) demonstrate that our SPC progressively enhances its error detection\ncapabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and\nsurpasses strong baselines, including distilled R1 model. Furthermore, applying\nSPC to guide the test-time search of diverse LLMs significantly improves their\nmathematical reasoning performance on MATH500 and AIME2024, outperforming\nstate-of-the-art process reward models."}
{"id": "2504.19191", "pdf": "https://arxiv.org/pdf/2504.19191", "abs": "https://arxiv.org/abs/2504.19191", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "WuNeng: Hybrid State with Attention", "categories": ["cs.CL"], "comment": null, "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."}
{"id": "2504.19209", "pdf": "https://arxiv.org/pdf/2504.19209", "abs": "https://arxiv.org/abs/2504.19209", "authors": ["Elisabeth Fittschen", "Bella Xia", "Leib Celnik", "Paul Dilley", "Tom Lippincott"], "title": "Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora", "categories": ["cs.CL", "cs.LG"], "comment": "Under review", "summary": "We measure the effects of several implementation choices for the Dynamic\nEmbedded Topic Model, as applied to five distinct diachronic corpora, with the\ngoal of isolating important decisions for its use and further development. We\nidentify priorities that will maximize utility in applied scholarship,\nincluding the practical scalability of vocabulary size to best exploit the\nstrengths of embedded representations, and more flexible modeling of intervals\nto accommodate the uneven temporal distributions of historical writing. Of\nsimilar importance, we find performance is not significantly or consistently\naffected by several aspects that otherwise limit the model's application or\nmight consume the resources of a grid search."}
{"id": "2504.19254", "pdf": "https://arxiv.org/pdf/2504.19254", "abs": "https://arxiv.org/abs/2504.19254", "authors": ["Dylan Bouchard", "Mohit Singh Chauhan"], "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "UQLM repository: https://github.com/cvs-health/uqlm", "summary": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs."}
{"id": "2504.19267", "pdf": "https://arxiv.org/pdf/2504.19267", "abs": "https://arxiv.org/abs/2504.19267", "authors": ["Mohamed Gado", "Towhid Taliee", "Muhammad Memon", "Dmitry Ignatov", "Radu Timofte"], "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Visual storytelling is an interdisciplinary field combining computer vision\nand natural language processing to generate cohesive narratives from sequences\nof images. This paper presents a novel approach that leverages recent\nadvancements in multimodal models, specifically adapting transformer-based\narchitectures and large multimodal models, for the visual storytelling task.\nLeveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT\nmodel produces visually grounded, contextually appropriate narratives. We\naddress the limitations of traditional evaluation metrics, such as BLEU,\nMETEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we\nutilize RoViST and GROOVIST, novel reference-free metrics designed to assess\nvisual storytelling, focusing on visual grounding, coherence, and\nnon-redundancy. These metrics provide a more nuanced evaluation of narrative\nquality, aligning closely with human judgment."}
{"id": "2504.19298", "pdf": "https://arxiv.org/pdf/2504.19298", "abs": "https://arxiv.org/abs/2504.19298", "authors": ["Hanyu Lai", "Junjie Gao", "Xiao Liu", "Yifan Xu", "Shudan Zhang", "Yuxiao Dong", "Jie Tang"], "title": "AndroidGen: Building an Android Language Agent under Data Scarcity", "categories": ["cs.CL"], "comment": null, "summary": "Large language models have opened up a world of possibilities for various NLP\ntasks, sparking optimism for the future. Despite their potential, LLMs have yet\nto be widely used as agents on real mobile devices. The main challenge is the\nneed for high-quality data sources. Time constraints and labor intensity often\nhinder human annotation. On the other hand, existing LLMs exhibit inadequate\ncompletion rates and need a robust data filtration strategy. Given these\nchallenges, we develop a framework called AndroidGen to enhance the\ncapabilities of LLM-based agents under data scarcity. In addition, we leverage\nAndroidGen to collect trajectories given human tasks and train open-source LLMs\non these trajectories to develop an open-source mobile agent without manually\nlabeled trajectories. We extensively evaluate AndroidGen with AndroidWorld,\nAitW, and various popular applications, demonstrating its improvements and\nrevealing potential areas for future improvement. Code, model, and data are\navailable at https://github.com/THUDM/AndroidGen."}
{"id": "2504.19314", "pdf": "https://arxiv.org/pdf/2504.19314", "abs": "https://arxiv.org/abs/2504.19314", "authors": ["Peilin Zhou", "Bruce Leon", "Xiang Ying", "Can Zhang", "Yifan Shao", "Qichen Ye", "Dading Chong", "Zhiling Jin", "Chenxuan Xie", "Meng Cao", "Yuxin Gu", "Sixin Hong", "Jing Ren", "Jian Chen", "Chao Liu", "Yining Hua"], "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese", "categories": ["cs.CL"], "comment": "Under Review", "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH."}
{"id": "2504.19333", "pdf": "https://arxiv.org/pdf/2504.19333", "abs": "https://arxiv.org/abs/2504.19333", "authors": ["James O' Neill", "Santhosh Subramanian", "Eric Lin", "Vaikkunth Mugunthan"], "title": "Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The trend towards large language models (LLMs) for guardrailing against\nundesired behaviors is increasing and has shown promise for censoring user\ninputs. However, increased latency, memory consumption, hosting expenses and\nnon-structured outputs can make their use prohibitive.\n  In this work, we show that task-specific data generation can lead to\nfine-tuned classifiers that significantly outperform current state of the art\n(SoTA) while being orders of magnitude smaller. Secondly, we show that using a\nsingle model, \\texttt{MultiTaskGuard}, that is pretrained on a large\nsynthetically generated dataset with unique task instructions further improves\ngeneralization. Thirdly, our most performant models, \\texttt{UniGuard}, are\nfound using our proposed search-based model merging approach that finds an\noptimal set of parameters to combine single-policy models and multi-policy\nguardrail models. % On 7 public datasets and 4 guardrail benchmarks we created,\nour efficient guardrail classifiers improve over the best performing SoTA\npublicly available LLMs and 3$^{\\text{rd}}$ party guardrail APIs in detecting\nunsafe and safe behaviors by an average F1 score improvement of \\textbf{29.92}\npoints over Aegis-LlamaGuard and \\textbf{21.62} over \\texttt{gpt-4o},\nrespectively. Lastly, our guardrail synthetic data generation process that uses\ncustom task-specific guardrail poli"}
{"id": "2504.19339", "pdf": "https://arxiv.org/pdf/2504.19339", "abs": "https://arxiv.org/abs/2504.19339", "authors": ["Dongqi Liu", "Xi Yu", "Vera Demberg", "Mirella Lapata"], "title": "Explanatory Summarization with Discourse-Driven Planning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by the Transactions of the Association for Computational\n  Linguistics (TACL)", "summary": "Lay summaries for scientific documents typically include explanations to help\nreaders grasp sophisticated concepts or arguments. However, current automatic\nsummarization methods do not explicitly model explanations, which makes it\ndifficult to align the proportion of explanatory content with human-written\nsummaries. In this paper, we present a plan-based approach that leverages\ndiscourse frameworks to organize summary generation and guide explanatory\nsentences by prompting responses to the plan. Specifically, we propose two\ndiscourse-driven planning strategies, where the plan is conditioned as part of\nthe input or part of the output prefix, respectively. Empirical experiments on\nthree lay summarization datasets show that our approach outperforms existing\nstate-of-the-art methods in terms of summary quality, and it enhances model\nrobustness, controllability, and mitigates hallucination."}
{"id": "2504.19395", "pdf": "https://arxiv.org/pdf/2504.19395", "abs": "https://arxiv.org/abs/2504.19395", "authors": ["Zhouxiang Fang", "Aayush Mishra", "Muhan Gao", "Anqi Liu", "Daniel Khashabi"], "title": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via Substitution Ciphers", "categories": ["cs.CL"], "comment": null, "summary": "Recent works have suggested that In-Context Learning (ICL) operates in dual\nmodes, i.e. task retrieval (remember learned patterns from pre-training) and\ntask learning (inference-time ``learning'' from demonstrations). However,\ndisentangling these the two modes remains a challenging goal. We introduce ICL\nCIPHERS, a class of task reformulations based on substitution ciphers borrowed\nfrom classic cryptography. In this approach, a subset of tokens in the\nin-context inputs are substituted with other (irrelevant) tokens, rendering\nEnglish sentences less comprehensible to human eye. However, by design, there\nis a latent, fixed pattern to this substitution, making it reversible. This\nbijective (reversible) cipher ensures that the task remains a well-defined task\nin some abstract sense, despite the transformations. It is a curious question\nif LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires\ndeciphering the latent cipher. We show that LLMs are better at solving ICL\nCIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,\nproviding a novel approach to quantify ``learning'' in ICL. While this gap is\nsmall, it is consistent across the board on four datasets and six models.\nFinally, we examine LLMs' internal representations and identify evidence in\ntheir ability to decode the ciphered inputs."}
{"id": "2504.19406", "pdf": "https://arxiv.org/pdf/2504.19406", "abs": "https://arxiv.org/abs/2504.19406", "authors": ["Mengxia Yu", "Bang Nguyen", "Olivia Zino", "Meng Jiang"], "title": "Context Selection and Rewriting for Video-based EducationalQuestion Generation", "categories": ["cs.CL"], "comment": null, "summary": "Educational question generation (EQG) is a crucial component of intelligent\neducational systems, significantly aiding self-assessment, active learning, and\npersonalized education. While EQG systems have emerged, existing datasets\ntypically rely on predefined, carefully edited texts, failing to represent\nreal-world classroom content, including lecture speech with a set of\ncomplementary slides. To bridge this gap, we collect a dataset of educational\nquestions based on lectures from real-world classrooms. On this realistic\ndataset, we find that current methods for EQG struggle with accurately\ngenerating questions from educational videos, particularly in aligning with\nspecific timestamps and target answers. Common challenges include selecting\ninformative contexts from extensive transcripts and ensuring generated\nquestions meaningfully incorporate the target answer. To address the\nchallenges, we introduce a novel framework utilizing large language models for\ndynamically selecting and rewriting contexts based on target timestamps and\nanswers. First, our framework selects contexts from both lecture transcripts\nand video keyframes based on answer relevance and temporal proximity. Then, we\nintegrate the contexts selected from both modalities and rewrite them into\nanswer-containing knowledge statements, to enhance the logical connection\nbetween the contexts and the desired answer. This approach significantly\nimproves the quality and relevance of the generated questions. Our dataset and\ncode are released in https://github.com/mengxiayu/COSER."}
{"id": "2504.19413", "pdf": "https://arxiv.org/pdf/2504.19413", "abs": "https://arxiv.org/abs/2504.19413", "authors": ["Prateek Chhikara", "Dev Khant", "Saket Aryan", "Taranjeet Singh", "Deshraj Yadav"], "title": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable prowess in\ngenerating contextually coherent responses, yet their fixed context windows\npose fundamental challenges for maintaining consistency over prolonged\nmulti-session dialogues. We introduce Mem0, a scalable memory-centric\narchitecture that addresses this issue by dynamically extracting,\nconsolidating, and retrieving salient information from ongoing conversations.\nBuilding on this foundation, we further propose an enhanced variant that\nleverages graph-based memory representations to capture complex relational\nstructures among conversational elements. Through comprehensive evaluations on\nLOCOMO benchmark, we systematically compare our approaches against six baseline\ncategories: (i) established memory-augmented systems, (ii) retrieval-augmented\ngeneration (RAG) with varying chunk sizes and k-values, (iii) a full-context\napproach that processes the entire conversation history, (iv) an open-source\nmemory solution, (v) a proprietary model system, and (vi) a dedicated memory\nmanagement platform. Empirical results show that our methods consistently\noutperform all existing memory systems across four question categories:\nsingle-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26%\nrelative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with\ngraph memory achieves around 2% higher overall score than the base\nconfiguration. Beyond accuracy gains, we also markedly reduce computational\noverhead compared to full-context method. In particular, Mem0 attains a 91%\nlower p95 latency and saves more than 90% token cost, offering a compelling\nbalance between advanced reasoning capabilities and practical deployment\nconstraints. Our findings highlight critical role of structured, persistent\nmemory mechanisms for long-term conversational coherence, paving the way for\nmore reliable and efficient LLM-driven AI agents."}
{"id": "2504.19436", "pdf": "https://arxiv.org/pdf/2504.19436", "abs": "https://arxiv.org/abs/2504.19436", "authors": ["Jacky He", "Guiran Liu", "Binrong Zhu", "Hanlu Zhang", "Hongye Zheng", "Xiaokai Wang"], "title": "Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper focuses on the dynamic optimization of the Retrieval-Augmented\nGeneration (RAG) architecture. It proposes a state-aware dynamic knowledge\nretrieval mechanism to enhance semantic understanding and knowledge scheduling\nefficiency in large language models for open-domain question answering and\ncomplex generation tasks. The method introduces a multi-level perceptive\nretrieval vector construction strategy and a differentiable document matching\npath. These components enable end-to-end joint training and collaborative\noptimization of the retrieval and generation modules. This effectively\naddresses the limitations of static RAG structures in context adaptation and\nknowledge access. Experiments are conducted on the Natural Questions dataset.\nThe proposed structure is thoroughly evaluated across different large models,\nincluding GPT-4, GPT-4o, and DeepSeek. Comparative and ablation experiments\nfrom multiple perspectives confirm the significant improvements in BLEU and\nROUGE-L scores. The approach also demonstrates stronger robustness and\ngeneration consistency in tasks involving semantic ambiguity and multi-document\nfusion. These results highlight its broad application potential and practical\nvalue in building high-quality language generation systems."}
{"id": "2504.19445", "pdf": "https://arxiv.org/pdf/2504.19445", "abs": "https://arxiv.org/abs/2504.19445", "authors": ["Yi-Long Lu", "Chunhui Zhang", "Wei Wang"], "title": "Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in tasks such as\npsychological text analysis and decision-making in automated workflows.\nHowever, their reliability remains a concern due to potential biases inherited\nfrom their training process. In this study, we examine how different response\nformat: binary versus continuous, may systematically influence LLMs' judgments.\nIn a value statement judgments task and a text sentiment analysis task, we\nprompted LLMs to simulate human responses and tested both formats across\nseveral models, including both open-source and commercial models. Our findings\nrevealed a consistent negative bias: LLMs were more likely to deliver\n\"negative\" judgments in binary formats compared to continuous ones. Control\nexperiments further revealed that this pattern holds across both tasks. Our\nresults highlight the importance of considering response format when applying\nLLMs to decision tasks, as small changes in task design can introduce\nsystematic biases."}
{"id": "2504.19457", "pdf": "https://arxiv.org/pdf/2504.19457", "abs": "https://arxiv.org/abs/2504.19457", "authors": ["Siyi Liu", "Kishaloy Halder", "Zheng Qi", "Wei Xiao", "Nikolaos Pappas", "Phu Mon Htut", "Neha Anna John", "Yassine Benajiba", "Dan Roth"], "title": "Towards Long Context Hallucination Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. However, they are prone to contextual hallucination, generating\ninformation that is either unsubstantiated or contradictory to the given\ncontext. Although many studies have investigated contextual hallucinations in\nLLMs, addressing them in long-context inputs remains an open problem. In this\nwork, we take an initial step toward solving this problem by constructing a\ndataset specifically designed for long-context hallucination detection.\nFurthermore, we propose a novel architecture that enables pre-trained encoder\nmodels, such as BERT, to process long contexts and effectively detect\ncontextual hallucinations through a decomposition and aggregation mechanism.\nOur experimental results show that the proposed architecture significantly\noutperforms previous models of similar size as well as LLM-based models across\nvarious metrics, while providing substantially faster inference."}
{"id": "2504.19467", "pdf": "https://arxiv.org/pdf/2504.19467", "abs": "https://arxiv.org/abs/2504.19467", "authors": ["Jiageng Wu", "Bowen Gu", "Ren Zhou", "Kevin Xie", "Doug Snyder", "Yixing Jiang", "Valentina Carducci", "Richard Wyss", "Rishi J Desai", "Emily Alsentzer", "Leo Anthony Celi", "Adam Rodman", "Sebastian Schneeweiss", "Jonathan H. Chen", "Santiago Romero-Brufau", "Kueiyu Joshua Lin", "Jie Yang"], "title": "BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold great promise for medical applications and\nare evolving rapidly, with new models being released at an accelerated pace.\nHowever, current evaluations of LLMs in clinical contexts remain limited. Most\nexisting benchmarks rely on medical exam-style questions or PubMed-derived\ntext, failing to capture the complexity of real-world electronic health record\n(EHR) data. Others focus narrowly on specific application scenarios, limiting\ntheir generalizability across broader clinical use. To address this gap, we\npresent BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks\nsourced from real-world clinical data sources across nine languages. We\nsystematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,\nGPT-4o, Gemini, and Llama 4) under various inference strategies. With a total\nof 13,572 experiments, our results reveal substantial performance variation\nacross model sizes, languages, natural language processing tasks, and clinical\nspecialties. Notably, we demonstrate that open-source LLMs can achieve\nperformance comparable to proprietary models, while medically fine-tuned LLMs\nbased on older architectures often underperform versus updated general-purpose\nmodels. The BRIDGE and its corresponding leaderboard serve as a foundational\nresource and a unique reference for the development and evaluation of new LLMs\nin real-world clinical text understanding."}
{"id": "2504.19472", "pdf": "https://arxiv.org/pdf/2504.19472", "abs": "https://arxiv.org/abs/2504.19472", "authors": ["Siyi Liu", "Dan Roth"], "title": "Conflicts in Texts: Data, Implications and Challenges", "categories": ["cs.CL"], "comment": null, "summary": "As NLP models become increasingly integrated into real-world applications, it\nbecomes clear that there is a need to address the fact that models often rely\non and generate conflicting information. Conflicts could reflect the complexity\nof situations, changes that need to be explained and dealt with, difficulties\nin data annotation, and mistakes in generated outputs. In all cases,\ndisregarding the conflicts in data could result in undesired behaviors of\nmodels and undermine NLP models' reliability and trustworthiness. This survey\ncategorizes these conflicts into three key areas: (1) natural texts on the web,\nwhere factual inconsistencies, subjective biases, and multiple perspectives\nintroduce contradictions; (2) human-annotated data, where annotator\ndisagreements, mistakes, and societal biases impact model training; and (3)\nmodel interactions, where hallucinations and knowledge conflicts emerge during\ndeployment. While prior work has addressed some of these conflicts in\nisolation, we unify them under the broader concept of conflicting information,\nanalyze their implications, and discuss mitigation strategies. We highlight key\nchallenges and future directions for developing conflict-aware NLP systems that\ncan reason over and reconcile conflicting information more effectively."}
{"id": "2504.19556", "pdf": "https://arxiv.org/pdf/2504.19556", "abs": "https://arxiv.org/abs/2504.19556", "authors": ["Kristen Sussman", "Daniel Carter"], "title": "Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment", "categories": ["cs.CL", "cs.HC", "J.4; K.4.0; I.2.7"], "comment": "5 pages, 3 figures, Companion Proceedings of the ACM Web Conference\n  2025", "summary": "Given the subtle human-like effects of large language models on linguistic\npatterns, this study examines shifts in language over time to detect the impact\nof AI-mediated communication (AI- MC) on social media. We compare a replicated\ndataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the\nsame period in 2024, all of which mention Donald Trump during election periods.\nUsing a combination of Flesch-Kincaid readability and polarity scores, we\nanalyze changes in text complexity and sentiment. Our findings reveal a\nsignificant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift\nfrom predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more\npositive expressions (28.6% to 45.9%). These findings suggest not only an\nincreasing presence of AI in social media communication but also its impact on\nlanguage and emotional expression patterns."}
{"id": "2504.19565", "pdf": "https://arxiv.org/pdf/2504.19565", "abs": "https://arxiv.org/abs/2504.19565", "authors": ["Meng Xiao", "Xunxin Cai", "Chengrui Wang", "Yuanchun Zhou"], "title": "m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training", "categories": ["cs.CL", "cs.AI", "q-bio.QM"], "comment": "22 pages, Large Language Model, Agentic AI, Dataset Distillation,\n  Multi-agent Collaboration", "summary": "The rapid progress of large language models (LLMs) in biomedical research has\nunderscored the limitations of existing open-source annotated scientific\ncorpora, which are often insufficient in quantity and quality. Addressing the\nchallenge posed by the complex hierarchy of biomedical knowledge, we propose a\nknowledge-driven, multi-agent framework for scientific corpus distillation\ntailored for LLM training in the biomedical domain. Central to our approach is\na collaborative multi-agent architecture, where specialized agents, each guided\nby the Medical Subject Headings (MeSH) hierarchy, work in concert to\nautonomously extract, synthesize, and self-evaluate high-quality textual data\nfrom vast scientific literature. These agents collectively generate and refine\ndomain-specific question-answer pairs, ensuring comprehensive coverage and\nconsistency with biomedical ontologies while minimizing manual involvement.\nExtensive experimental results show that language models trained on our\nmulti-agent distilled datasets achieve notable improvements in biomedical\nquestion-answering tasks, outperforming both strong life sciences LLM baselines\nand advanced proprietary models. Notably, our AI-Ready dataset enables\nLlama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger\nscale. Detailed ablation studies and case analyses further validate the\neffectiveness and synergy of each agent within the framework, highlighting the\npotential of multi-agent collaboration in biomedical LLM training."}
{"id": "2504.19590", "pdf": "https://arxiv.org/pdf/2504.19590", "abs": "https://arxiv.org/abs/2504.19590", "authors": ["Israa Alsiyat"], "title": "Arabic Metaphor Sentiment Classification Using Semantic Information", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1]\nusing newly designed automatic tools for sentiment classification for AMC based\non semantic tags. The tool incorporates semantic emotional tags for sentiment\nclassification. I evaluate the tool using standard methods, which are F-score,\nrecall, and precision. The method is to show the impact of Arabic online\nmetaphors on sentiment through the newly designed tools. To the best of our\nknowledge, this is the first approach to conduct sentiment classification for\nArabic metaphors using semantic tags to find the impact of the metaphor."}
{"id": "2504.19606", "pdf": "https://arxiv.org/pdf/2504.19606", "abs": "https://arxiv.org/abs/2504.19606", "authors": ["Hieu-Dai Tran", "Duc-Vu Nguyen", "Ngan Luu-Thuy Nguyen"], "title": "Coreference Resolution for Vietnamese Narrative Texts", "categories": ["cs.CL"], "comment": "Accepted at PACLIC 2024", "summary": "Coreference resolution is a vital task in natural language processing (NLP)\nthat involves identifying and linking different expressions in a text that\nrefer to the same entity. This task is particularly challenging for Vietnamese,\na low-resource language with limited annotated datasets. To address these\nchallenges, we developed a comprehensive annotated dataset using narrative\ntexts from VnExpress, a widely-read Vietnamese online news platform. We\nestablished detailed guidelines for annotating entities, focusing on ensuring\nconsistency and accuracy. Additionally, we evaluated the performance of large\nlanguage models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset.\nOur results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in\nterms of both accuracy and response consistency, making it a more reliable tool\nfor coreference resolution in Vietnamese."}
{"id": "2504.19627", "pdf": "https://arxiv.org/pdf/2504.19627", "abs": "https://arxiv.org/abs/2504.19627", "authors": ["Run Luo", "Renke Shan", "Longze Chen", "Ziqiang Liu", "Lu Wang", "Min Yang", "Xiaobo Xia"], "title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "VCM", "summary": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like\nembodied intelligence due to their strong vision-language reasoning abilities.\nHowever, current LVLMs process entire images at the token level, which is\ninefficient compared to humans who analyze information and generate content at\nthe conceptual level, extracting relevant visual concepts with minimal effort.\nThis inefficiency, stemming from the lack of a visual concept model, limits\nLVLMs' usability in real-world applications. To address this, we propose VCM,\nan end-to-end self-supervised visual concept modeling framework. VCM leverages\nimplicit contrastive learning across multiple sampled instances and\nvision-language fine-tuning to construct a visual concept model without\nrequiring costly concept-level annotations. Our results show that VCM\nsignificantly reduces computational costs (e.g., 85\\% fewer FLOPs for\nLLaVA-1.5-7B) while maintaining strong performance across diverse image\nunderstanding tasks. Moreover, VCM enhances visual encoders' capabilities in\nclassic visual concept perception tasks. Extensive quantitative and qualitative\nexperiments validate the effectiveness and efficiency of VCM."}
{"id": "2504.19645", "pdf": "https://arxiv.org/pdf/2504.19645", "abs": "https://arxiv.org/abs/2504.19645", "authors": ["Shadan Shukr Sabr", "Nazira Sabr Mustafa", "Talar Sabah Omar", "Salah Hwayyiz Rasool", "Nawzad Anwer Omer", "Darya Sabir Hamad", "Hemin Abdulhameed Shams", "Omer Mahmood Kareem", "Rozhan Noori Abdullah", "Khabat Atar Abdullah", "Mahabad Azad Mohammad", "Haneen Al-Raghefy", "Safar M. Asaad", "Sara Jamal Mohammed", "Twana Saeed Ali", "Fazil Shawrow", "Halgurd S. Maghdid"], "title": "A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks", "categories": ["cs.CL", "cs.AI", "K.5; K.7; J.7"], "comment": "25 pages, 4 figures, 2 tables", "summary": "- The field of natural language processing (NLP) has dramatically expanded\nwithin the last decade. Many human-being applications are conducted daily via\nNLP tasks, starting from machine translation, speech recognition, text\ngeneration and recommendations, Part-of-Speech tagging (POS), and Named-Entity\nRecognition (NER). However, low-resourced languages, such as the\nCentral-Kurdish language (CKL), mainly remain unexamined due to shortage of\nnecessary resources to support their development. The POS tagging task is the\nbase of other NLP tasks; for example, the POS tag set has been used to\nstandardized languages to provide the relationship between words among the\nsentences, followed by machine translation and text recommendation.\nSpecifically, for the CKL, most of the utilized or provided POS tagsets are\nneither standardized nor comprehensive. To this end, this study presented an\naccurate and comprehensive POS tagset for the CKL to provide better performance\nof the Kurdish NLP tasks. The article also collected most of the POS tags from\ndifferent studies as well as from Kurdish linguistic experts to standardized\npart-of-speech tags. The proposed POS tagset is designed to annotate a large\nCKL corpus and support Kurdish NLP tasks. The initial investigations of this\nstudy via comparison with the Universal Dependencies framework for standard\nlanguages, show that the proposed POS tagset can streamline or correct\nsentences more accurately for Kurdish NLP tasks."}
{"id": "2504.19669", "pdf": "https://arxiv.org/pdf/2504.19669", "abs": "https://arxiv.org/abs/2504.19669", "authors": ["Chen Su", "Yuanhe Tian", "Yan Song"], "title": "Multimodal Conditioned Diffusive Time Series Forecasting", "categories": ["cs.CL"], "comment": null, "summary": "Diffusion models achieve remarkable success in processing images and text,\nand have been extended to special domains such as time series forecasting\n(TSF). Existing diffusion-based approaches for TSF primarily focus on modeling\nsingle-modality numerical sequences, overlooking the rich multimodal\ninformation in time series data. To effectively leverage such information for\nprediction, we propose a multimodal conditioned diffusion model for TSF,\nnamely, MCD-TSF, to jointly utilize timestamps and texts as extra guidance for\ntime series modeling, especially for forecasting. Specifically, Timestamps are\ncombined with time series to establish temporal and semantic correlations among\ndifferent data points when aggregating information along the temporal\ndimension. Texts serve as supplementary descriptions of time series' history,\nand adaptively aligned with data points as well as dynamically controlled in a\nclassifier-free manner. Extensive experiments on real-world benchmark datasets\nacross eight domains demonstrate that the proposed MCD-TSF model achieves\nstate-of-the-art performance."}
{"id": "2504.19675", "pdf": "https://arxiv.org/pdf/2504.19675", "abs": "https://arxiv.org/abs/2504.19675", "authors": ["Osma Suominen", "Juho Inkinen", "Mona Lehtinen"], "title": "Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG", "I.2.7"], "comment": "6 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects", "summary": "This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),\nwhich focussed on subject indexing using large language models (LLMs). The task\nrequired creating subject predictions for bibliographic records from the\nbilingual TIBKAT database using the GND subject vocabulary. Our approach\ncombines traditional natural language processing and machine learning\ntechniques implemented in the Annif toolkit with innovative LLM-based methods\nfor translation and synthetic data generation, and merging predictions from\nmonolingual models. The system ranked first in the all-subjects category and\nsecond in the tib-core-subjects category in the quantitative evaluation, and\nfourth in qualitative evaluations. These findings demonstrate the potential of\ncombining traditional XMTC algorithms with modern LLM techniques to improve the\naccuracy and efficiency of subject indexing in multilingual contexts."}
{"id": "2504.19720", "pdf": "https://arxiv.org/pdf/2504.19720", "abs": "https://arxiv.org/abs/2504.19720", "authors": ["Ranran Zhen", "Juntao Li", "Yixin Ji", "Zhenlin Yang", "Tong Liu", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "comment": "work in progress;11 pages of main paper with 7 main figures, overall\n  20 pages", "summary": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving."}
{"id": "2504.19734", "pdf": "https://arxiv.org/pdf/2504.19734", "abs": "https://arxiv.org/abs/2504.19734", "authors": ["Ying Na", "Shihui Feng"], "title": "LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Dialogue data has been a key source for understanding learning processes,\noffering critical insights into how students engage in collaborative\ndiscussions and how these interactions shape their knowledge construction. The\nadvent of Large Language Models (LLMs) has introduced promising opportunities\nfor advancing qualitative research, particularly in the automated coding of\ndialogue data. However, the inherent contextual complexity of dialogue presents\nunique challenges for these models, especially in understanding and\ninterpreting complex contextual information. This study addresses these\nchallenges by developing a novel LLM-assisted automated coding approach for\ndialogue data. The novelty of our proposed framework is threefold: 1) We\npredict the code for an utterance based on dialogue-specific characteristics --\ncommunicative acts and communicative events -- using separate prompts following\nthe role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs\nincluding GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We\nleveraged the interrelation between events and acts to implement consistency\nchecking using GPT-4o. In particular, our contextual consistency checking\nprovided a substantial accuracy improvement. We also found the accuracy of act\npredictions was consistently higher than that of event predictions. This study\ncontributes a new methodological framework for enhancing the precision of\nautomated coding of dialogue data as well as offers a scalable solution for\naddressing the contextual challenges inherent in dialogue analysis."}
{"id": "2504.19759", "pdf": "https://arxiv.org/pdf/2504.19759", "abs": "https://arxiv.org/abs/2504.19759", "authors": ["Huichi Zhou", "Zehao Xu", "Munan Zhao", "Kaihong Li", "Yiqiang Li", "Hongtao Wang"], "title": "Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs", "categories": ["cs.CL"], "comment": "5 pages, 2 figures", "summary": "In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB)\nto evaluate the moral reasoning abilities of large language models (LLMs)\nacross five typologically diverse languages and three levels of contextual\ncomplexity: sentence, paragraph, and document. Our results show moral reasoning\nperformance degrades with increasing context complexity, particularly for\nlow-resource languages such as Vietnamese. We further fine-tune the open-source\nLLaMA-3-8B model using curated monolingual data for alignment and poisoning.\nSurprisingly, low-resource languages have a stronger impact on multilingual\nreasoning than high-resource ones, highlighting their critical role in\nmultilingual NLP."}
{"id": "2504.19811", "pdf": "https://arxiv.org/pdf/2504.19811", "abs": "https://arxiv.org/abs/2504.19811", "authors": ["Takuya Tamura", "Taro Yano", "Masafumi Enomoto", "Masafumi Oyamada"], "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance", "categories": ["cs.CL"], "comment": null, "summary": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships - i.e., which models are derived\nor merged from which parents. In this work, we propose a novel\nLineage-Regularized Matrix Factorization (LRMF) framework that encodes\nancestral ties among LLMs via a graph Laplacian regularizer. By leveraging\nmulti-hop parent-child connections, LRMF consistently outperforms conventional\nmatrix factorization and collaborative filtering methods in both instance-level\nand benchmark-level performance prediction. Our large-scale study includes\n2,934 publicly available Hugging Face models and 21,000+ instances across 6\nmajor benchmarks, showing that lineage constraints yield up to 7-10 percentage\npoints higher correlation with actual performance compared to baselines.\nMoreover, LRMF effectively addresses the cold-start problem, providing accurate\nestimates for newly derived or merged models even with minimal data. This\nlineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment."}
{"id": "2504.19850", "pdf": "https://arxiv.org/pdf/2504.19850", "abs": "https://arxiv.org/abs/2504.19850", "authors": ["Kyo Gerrits", "Ana Guerberof-Arenas"], "title": "To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels", "categories": ["cs.CL"], "comment": "This paper has been accepted to the MT Summit 2025 to be held in\n  Geneva on June 23-27 2025", "summary": "This article presents the results of a pilot study involving the reception of\na fictional short story translated from English into Dutch under four\nconditions: machine translation (MT), post-editing (PE), human translation (HT)\nand original source text (ST). The aim is to understand how creativity and\nerrors in different translation modalities affect readers, specifically\nregarding cognitive load. Eight participants filled in a questionnaire, read a\nstory using an eye-tracker, and conducted a retrospective think-aloud (RTA)\ninterview. The results show that units of creative potential (UCP) increase\ncognitive load and that this effect is highest for HT and lowest for MT; no\neffect of error was observed. Triangulating the data with RTAs leads us to\nhypothesize that the higher cognitive load in UCPs is linked to increases in\nreader enjoyment and immersion. The effect of translation creativity on\ncognitive load in different translation modalities at word-level is novel and\nopens up new avenues for further research. All the code and data are available\nat https://github.com/INCREC/Pilot_to_MT_or_not_to_MT"}
{"id": "2504.19856", "pdf": "https://arxiv.org/pdf/2504.19856", "abs": "https://arxiv.org/abs/2504.19856", "authors": ["Anastasia Zhukova", "Christian E. Matt", "Terry Ruas", "Bela Gipp"], "title": "Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language", "categories": ["cs.CL"], "comment": null, "summary": "Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique\nthat further trains a language model (LM) on its pretraining task, e.g.,\nlanguage masking. Although popular, it requires a significant corpus of\ndomain-related data, which is difficult to obtain for specific domains in\nlanguages other than English, such as the process industry in the German\nlanguage. This paper introduces an efficient approach called ICL-augmented\npretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest\nneighbors (kNN) to augment target data with domain-related and in-domain texts,\nsignificantly reducing GPU time while maintaining strong model performance. Our\nresults show that this approach performs better than traditional DAPT by 3.5 of\nthe average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times\nless computing time, providing a cost-effective solution for industries with\nlimited computational capacity. The findings highlight the broader\napplicability of this framework to other low-resource industries, making\nNLP-based solutions more accessible and feasible in production environments."}
{"id": "2504.19867", "pdf": "https://arxiv.org/pdf/2504.19867", "abs": "https://arxiv.org/abs/2504.19867", "authors": ["Ke Hong", "Lufang Chen", "Zhong Wang", "Xiuhong Li", "Qiuli Mao", "Jianping Ma", "Chao Xiong", "Guanyu Wu", "Buhe Han", "Guohao Dai", "Yun Liang", "Yu Wang"], "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage", "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "18 pages, 16 figures", "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."}
{"id": "2504.19898", "pdf": "https://arxiv.org/pdf/2504.19898", "abs": "https://arxiv.org/abs/2504.19898", "authors": ["Mingqian He", "Fei Zhao", "Chonggang Lu", "Ziyan Liu", "Yue Wang", "Haofu Qian"], "title": "GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets", "categories": ["cs.CL"], "comment": null, "summary": "As a fundamental task in machine learning, text classification plays a\ncrucial role in many areas. With the rapid scaling of Large Language Models\n(LLMs), particularly through reinforcement learning (RL), there is a growing\nneed for more capable discriminators. Consequently, advances in classification\nare becoming increasingly vital for enhancing the overall capabilities of LLMs.\nTraditional discriminative methods map text to labels but overlook LLMs'\nintrinsic generative strengths. Generative classification addresses this by\nprompting the model to directly output labels. However, existing studies still\nrely on simple SFT alone, seldom probing the interplay between training and\ninference prompts, and no work has systematically leveraged RL for generative\ntext classifiers and unified SFT, RL, and inference-time prompting in one\nframework. We bridge this gap with GenCLS++, a framework that jointly optimizes\nSFT and RL while systematically exploring five high-level strategy\ndimensions-in-context learning variants, category definitions, explicit\nuncertainty labels, semantically irrelevant numeric labels, and\nperplexity-based decoding-during both training and inference. After an SFT\n\"policy warm-up,\" we apply RL with a simple rule-based reward, yielding sizable\nextra gains. Across seven datasets, GenCLS++ achieves an average accuracy\nimprovement of 3.46% relative to the naive SFT baseline; on public datasets,\nthis improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that\nbenefit from explicit thinking processes, we find that classification tasks\nperform better without such reasoning steps. These insights into the role of\nexplicit reasoning provide valuable guidance for future LLM applications."}
{"id": "2504.19940", "pdf": "https://arxiv.org/pdf/2504.19940", "abs": "https://arxiv.org/abs/2504.19940", "authors": ["Luigia Costabile", "Gian Marco Orlando", "Valerio La Gatta", "Vincenzo Moscato"], "title": "Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "The growing spread of online misinformation has created an urgent need for\nscalable, reliable fact-checking solutions. Crowdsourced fact-checking - where\nnon-experts evaluate claim veracity - offers a cost-effective alternative to\nexpert verification, despite concerns about variability in quality and bias.\nEncouraged by promising results in certain contexts, major platforms such as X\n(formerly Twitter), Facebook, and Instagram have begun shifting from\ncentralized moderation to decentralized, crowd-based approaches.\n  In parallel, advances in Large Language Models (LLMs) have shown strong\nperformance across core fact-checking tasks, including claim detection and\nevidence evaluation. However, their potential role in crowdsourced workflows\nremains unexplored. This paper investigates whether LLM-powered generative\nagents - autonomous entities that emulate human behavior and decision-making -\ncan meaningfully contribute to fact-checking tasks traditionally reserved for\nhuman crowds. Using the protocol of La Barbera et al. (2024), we simulate\ncrowds of generative agents with diverse demographic and ideological profiles.\nAgents retrieve evidence, assess claims along multiple quality dimensions, and\nissue final veracity judgments.\n  Our results show that agent crowds outperform human crowds in truthfulness\nclassification, exhibit higher internal consistency, and show reduced\nsusceptibility to social and cognitive biases. Compared to humans, agents rely\nmore systematically on informative criteria such as Accuracy, Precision, and\nInformativeness, suggesting a more structured decision-making process. Overall,\nour findings highlight the potential of generative agents as scalable,\nconsistent, and less biased contributors to crowd-based fact-checking systems."}
{"id": "2504.19982", "pdf": "https://arxiv.org/pdf/2504.19982", "abs": "https://arxiv.org/abs/2504.19982", "authors": ["Emre Can Acikgoz", "Carl Guo", "Suvodip Dey", "Akul Datta", "Takyoung Kim", "Gokhan Tur", "Dilek Hakkani-Tür"], "title": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research."}
{"id": "2504.20000", "pdf": "https://arxiv.org/pdf/2504.20000", "abs": "https://arxiv.org/abs/2504.20000", "authors": ["Rishika Sen", "Sujoy Roychowdhury", "Sumit Soman", "H. G. Ranjani", "Srikhetra Mohanty"], "title": "Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "comment": "10 pages, 4 figures, 3 tables", "summary": "Knowledge Distillation (KD) is one of the approaches to reduce the size of\nLarge Language Models (LLMs). A LLM with smaller number of model parameters\n(student) is trained to mimic the performance of a LLM of a larger size\n(teacher model) on a specific task. For domain-specific tasks, it is not clear\nif teacher or student model, or both, must be considered for domain adaptation.\nIn this work, we study this problem from perspective of telecom domain\nQuestion-Answering (QA) task. We systematically experiment with Supervised\nFine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to\nKD. We design experiments to study the impact of vocabulary (same and\ndifferent) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the\ndistilled model. Multi-faceted evaluation of the distillation using 14\ndifferent metrics (N-gram, embedding and LLM-based metrics) is considered.\nExperimental results show that SFT of teacher improves performance of distilled\nmodel when both models have same vocabulary, irrespective of algorithm and\nmetrics. Overall, SFT of both teacher and student results in better performance\nacross all metrics, although the statistical significance of the same depends\non the vocabulary of the teacher models."}
{"id": "2504.20013", "pdf": "https://arxiv.org/pdf/2504.20013", "abs": "https://arxiv.org/abs/2504.20013", "authors": ["Beizhe Hu", "Qiang Sheng", "Juan Cao", "Yang Li", "Danding Wang"], "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation", "categories": ["cs.CL", "cs.CY", "cs.IR"], "comment": "ACM SIGIR 2025 Full Paper", "summary": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems."}
{"id": "2504.20022", "pdf": "https://arxiv.org/pdf/2504.20022", "abs": "https://arxiv.org/abs/2504.20022", "authors": ["Pritika Rohera", "Chaitrali Ginimav", "Gayatri Sawant", "Raviraj Joshi"], "title": "Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual Large Language Models (LLMs) have demonstrated significant\neffectiveness across various languages, particularly in high-resource languages\nsuch as English. However, their performance in terms of factual accuracy across\nother low-resource languages, especially Indic languages, remains an area of\ninvestigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\nGemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\nEnglish and Indic languages using the IndicQuest dataset, which contains\nquestion-answer pairs in English and 19 Indic languages. By asking the same\nquestions in English and their respective Indic translations, we analyze\nwhether the models are more reliable for regional context questions in Indic\nlanguages or when operating in English. Our findings reveal that LLMs often\nperform better in English, even for questions rooted in Indic contexts.\nNotably, we observe a higher tendency for hallucination in responses generated\nin low-resource Indic languages, highlighting challenges in the multilingual\nunderstanding capabilities of current LLMs."}
{"id": "2504.20039", "pdf": "https://arxiv.org/pdf/2504.20039", "abs": "https://arxiv.org/abs/2504.20039", "authors": ["Roman Garipov", "Fedor Velikonivtsev", "Ruslan Svirschevski", "Vage Egiazarian", "Max Ryabinin"], "title": "AutoJudge: Judge Decoding Without Manual Annotation", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint, Work in progress", "summary": "We introduce AutoJudge, a framework that accelerates large language model\n(LLM) inference with task-specific lossy speculative decoding. Instead of\nmatching the original model output distribution token-by-token, we identify\nwhich of the generated tokens affect the downstream quality of the generated\nresponse, relaxing the guarantee so that the \"unimportant\" tokens can be\ngenerated faster. Our approach relies on a semi-greedy search algorithm to test\nwhich of the mismatches between target and draft model should be corrected to\npreserve quality, and which ones may be skipped. We then train a lightweight\nclassifier based on existing LLM embeddings to predict, at inference time,\nwhich mismatching tokens can be safely accepted without compromising the final\nanswer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\naccepted tokens per verification cycle with under 1% degradation in answer\naccuracy compared to standard speculative decoding and over 2x with small loss\nin accuracy. When applied to the LiveCodeBench benchmark, our approach\nautomatically detects other, programming-specific important tokens and shows\nsimilar speedups, demonstrating its ability to generalize across tasks."}
{"id": "2504.18596", "pdf": "https://arxiv.org/pdf/2504.18596", "abs": "https://arxiv.org/abs/2504.18596", "authors": ["Anantha Sharma", "Swetha Devabhaktuni", "Eklove Mohan"], "title": "Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "math.PR"], "comment": "18 pages, 8 figures, 5 tables", "summary": "This paper explores the strategic use of modern synthetic data generation and\nadvanced data perturbation techniques to enhance security, maintain analytical\nutility, and improve operational efficiency when managing large datasets, with\na particular focus on the Banking, Financial Services, and Insurance (BFSI)\nsector. We contrast these advanced methods encompassing generative models like\nGANs, sophisticated context-aware PII transformation, configurable statistical\nperturbation, and differential privacy with traditional anonymization\napproaches.\n  The goal is to create realistic, privacy-preserving datasets that retain high\nutility for complex machine learning tasks and analytics, a critical need in\nthe data-sensitive industries like BFSI, Healthcare, Retail, and\nTelecommunications. We discuss how these modern techniques potentially offer\nsignificant improvements in balancing privacy preservation while maintaining\ndata utility compared to older methods. Furthermore, we examine the potential\nfor operational gains, such as reduced overhead and accelerated analytics, by\nusing these privacy-enhanced datasets. We also explore key use cases where\nthese methods can mitigate regulatory risks and enable scalable, data-driven\ninnovation without compromising sensitive customer information."}
{"id": "2504.18748", "pdf": "https://arxiv.org/pdf/2504.18748", "abs": "https://arxiv.org/abs/2504.18748", "authors": ["Kaustubh D. Dhole", "Nikhita Vedula", "Saar Kuzi", "Giuseppe Castellucci", "Eugene Agichtein", "Shervin Malmasi"], "title": "Generative Product Recommendations for Implicit Superlative Queries", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "In Recommender Systems, users often seek the best products through indirect,\nvague, or under-specified queries, such as \"best shoes for trail running\". Such\nqueries, also referred to as implicit superlative queries, pose a significant\nchallenge for standard retrieval and ranking systems as they lack an explicit\nmention of attributes and require identifying and reasoning over complex\nfactors. We investigate how Large Language Models (LLMs) can generate implicit\nattributes for ranking as well as reason over them to improve product\nrecommendations for such queries. As a first step, we propose a novel\nfour-point schema for annotating the best product candidates for superlative\nqueries called SUPERB, paired with LLM-based product annotations. We then\nempirically evaluate several existing retrieval and ranking approaches on our\nnew dataset, providing insights and discussing their integration into\nreal-world e-commerce production systems."}
{"id": "2504.18919", "pdf": "https://arxiv.org/pdf/2504.18919", "abs": "https://arxiv.org/abs/2504.18919", "authors": ["Andrew M. Bean", "Rebecca Payne", "Guy Parsons", "Hannah Rose Kirk", "Juan Ciro", "Rafael Mosquera", "Sara Hincapié Monsalve", "Aruna S. Ekanayaka", "Lionel Tarassenko", "Luc Rocher", "Adam Mahdi"], "title": "Clinical knowledge in LLMs does not translate to human interactions", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "52 pages, 4 figures", "summary": "Global healthcare providers are exploring use of large language models (LLMs)\nto provide medical advice to the public. LLMs now achieve nearly perfect scores\non medical licensing exams, but this does not necessarily translate to accurate\nperformance in real-world settings. We tested if LLMs can assist members of the\npublic in identifying underlying conditions and choosing a course of action\n(disposition) in ten medical scenarios in a controlled study with 1,298\nparticipants. Participants were randomly assigned to receive assistance from an\nLLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested\nalone, LLMs complete the scenarios accurately, correctly identifying conditions\nin 94.9% of cases and disposition in 56.3% on average. However, participants\nusing the same LLMs identified relevant conditions in less than 34.5% of cases\nand disposition in less than 44.2%, both no better than the control group. We\nidentify user interactions as a challenge to the deployment of LLMs for medical\nadvice. Standard benchmarks for medical knowledge and simulated patient\ninteractions do not predict the failures we find with human participants.\nMoving forward, we recommend systematic human user testing to evaluate\ninteractive capabilities prior to public deployments in healthcare."}
{"id": "2504.18988", "pdf": "https://arxiv.org/pdf/2504.18988", "abs": "https://arxiv.org/abs/2504.18988", "authors": ["Saramsh Gautam", "Mahmood Jasim"], "title": "LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings", "categories": ["cs.HC", "cs.CL", "H.5.3"], "comment": "19 pages, 4 figures. Multimodal system design and evaluation study", "summary": "Collaborative research often includes contributors with varied perspectives\nfrom diverse linguistic backgrounds. However, English as a Second Language\n(ESL) researchers often struggle to communicate during meetings in English and\ncomprehend discussions, leading to limited contribution. To investigate these\nchallenges, we surveyed 64 ESL researchers who frequently collaborate in\nmultilingual teams and identified four key design goals around participation,\ncomprehension, documentation, and feedback. Guided by these design goals, we\ndeveloped LINC, a multimodal Language INdependent Collaboration system with two\ncomponents: a real-time module for multilingual communication during meetings\nand a post-meeting dashboard for discussion analysis. We evaluated the system\nthrough a two-phased study with six triads of multilingual teams. We found that\nusing LINC, participants benefited from communicating in their preferred\nlanguage, recalled and reviewed actionable insights, and prepared for upcoming\nmeetings effectively. We discuss external factors that impact multilingual\nmeeting participation beyond language preferences and the implications of\nmultimodal systems in facilitating meetings in hybrid multilingual\ncollaborative settings beyond research."}
{"id": "2504.19056", "pdf": "https://arxiv.org/pdf/2504.19056", "abs": "https://arxiv.org/abs/2504.19056", "authors": ["Mohammad Mahdi Abootorabi", "Omid Ghahroodi", "Pardis Sadat Zahraei", "Hossein Behzadasl", "Alireza Mirrokni", "Mobina Salimipanah", "Arash Rasouli", "Bahar Behzadipour", "Sara Azarnoush", "Benyamin Maleki", "Erfan Sadraiye", "Kiarash Kiani Feriz", "Mahdi Teymouri Nahad", "Ali Moghadasi", "Abolfazl Eshagh Abianeh", "Nizi Nazar", "Hamid R. Rabiee", "Mahdieh Soleymani Baghshah", "Meisam Ahmadi", "Ehsaneddin Asgari"], "title": "Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "50 main pages, 30 pages appendix, 21 figures, 8 tables, GitHub\n  Repository:\n  https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey", "summary": "Generative AI is reshaping art, gaming, and most notably animation. Recent\nbreakthroughs in foundation and diffusion models have reduced the time and cost\nof producing animated content. Characters are central animation components,\ninvolving motion, emotions, gestures, and facial expressions. The pace and\nbreadth of advances in recent months make it difficult to maintain a coherent\nview of the field, motivating the need for an integrative review. Unlike\nearlier overviews that treat avatars, gestures, or facial animation in\nisolation, this survey offers a single, comprehensive perspective on all the\nmain generative AI applications for character animation. We begin by examining\nthe state-of-the-art in facial animation, expression rendering, image\nsynthesis, avatar creation, gesture modeling, motion synthesis, object\ngeneration, and texture synthesis. We highlight leading research, practical\ndeployments, commonly used datasets, and emerging trends for each area. To\nsupport newcomers, we also provide a comprehensive background section that\nintroduces foundational models and evaluation metrics, equipping readers with\nthe knowledge needed to enter the field. We discuss open challenges and map\nfuture research directions, providing a roadmap to advance AI-driven\ncharacter-animation technologies. This survey is intended as a resource for\nresearchers and developers entering the field of generative AI animation or\nadjacent fields. Resources are available at:\nhttps://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey."}
{"id": "2504.19062", "pdf": "https://arxiv.org/pdf/2504.19062", "abs": "https://arxiv.org/abs/2504.19062", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Ruiqi Li", "Jingyu Lu", "Rongjie Huang", "Ruiyuan Zhang", "Zhiqing Hong", "Ziyue Jiang", "Zhou Zhao"], "title": "Versatile Framework for Song Generation with Prompt-based Control", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Song generation focuses on producing controllable high-quality songs based on\nvarious prompts. However, existing methods struggle to generate vocals and\naccompaniments with prompt-based control and proper alignment. Additionally,\nthey fall short in supporting various tasks. To address these challenges, we\nintroduce VersBand, a multi-task song generation framework for synthesizing\nhigh-quality, aligned songs with prompt-based control. VersBand comprises these\nprimary models: 1) VocalBand, a decoupled model, leverages the flow-matching\nmethod for generating singing styles, pitches, and mel-spectrograms, allowing\nfast, high-quality vocal generation with style control. 2) AccompBand, a\nflow-based transformer model, incorporates the Band-MOE, selecting suitable\nexperts for enhanced quality, alignment, and control. This model allows for\ngenerating controllable, high-quality accompaniments aligned with vocals. 3)\nTwo generation models, LyricBand for lyrics and MelodyBand for melodies,\ncontribute to the comprehensive multi-task song generation system, allowing for\nextensive control based on multiple prompts. Experimental results demonstrate\nthat VersBand performs better over baseline models across multiple song\ngeneration tasks using objective and subjective metrics. Audio samples are\navailable at https://VersBand.github.io."}
{"id": "2504.19188", "pdf": "https://arxiv.org/pdf/2504.19188", "abs": "https://arxiv.org/abs/2504.19188", "authors": ["Jianlong Chen", "Chao Li", "Yang Yuan", "Andrew C Yao"], "title": "Hierarchical Attention Generates Better Proofs", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.LO"], "comment": "15 pages with 3 figures", "summary": "Large language models (LLMs) have shown promise in formal theorem proving,\nbut their token-level processing often fails to capture the inherent\nhierarchical nature of mathematical proofs. We introduce \\textbf{Hierarchical\nAttention}, a regularization method that aligns LLMs' attention mechanisms with\nmathematical reasoning structures. Our approach establishes a five-level\nhierarchy from foundational elements to high-level concepts, ensuring\nstructured information flow in proof generation. Experiments demonstrate that\nour method improves proof success rates by 2.05\\% on miniF2F and 1.69\\% on\nProofNet while reducing proof complexity by 23.81\\% and 16.50\\% respectively.\nThe code is available at https://github.com/Car-pe/HAGBP."}
{"id": "2504.19276", "pdf": "https://arxiv.org/pdf/2504.19276", "abs": "https://arxiv.org/abs/2504.19276", "authors": ["Yiyang Zhou", "Zhaoyang Wang", "Tianle Wang", "Shangyu Xing", "Peng Xia", "Bo Li", "Kaiyuan Zheng", "Zijian Zhang", "Zhaorun Chen", "Wenhao Zheng", "Xuchao Zhang", "Chetan Bansal", "Weitong Zhang", "Ying Wei", "Mohit Bansal", "Huaxiu Yao"], "title": "Anyprefer: An Agentic Framework for Preference Data Synthesis", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "High-quality preference data is essential for aligning foundation models with\nhuman values through preference learning. However, manual annotation of such\ndata is often time-consuming and costly. Recent methods often adopt a\nself-rewarding approach, where the target model generates and annotates its own\npreference data, but this can lead to inaccuracies since the reward model\nshares weights with the target model, thereby amplifying inherent biases. To\naddress these issues, we propose Anyprefer, a framework designed to synthesize\nhigh-quality preference data for aligning the target model. Anyprefer frames\nthe data synthesis process as a cooperative two-player Markov Game, where the\ntarget model and the judge model collaborate together. Here, a series of\nexternal tools are introduced to assist the judge model in accurately rewarding\nthe target model's responses, mitigating biases in the rewarding process. In\naddition, a feedback mechanism is introduced to optimize prompts for both\nmodels, enhancing collaboration and improving data quality. The synthesized\ndata is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K\nhigh-quality preference pairs. Extensive experiments show that Anyprefer\nsignificantly improves model alignment performance across four main\napplications, covering 21 datasets, achieving average improvements of 18.55% in\nfive natural language generation datasets, 3.66% in nine vision-language\nunderstanding datasets, 30.05% in three medical image analysis datasets, and\n16.00% in four visuo-motor control tasks."}
{"id": "2504.19444", "pdf": "https://arxiv.org/pdf/2504.19444", "abs": "https://arxiv.org/abs/2504.19444", "authors": ["Kang Yang", "Xinjun Mao", "Shangwen Wang", "Yanlin Wang", "Tanghaoran Zhang", "Bo Lin", "Yihao Qin", "Zhang Zhang", "Yao Lu", "Kamal Al-Sabahi"], "title": "Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks", "categories": ["cs.SE", "cs.CL"], "comment": "Awarded the ACM SIGSOFT Distinguished Paper Award in ICPC 2025", "summary": "Pre-trained code models rely heavily on high-quality pre-training data,\nparticularly human-written reference comments that bridge code and natural\nlanguage. However, these comments often become outdated as software evolves,\ndegrading model performance. Large language models (LLMs) excel at generating\nhigh-quality code comments. We investigate whether replacing human-written\ncomments with LLM-generated ones improves pre-training datasets. Since standard\nmetrics cannot assess reference comment quality, we propose two novel\nreference-free evaluation tasks: code-comment inconsistency detection and\nsemantic code search. Results show that LLM-generated comments are more\nsemantically consistent with code than human-written ones, as confirmed by\nmanual evaluation. Leveraging this finding, we rebuild the CodeSearchNet\ndataset with LLM-generated comments and re-pre-train CodeT5. Evaluations\ndemonstrate that models trained on LLM-enhanced data outperform those using\noriginal human comments in code summarization, generation, and translation\ntasks. This work validates rebuilding pre-training datasets with LLMs to\nadvance code intelligence, challenging the traditional reliance on human\nreference comments."}
{"id": "2504.19458", "pdf": "https://arxiv.org/pdf/2504.19458", "abs": "https://arxiv.org/abs/2504.19458", "authors": ["Taoyu Su", "Jiawei Sheng", "Duohe Ma", "Xiaodong Li", "Juwei Yue", "Mengxiao Song", "Yingkai Tang", "Tingwen Liu"], "title": "Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective", "categories": ["cs.MM", "cs.CL", "cs.IR"], "comment": "Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,", "summary": "Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from\ndifferent Multi-Modal Knowledge Graphs (MMKGs), a critical information\nretrieval task. Existing studies have explored various fusion paradigms and\nconsistency constraints to improve the alignment of equivalent entities, while\noverlooking that the visual modality may not always contribute positively.\nEmpirically, entities with low-similarity images usually generate\nunsatisfactory performance, highlighting the limitation of overly relying on\nvisual features. We believe the model can be biased toward the visual modality,\nleading to a shortcut image-matching task. To address this, we propose a\ncounterfactual debiasing framework for MMEA, termed CDMEA, which investigates\nvisual modality bias from a causal perspective. Our approach aims to leverage\nboth visual and graph modalities to enhance MMEA while suppressing the direct\ncausal effect of the visual modality on model predictions. By estimating the\nTotal Effect (TE) of both modalities and excluding the Natural Direct Effect\n(NDE) of the visual modality, we ensure that the model predicts based on the\nTotal Indirect Effect (TIE), effectively utilizing both modalities and reducing\nvisual modality bias. Extensive experiments on 9 benchmark datasets show that\nCDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,\nhigh-noise, and low-resource data scenarios."}
{"id": "2504.19483", "pdf": "https://arxiv.org/pdf/2504.19483", "abs": "https://arxiv.org/abs/2504.19483", "authors": ["Bertram Højer", "Oliver Jarvis", "Stefan Heinrich"], "title": "Improving Reasoning Performance in Large Language Models via Representation Engineering", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Has been accepted at \"The Thirteenth International Conference on\n  Learning Representations (ICLR 2025)\" Link to publication:\n  https://openreview.net/forum?id=IssPhpUsKt", "summary": "Recent advancements in large language models (LLMs) have resulted in\nincreasingly anthropomorphic language concerning the ability of LLMs to reason.\nWhether reasoning in LLMs should be understood to be inherently different is,\nhowever, widely debated. We propose utilizing a representation engineering\napproach wherein model activations are read from the residual stream of an LLM\nwhen processing a reasoning task. The activations are used to derive a control\nvector that is applied to the model as an inference-time intervention,\nmodulating the representational space of the model, to improve performance on\nthe specified task. We publish the code for deriving control vectors and\nanalyzing model representations. The method allows us to improve performance on\nreasoning benchmarks and assess how control vectors influence the final logit\ndistribution of a model via metrics such as KL divergence and entropy. We apply\ncontrol vectors to Mistral-7B-Instruct and a range of Pythia models on an\ninductive, a deductive and mathematical reasoning task. We show that an LLM\ncan, to a certain degree, be controlled to improve its perceived reasoning\nability by modulating activations. The intervention is dependent upon the\nability to reliably extract the model's typical state when correctly solving a\ntask. Our results suggest that reasoning performance can be modulated in the\nsame manner as other information-processing tasks performed by LLMs and\ndemonstrate that we are capable of improving performance on specific tasks via\na simple intervention on the residual stream with no additional training."}
{"id": "2504.19500", "pdf": "https://arxiv.org/pdf/2504.19500", "abs": "https://arxiv.org/abs/2504.19500", "authors": ["Yan Wang", "Baoxiong Jia", "Ziyu Zhu", "Siyuan Huang"], "title": "Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025", "summary": "Open-vocabulary 3D scene understanding is pivotal for enhancing physical\nintelligence, as it enables embodied agents to interpret and interact\ndynamically within real-world environments. This paper introduces MPEC, a novel\nMasked Point-Entity Contrastive learning method for open-vocabulary 3D semantic\nsegmentation that leverages both 3D entity-language alignment and point-entity\nconsistency across different point cloud views to foster entity-specific\nfeature representations. Our method improves semantic discrimination and\nenhances the differentiation of unique instances, achieving state-of-the-art\nresults on ScanNet for open-vocabulary 3D semantic segmentation and\ndemonstrating superior zero-shot scene understanding capabilities. Extensive\nfine-tuning experiments on 8 datasets, spanning from low-level perception to\nhigh-level reasoning tasks, showcase the potential of learned 3D features,\ndriving consistent performance gains across varied 3D scene understanding\ntasks. Project website: https://mpec-3d.github.io/"}
{"id": "2504.19519", "pdf": "https://arxiv.org/pdf/2504.19519", "abs": "https://arxiv.org/abs/2504.19519", "authors": ["Ke Hong", "Xiuhong Li", "Minxu Liu", "Qiuli Mao", "Tianqi Wu", "Zixiao Huang", "Lufang Chen", "Zhong Wang", "Yichong Zhang", "Zhenhua Zhu", "Guohao Dai", "Yu Wang"], "title": "FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation", "categories": ["cs.DC", "cs.CL", "cs.LG"], "comment": "17 pages, 11 figures, 4 tables", "summary": "Generative models have achieved remarkable success across various\napplications, driving the demand for multi-GPU computing. Inter-GPU\ncommunication becomes a bottleneck in multi-GPU computing systems, particularly\non consumer-grade GPUs. By exploiting concurrent hardware execution,\noverlapping computation and communication latency is an effective technique for\nmitigating the communication overhead. We identify that an efficient and\nadaptable overlapping design should satisfy (1) tile-wise overlapping to\nmaximize the overlapping opportunity, (2) interference-free computation to\nmaintain the original computational performance, and (3) communication\nagnosticism to reduce the development burden against varying communication\nprimitives. Nevertheless, current designs fail to simultaneously optimize for\nall of those features.\n  To address the issue, we propose FlashOverlap, a lightweight design\ncharacterized by tile-wise overlapping, interference-free computation, and\ncommunication agnosticism. FlashOverlap utilizes a novel signaling mechanism to\nidentify tile-wise data dependency without interrupting the computation\nprocess, and reorders data to contiguous addresses, enabling communication by\nsimply calling NCCL APIs. Experiments show that such a lightweight design\nachieves up to 1.65x speedup, outperforming existing works in most cases."}
{"id": "2504.19583", "pdf": "https://arxiv.org/pdf/2504.19583", "abs": "https://arxiv.org/abs/2504.19583", "authors": ["Hanlu Zhang", "Yumeng Ma", "Shuo Wang", "Guiran Liu", "Binrong Zhu"], "title": "Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "This paper proposes a parameter collaborative optimization algorithm for\nlarge language models, enhanced with graph spectral analysis. The goal is to\nimprove both fine-tuning efficiency and structural awareness during training.\nIn the proposed method, the parameters of a pre-trained language model are\ntreated as nodes in a graph. A weighted graph is constructed, and Laplacian\nspectral decomposition is applied to enable frequency-domain modeling and\nstructural representation of the parameter space. Based on this structure, a\njoint loss function is designed. It combines the task loss with a spectral\nregularization term to facilitate collaborative updates among parameters. In\naddition, a spectral filtering mechanism is introduced during the optimization\nphase. This mechanism adjusts gradients in a structure-aware manner, enhancing\nthe model's training stability and convergence behavior. The method is\nevaluated on multiple tasks, including traditional fine-tuning comparisons,\nfew-shot generalization tests, and convergence speed analysis. In all settings,\nthe proposed approach demonstrates superior performance. The experimental\nresults confirm that the spectral collaborative optimization framework\neffectively reduces parameter perturbations and improves fine-tuning quality\nwhile preserving overall model performance. This work contributes significantly\nto the field of artificial intelligence by advancing parameter-efficient\ntraining methodologies for large-scale models, reinforcing the importance of\nstructural signal processing in deep learning optimization, and offering a\nrobust, generalizable framework for enhancing language model adaptability and\nperformance."}
{"id": "2504.19730", "pdf": "https://arxiv.org/pdf/2504.19730", "abs": "https://arxiv.org/abs/2504.19730", "authors": ["Wenhan Mu", "Ling Xu", "Shuren Pei", "Le Mi", "Huichi Zhou"], "title": "Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge", "categories": ["cs.SE", "cs.CL"], "comment": "25 pages, 6 figures", "summary": "The widespread adoption of code language models in software engineering tasks\nhas exposed vulnerabilities to adversarial attacks, especially the identifier\nsubstitution attacks. Although existing identifier substitution attackers\ndemonstrate high success rates, they often produce adversarial examples with\nunnatural code patterns. In this paper, we systematically assess the quality of\nadversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80%\nof adversarial examples generated by state-of-the-art identifier substitution\nattackers (e.g., ALERT) are actually detectable. Based on this insight, we\npropose EP-Shield, a unified framework for evaluating and purifying identifier\nsubstitution attacks via naturalness-aware reasoning. Specifically, we first\nevaluate the naturalness of code and identify the perturbed adversarial code,\nthen purify it so that the victim model can restore correct prediction.\nExtensive experiments demonstrate the superiority of EP-Shield over adversarial\nfine-tuning (up to 83.36% improvement) and its lightweight design 7B\nparameters) with GPT-4-level performance."}
{"id": "2504.19754", "pdf": "https://arxiv.org/pdf/2504.19754", "abs": "https://arxiv.org/abs/2504.19754", "authors": ["Carlo Merola", "Jaspinder Singh"], "title": "Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "13 pages, 2 figures, Second Workshop on Knowledge-Enhanced\n  Information Retrieval, ECIR 2025", "summary": "Retrieval-augmented generation (RAG) has become a transformative approach for\nenhancing large language models (LLMs) by grounding their outputs in external\nknowledge sources. Yet, a critical question persists: how can vast volumes of\nexternal knowledge be managed effectively within the input constraints of LLMs?\nTraditional methods address this by chunking external documents into smaller,\nfixed-size segments. While this approach alleviates input limitations, it often\nfragments context, resulting in incomplete retrieval and diminished coherence\nin generation. To overcome these shortcomings, two advanced techniques, late\nchunking and contextual retrieval, have been introduced, both aiming to\npreserve global context. Despite their potential, their comparative strengths\nand limitations remain unclear. This study presents a rigorous analysis of late\nchunking and contextual retrieval, evaluating their effectiveness and\nefficiency in optimizing RAG systems. Our results indicate that contextual\nretrieval preserves semantic coherence more effectively but requires greater\ncomputational resources. In contrast, late chunking offers higher efficiency\nbut tends to sacrifice relevance and completeness."}
