{"id": "2504.20049", "pdf": "https://arxiv.org/pdf/2504.20049", "abs": "https://arxiv.org/abs/2504.20049", "authors": ["Marina Mayor-Rocher", "Cristina Pozo", "Nina Melero", "Gonzalo Martínez", "María Grandury", "Pedro Reviriego"], "title": "It's the same but not the same: Do LLMs distinguish Spanish varieties?", "categories": ["cs.CL"], "comment": "in Spanish language", "summary": "In recent years, large language models (LLMs) have demonstrated a high\ncapacity for understanding and generating text in Spanish. However, with five\nhundred million native speakers, Spanish is not a homogeneous language but\nrather one rich in diatopic variations spanning both sides of the Atlantic. For\nthis reason, in this study, we evaluate the ability of nine language models to\nidentify and distinguish the morphosyntactic and lexical peculiarities of seven\nvarieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean,\nPeninsular, Mexican and Central American and Rioplatense) through a\nmultiple-choice test. The results indicate that the Peninsular Spanish variety\nis the best identified by all models and that, among them, GPT-4o is the only\nmodel capable of recognizing the variability of the Spanish language.\n  --\n  En los \\'ultimos a\\~nos, los grandes modelos de lenguaje (LLMs, por sus\nsiglas en ingl\\'es) han demostrado una alta capacidad para comprender y generar\ntexto en espa\\~nol. Sin embargo, con quinientos millones de hablantes nativos,\nla espa\\~nola no es una lengua homog\\'enea, sino rica en variedades\ndiat\\'opicas que se extienden a ambos lados del Atl\\'antico. Por todo ello,\nevaluamos en este trabajo la capacidad de nueve modelos de lenguaje de\nidentificar y discernir las peculiaridades morfosint\\'acticas y l\\'exicas de\nsiete variedades de espa\\~nol (andino, antillano, caribe\\~no continental,\nchileno, espa\\~nol peninsular, mexicano y centroamericano y rioplatense)\nmediante un test de respuesta m\\'ultiple. Los resultados obtenidos indican que\nla variedad de espa\\~nol peninsular es la mejor identificada por todos los\nmodelos y que, de entre todos, GPT-4o es el \\'unico modelo capaz de identificar\nla variabilidad de la lengua espa\\~nola."}
{"id": "2504.20051", "pdf": "https://arxiv.org/pdf/2504.20051", "abs": "https://arxiv.org/abs/2504.20051", "authors": ["Frances Laureano De Leon", "Harish Tayyar Madabushi", "Mark G. Lee"], "title": "Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts", "categories": ["cs.CL"], "comment": null, "summary": "Multiword expressions, characterised by non-compositional meanings and\nsyntactic irregularities, are an example of nuanced language. These expressions\ncan be used literally or idiomatically, leading to significant changes in\nmeaning. While large language models have demonstrated strong performance\nacross many tasks, their ability to handle such linguistic subtleties remains\nuncertain. Therefore, this study evaluates how state-of-the-art language models\nprocess the ambiguity of potentially idiomatic multiword expressions,\nparticularly in contexts that are less frequent, where models are less likely\nto rely on memorisation. By evaluating models across in Portuguese and\nGalician, in addition to English, and using a novel code-switched dataset and a\nnovel task, we find that large language models, despite their strengths,\nstruggle with nuanced language. In particular, we find that the latest models,\nincluding GPT-4, fail to outperform the xlm-roBERTa-base baselines in both\ndetection and semantic tasks, with especially poor performance on the novel\ntasks we introduce, despite its similarity to existing tasks. Overall, our\nresults demonstrate that multiword expressions, especially those which are\nambiguous, continue to be a challenge to models."}
{"id": "2504.20086", "pdf": "https://arxiv.org/pdf/2504.20086", "abs": "https://arxiv.org/abs/2504.20086", "authors": ["Sebastian Gehrmann", "Claire Huang", "Xian Teng", "Sergei Yurovski", "Iyanuoluwa Shode", "Chirag S. Patel", "Arjun Bhorkar", "Naveen Thomas", "John Doucette", "David Rosenberg", "Mark Dredze", "David Rabinowitz"], "title": "Understanding and Mitigating Risks of Generative AI in Financial Services", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Accepted to FAccT 2025", "summary": "To responsibly develop Generative AI (GenAI) products, it is critical to\ndefine the scope of acceptable inputs and outputs. What constitutes a \"safe\"\nresponse is an actively debated question. Academic work puts an outsized focus\non evaluating models by themselves for general purpose aspects such as\ntoxicity, bias, and fairness, especially in conversational applications being\nused by a broad audience. In contrast, less focus is put on considering\nsociotechnical systems in specialized domains. Yet, those specialized systems\ncan be subject to extensive and well-understood legal and regulatory scrutiny.\nThese product-specific considerations need to be set in industry-specific laws,\nregulations, and corporate governance requirements. In this paper, we aim to\nhighlight AI content safety considerations specific to the financial services\ndomain and outline an associated AI content risk taxonomy. We compare this\ntaxonomy to existing work in this space and discuss implications of risk\ncategory violations on various stakeholders. We evaluate how existing\nopen-source technical guardrail solutions cover this taxonomy by assessing them\non data collected via red-teaming activities. Our results demonstrate that\nthese guardrails fail to detect most of the content risks we discuss."}
{"id": "2504.20157", "pdf": "https://arxiv.org/pdf/2504.20157", "abs": "https://arxiv.org/abs/2504.20157", "authors": ["Zae Myung Kim", "Chanwoo Park", "Vipul Raheja", "Dongyeop Kang"], "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models", "categories": ["cs.CL"], "comment": null, "summary": "Reward-based alignment methods for large language models (LLMs) face two key\nlimitations: vulnerability to reward hacking, where models exploit flaws in the\nreward signal; and reliance on brittle, labor-intensive prompt engineering when\nLLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a\nframework that addresses these challenges by integrating a meta-reward model\nthat dynamically refines the reward model's prompt throughout training. In MPO,\nthe meta-reward model monitors the evolving training context and continuously\nadjusts the reward model's prompt to maintain high alignment, providing an\nadaptive reward signal that resists exploitation by the policy. This\nmeta-learning approach promotes a more stable policy optimization, and greatly\nreduces the need for manual reward prompt design. It yields performance on par\nwith or better than models guided by extensively hand-crafted reward prompts.\nFurthermore, we show that MPO maintains its effectiveness across diverse tasks,\nsuch as question answering and mathematical reasoning, without requiring\nspecialized reward designs. Beyond standard RLAIF, MPO's meta-learning\nformulation is readily extensible to higher-level alignment frameworks.\nOverall, this method addresses theoretical and practical challenges in\nreward-based RL alignment for LLMs, paving the way for more robust and\nadaptable alignment strategies. The code and models will be publicly shared."}
{"id": "2504.20168", "pdf": "https://arxiv.org/pdf/2504.20168", "abs": "https://arxiv.org/abs/2504.20168", "authors": ["Nishant Subramani", "Jason Eisner", "Justin Svegliato", "Benjamin Van Durme", "Yu Su", "Sam Thomson"], "title": "MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at NAACL 2025. Code:\n  https://github.com/microsoft/mice_for_cats", "summary": "Tool-using agents that act in the world need to be both useful and safe.\nWell-calibrated model confidences can be used to weigh the risk versus reward\nof potential actions, but prior work shows that many models are poorly\ncalibrated. Inspired by interpretability literature exploring the internals of\nmodels, we propose a novel class of model-internal confidence estimators (MICE)\nto better assess confidence when calling tools. MICE first decodes from each\nintermediate layer of the language model using logitLens and then computes\nsimilarity scores between each layer's generation and the final output. These\nfeatures are fed into a learned probabilistic classifier to assess confidence\nin the decoded output. On the simulated trial and error (STE) tool-calling\ndataset using Llama3 models, we find that MICE beats or matches the baselines\non smoothed expected calibration error. Using MICE confidences to determine\nwhether to call a tool significantly improves over strong baselines on a new\nmetric, expected tool-calling utility. Further experiments show that MICE is\nsample-efficient, can generalize zero-shot to unseen APIs, and results in\nhigher tool-calling utility in scenarios with varying risk levels. Our code is\nopen source, available at https://github.com/microsoft/mice_for_cats."}
{"id": "2504.20220", "pdf": "https://arxiv.org/pdf/2504.20220", "abs": "https://arxiv.org/abs/2504.20220", "authors": ["Henning Schäfer", "Cynthia S. Schmidt", "Johannes Wutzkowsky", "Kamil Lorek", "Lea Reinartz", "Johannes Rückert", "Christian Temme", "Britta Böckmann", "Peter A. Horn", "Christoph M. Friedrich"], "title": "A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports", "categories": ["cs.CL", "cs.CV", "68T07", "I.7.5; I.4.7; I.2.7; H.3.3; J.3"], "comment": null, "summary": "Despite the growing adoption of electronic health records, many processes\nstill rely on paper documents, reflecting the heterogeneous real-world\nconditions in which healthcare is delivered. The manual transcription process\nis time-consuming and prone to errors when transferring paper-based data to\ndigital formats. To streamline this workflow, this study presents an\nopen-source pipeline that extracts and categorizes checkbox data from scanned\ndocuments. Demonstrated on transfusion reaction reports, the design supports\nadaptation to other checkbox-rich document types. The proposed method\nintegrates checkbox detection, multilingual optical character recognition (OCR)\nand multilingual vision-language models (VLMs). The pipeline achieves high\nprecision and recall compared against annually compiled gold-standards from\n2017 to 2024. The result is a reduction in administrative workload and accurate\nregulatory reporting. The open-source availability of this pipeline encourages\nself-hosted parsing of checkbox forms."}
{"id": "2504.20251", "pdf": "https://arxiv.org/pdf/2504.20251", "abs": "https://arxiv.org/abs/2504.20251", "authors": ["Aiala Rosá", "Santiago Góngora", "Juan Pablo Filevich", "Ignacio Sastre", "Laura Musto", "Brian Carpenter", "Luis Chiruzzo"], "title": "A Platform for Generating Educational Activities to Teach English as a Second Language", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Unpublished report written in 2023", "summary": "We present a platform for the generation of educational activities oriented\nto teaching English as a foreign language. The different activities -- games\nand language practice exercises -- are strongly based on Natural Language\nProcessing techniques. The platform offers the possibility of playing\nout-of-the-box games, generated from resources created semi-automatically and\nthen manually curated. It can also generate games or exercises of greater\ncomplexity from texts entered by teachers, providing a stage of review and\nedition of the generated content before use. As a way of expanding the variety\nof activities in the platform, we are currently experimenting with image and\ntext generation. In order to integrate them and improve the performance of\nother neural tools already integrated, we are working on migrating the platform\nto a more powerful server. In this paper we describe the development of our\nplatform and its deployment for end users, discussing the challenges faced and\nhow we overcame them, and also detail our future work plans."}
{"id": "2504.20276", "pdf": "https://arxiv.org/pdf/2504.20276", "abs": "https://arxiv.org/abs/2504.20276", "authors": ["Dandan Chen Kaptur", "Yue Huang", "Xuejun Ryan Ji", "Yanhui Guo", "Bradley Kaptur"], "title": "Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi", "categories": ["cs.CL", "stat.AP"], "comment": "13 pages, Paper presented at the National Council on Measurement in\n  Education (NCME) Conference, Denver, Colorado, in April 2025", "summary": "This research delved into GPT-4 and Kimi, two Large Language Models (LLMs),\nfor systematic reviews. We evaluated their performance by comparing\nLLM-generated codes with human-generated codes from a peer-reviewed systematic\nreview on assessment. Our findings suggested that the performance of LLMs\nfluctuates by data volume and question complexity for systematic reviews."}
{"id": "2504.20304", "pdf": "https://arxiv.org/pdf/2504.20304", "abs": "https://arxiv.org/abs/2504.20304", "authors": ["Xiulin Yang", "Zhuoxuan Ju", "Lanni Bu", "Zoey Liu", "Nathan Schneider"], "title": "UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "CHILDES is a widely used resource of transcribed child and child-directed\nspeech. This paper introduces UD-English-CHILDES, the first officially released\nUniversal Dependencies (UD) treebank derived from previously\ndependency-annotated CHILDES data with consistent and unified annotation\nguidelines. Our corpus harmonizes annotations from 11 children and their\ncaregivers, totaling over 48k sentences. We validate existing gold-standard\nannotations under the UD v2 framework and provide an additional 1M\nsilver-standard sentences, offering a consistent resource for computational and\nlinguistic research."}
{"id": "2504.20323", "pdf": "https://arxiv.org/pdf/2504.20323", "abs": "https://arxiv.org/abs/2504.20323", "authors": ["Chao-Lin Liu", "Po-Hsien Wu", "Yi-Ting Yu"], "title": "Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG"], "comment": "16 pages, 9 figures, 2 tables, the Nineteenth International Workshop\n  on Juris-Informatics (JURISIN 2025), associated with the Seventeenth JSAI\n  International Symposium on AI (JSAI-isAI 2025)", "summary": "This report addresses the challenge of limited labeled datasets for\ndeveloping legal recommender systems, particularly in specialized domains like\nlabor disputes. We propose a new approach leveraging the co-citation of legal\narticles within cases to establish similarity and enable algorithmic\nannotation. This method draws a parallel to the concept of case co-citation,\nutilizing cited precedents as indicators of shared legal issues. To evaluate\nthe labeled results, we employ a system that recommends similar cases based on\nplaintiffs' accusations, defendants' rebuttals, and points of disputes. The\nevaluation demonstrates that the recommender, with finetuned text embedding\nmodels and a reasonable BiLSTM module can recommend labor cases whose\nsimilarity was measured by the co-citation of the legal articles. This research\ncontributes to the development of automated annotation techniques for legal\ndocuments, particularly in areas with limited access to comprehensive legal\ndatabases."}
{"id": "2504.20355", "pdf": "https://arxiv.org/pdf/2504.20355", "abs": "https://arxiv.org/abs/2504.20355", "authors": ["Yash Jain", "Vishal Chowdhary"], "title": "Local Prompt Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted as Oral at NAACL 2025 (Main Conference)", "summary": "In recent years, the use of prompts to guide the output of Large Language\nModels have increased dramatically. However, even the best of experts struggle\nto choose the correct words to stitch up a prompt for the desired task. To\nsolve this, LLM driven prompt optimization emerged as an important problem.\nExisting prompt optimization methods optimize a prompt globally, where in all\nthe prompt tokens have to be optimized over a large vocabulary while solving a\ncomplex task. The large optimization space (tokens) leads to insufficient\nguidance for a better prompt. In this work, we introduce Local Prompt\nOptimization (LPO) that integrates with any general automatic prompt\nengineering method. We identify the optimization tokens in a prompt and nudge\nthe LLM to focus only on those tokens in its optimization step. We observe\nremarkable performance improvements on Math Reasoning (GSM8k and MultiArith)\nand BIG-bench Hard benchmarks across various automatic prompt engineering\nmethods. Further, we show that LPO converges to the optimal prompt faster than\nglobal methods."}
{"id": "2504.20356", "pdf": "https://arxiv.org/pdf/2504.20356", "abs": "https://arxiv.org/abs/2504.20356", "authors": ["Maria Khelli", "Samuel Cahyawijaya", "Ayu Purwarianti", "Genta Indra Winata"], "title": "What Causes Knowledge Loss in Multilingual Language Models?", "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual transfer in natural language processing (NLP) models enhances\nmultilingual performance by leveraging shared linguistic knowledge. However,\ntraditional methods that process all data simultaneously often fail to mimic\nreal-world scenarios, leading to challenges like catastrophic forgetting, where\nfine-tuning on new tasks degrades performance on previously learned ones. Our\nstudy explores this issue in multilingual contexts, focusing on linguistic\ndifferences affecting representational learning rather than just model\nparameters. We experiment with 52 languages using LoRA adapters of varying\nranks to evaluate non-shared, partially shared, and fully shared parameters.\nOur aim is to see if parameter sharing through adapters can mitigate forgetting\nwhile preserving prior knowledge. We find that languages using non-Latin\nscripts are more susceptible to catastrophic forgetting, whereas those written\nin Latin script facilitate more effective cross-lingual transfer."}
{"id": "2504.20371", "pdf": "https://arxiv.org/pdf/2504.20371", "abs": "https://arxiv.org/abs/2504.20371", "authors": ["Zhibo Man", "Yuanmeng Chen", "Yujie Zhang", "Yufeng Chen", "Jinan Xu"], "title": "DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation", "categories": ["cs.CL"], "comment": null, "summary": "Currently, Large Language Models (LLMs) have achieved remarkable results in\nmachine translation. However, their performance in multi-domain translation\n(MDT) is less satisfactory; the meanings of words can vary across different\ndomains, highlighting the significant ambiguity inherent in MDT. Therefore,\nevaluating the disambiguation ability of LLMs in MDT remains an open problem.\nTo this end, we present an evaluation and analysis of LLMs on disambiguation in\nmulti-domain translation (DMDTEval), our systematic evaluation framework\nconsisting of three critical aspects: (1) we construct a translation test set\nwith multi-domain ambiguous word annotation, (2) we curate a diverse set of\ndisambiguation prompting templates, and (3) we design precise disambiguation\nmetrics, and study the efficacy of various prompting strategies on multiple\nstate-of-the-art LLMs. Our extensive experiments reveal a number of crucial\nfindings that we believe will pave the way and also facilitate further research\nin the critical area of improving the disambiguation of LLMs."}
{"id": "2504.20444", "pdf": "https://arxiv.org/pdf/2504.20444", "abs": "https://arxiv.org/abs/2504.20444", "authors": ["Mika Hämäläinen"], "title": "On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and\nClaude. We do this by repurposing the famous experiment Asch (1946) conducted\nusing human subjects. The experiment is simple, given two candidates with equal\ndescriptions which one is preferred if one description has positive adjectives\nfirst before negative ones and another description has negative adjectives\nfollowed by positive ones. We test this in two experiments. In one experiment,\nLLMs are given both candidates simultaneously in the same prompt, and in\nanother experiment, LLMs are given both candidates separately. We test all the\nmodels with 200 candidate pairs. We found that, in the first experiment,\nChatGPT preferred the candidate with positive adjectives listed first, while\nGemini preferred both equally often. Claude refused to make a choice. In the\nsecond experiment, ChatGPT and Claude were most likely to rank both candidates\nequally. In the case where they did not give an equal rating, both showed a\nclear preference to a candidate that had negative adjectives listed first.\nGemini was most likely to prefer a candidate with negative adjectives listed\nfirst."}
{"id": "2504.20451", "pdf": "https://arxiv.org/pdf/2504.20451", "abs": "https://arxiv.org/abs/2504.20451", "authors": ["Daniel Lee", "Harsh Sharma", "Jieun Han", "Sunny Jeong", "Alice Oh", "Vered Shwartz"], "title": "Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at SemEval-2025 Workshop (ACL 2025)", "summary": "Translating knowledge-intensive and entity-rich text between English and\nKorean requires transcreation to preserve language-specific and cultural\nnuances beyond literal, phonetic or word-for-word conversion. We evaluate 13\nmodels (LLMs and MT models) using automatic metrics and human assessment by\nbilingual annotators. Our findings show LLMs outperform traditional MT systems\nbut struggle with entity translation requiring cultural adaptation. By\nconstructing an error taxonomy, we identify incorrect responses and entity name\nerrors as key issues, with performance varying by entity type and popularity\nlevel. This work exposes gaps in automatic evaluation metrics and hope to\nenable future work in completing culturally-nuanced machine translation."}
{"id": "2504.20469", "pdf": "https://arxiv.org/pdf/2504.20469", "abs": "https://arxiv.org/abs/2504.20469", "authors": ["Enfa Fane", "Mihai Surdeanu", "Eduardo Blanco", "Steven R. Corman"], "title": "Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models", "categories": ["cs.CL", "cs.CY", "I.2.7"], "comment": "Accepted to The 19th International Workshop on Semantic Evaluation\n  (Semeval 2025)", "summary": "Understanding how news narratives frame entities is crucial for studying\nmedia's impact on societal perceptions of events. In this paper, we evaluate\nthe zero-shot capabilities of large language models (LLMs) in classifying\nframing roles. Through systematic experimentation, we assess the effects of\ninput context, prompting strategies, and task decomposition. Our findings show\nthat a hierarchical approach of first identifying broad roles and then\nfine-grained roles, outperforms single-step classification. We also demonstrate\nthat optimal input contexts and prompts vary across task levels, highlighting\nthe need for subtask-specific strategies. We achieve a Main Role Accuracy of\n89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our\napproach. Our findings emphasize the importance of tailored prompt design and\ninput context optimization for improving LLM performance in entity framing."}
{"id": "2504.20484", "pdf": "https://arxiv.org/pdf/2504.20484", "abs": "https://arxiv.org/abs/2504.20484", "authors": ["Linjuan Wu", "Haoran Wei", "Huan Lin", "Tianhao Li", "Baosong Yang", "Weiming Lu"], "title": "Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training", "categories": ["cs.CL"], "comment": "12 pages, 6 figures, Under Review", "summary": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite English-dominated pre-training, attributed to cross-lingual mechanisms\nduring pre-training. Existing methods for enhancing cross-lingual transfer\nremain constrained by parallel resources, suffering from limited linguistic and\ndomain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT),\na simple and scalable approach that enhances cross-lingual transfer by\nleveraging semantically related bilingual texts via simple next-word\nprediction. We construct CrossIC-PT samples by interleaving semantic-related\nbilingual Wikipedia documents into a single context window. To access window\nsize constraints, we implement a systematic segmentation policy to split long\nbilingual document pairs into chunks while adjusting the sliding window\nmechanism to preserve contextual coherence. We further extend data availability\nthrough a semantic retrieval framework to construct CrossIC-PT samples from\nweb-crawled corpus. Experimental results demonstrate that CrossIC-PT improves\nmultilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and\nQwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%,\n3.99%, and 1.95%, respectively, with additional improvements after data\naugmentation."}
{"id": "2504.20500", "pdf": "https://arxiv.org/pdf/2504.20500", "abs": "https://arxiv.org/abs/2504.20500", "authors": ["Huimin Lu", "Masaru Isonuma", "Junichiro Mori", "Ichiro Sakata"], "title": "UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ICLR 2025 (poster)", "summary": "We present UniDetox, a universally applicable method designed to mitigate\ntoxicity across various large language models (LLMs). Previous detoxification\nmethods are typically model-specific, addressing only individual models or\nmodel families, and require careful hyperparameter tuning due to the trade-off\nbetween detoxification efficacy and language modeling performance. In contrast,\nUniDetox provides a detoxification technique that can be universally applied to\na wide range of LLMs without the need for separate model-specific tuning.\nSpecifically, we propose a novel and efficient dataset distillation technique\nfor detoxification using contrastive decoding. This approach distills\ndetoxifying representations in the form of synthetic text data, enabling\nuniversal detoxification of any LLM through fine-tuning with the distilled\ntext. Our experiments demonstrate that the detoxifying text distilled from\nGPT-2 can effectively detoxify larger models, including OPT, Falcon, and\nLLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter\ntuning for each model, as a single hyperparameter configuration can be\nseamlessly applied across different models. Additionally, analysis of the\ndetoxifying text reveals a reduction in politically biased content, providing\ninsights into the attributes necessary for effective detoxification of LLMs."}
{"id": "2504.20547", "pdf": "https://arxiv.org/pdf/2504.20547", "abs": "https://arxiv.org/abs/2504.20547", "authors": ["Jesus Lovon", "Thouria Ben-Haddi", "Jules Di Scala", "Jose G. Moreno", "Lynda Tamine"], "title": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records", "categories": ["cs.CL"], "comment": null, "summary": "The lack of standardized evaluation benchmarks in the medical domain for text\ninputs can be a barrier to widely adopting and leveraging the potential of\nnatural language models for health-related downstream tasks. This paper\nrevisited an openly available MIMIC-IV benchmark for electronic health records\n(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the\nHugging Face datasets library to allow an easy share and use of this\ncollection. Second, we investigate the application of templates to convert EHR\ntabular data to text. Experiments using fine-tuned and zero-shot LLMs on the\nmortality of patients task show that fine-tuned text-based models are\ncompetitive against robust tabular classifiers. In contrast, zero-shot LLMs\nstruggle to leverage EHR representations. This study underlines the potential\nof text-based approaches in the medical field and highlights areas for further\nimprovement."}
{"id": "2504.20552", "pdf": "https://arxiv.org/pdf/2504.20552", "abs": "https://arxiv.org/abs/2504.20552", "authors": ["Baz Roland", "Kristina Malyseva", "Anna Pappa", "Tristan Cazenave"], "title": "BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters", "categories": ["cs.CL"], "comment": null, "summary": "This project introduces BrAIcht, an AI conversational agent that creates\ndialogues in the distinctive style of the famous German playwright Bertolt\nBrecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7\nbillion parameters and a modified version of the base Llama2 suitable for\nGerman language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of\nother German plays that are stylistically similar to Bertolt Brecht are used to\nform a more di-erse dataset. Due to the limited memory capacity, a\nparameterefficient fine-tuning technique called QLoRA is implemented to train\nthe large language model. The results, based on BLEU score and perplexity, show\nvery promising performance of BrAIcht in generating dialogues in the style of\nBertolt Brecht."}
{"id": "2504.20581", "pdf": "https://arxiv.org/pdf/2504.20581", "abs": "https://arxiv.org/abs/2504.20581", "authors": ["Iwona Christop", "Tomasz Kuczyński", "Marek Kubis"], "title": "ClonEval: An Open Voice Cloning Benchmark", "categories": ["cs.CL"], "comment": null, "summary": "We present a novel benchmark for voice cloning text-to-speech models. The\nbenchmark consists of an evaluation protocol, an open-source library for\nassessing the performance of voice cloning models, and an accompanying\nleaderboard. The paper discusses design considerations and presents a detailed\ndescription of the evaluation procedure. The usage of the software library is\nexplained, along with the organization of results on the leaderboard."}
{"id": "2504.20605", "pdf": "https://arxiv.org/pdf/2504.20605", "abs": "https://arxiv.org/abs/2504.20605", "authors": ["Mihai Nadas", "Laura Diosan", "Andrei Piscoran", "Andreea Tomescu"], "title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Moral stories are a time-tested vehicle for transmitting values, yet modern\nNLP lacks a large, structured corpus that couples coherent narratives with\nexplicit ethical lessons. We close this gap with TF1-EN-3M, the first open\ndataset of three million English-language fables generated exclusively by\ninstruction-tuned models no larger than 8B parameters. Each story follows a\nsix-slot scaffold (character -> trait -> setting -> conflict -> resolution ->\nmoral), produced through a combinatorial prompt engine that guarantees genre\nfidelity while covering a broad thematic space.\n  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores\ngrammar, creativity, moral clarity, and template adherence with (ii)\nreference-free diversity and readability metrics. Among ten open-weight\ncandidates, an 8B-parameter Llama-3 variant delivers the best quality-speed\ntrade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)\nat approximately 13.5 cents per 1,000 fables.\n  We release the dataset, generation code, evaluation scripts, and full\nmetadata under a permissive license, enabling exact reproducibility and cost\nbenchmarking. TF1-EN-3M opens avenues for research in instruction following,\nnarrative intelligence, value alignment, and child-friendly educational AI,\ndemonstrating that large-scale moral storytelling no longer requires\nproprietary giant models."}
{"id": "2504.20609", "pdf": "https://arxiv.org/pdf/2504.20609", "abs": "https://arxiv.org/abs/2504.20609", "authors": ["Xinyu Yao", "Mengdi Wang", "Bo Chen", "Xiaobing Zhao"], "title": "WenyanGPT: A Large Language Model for Classical Chinese Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Classical Chinese, as the core carrier of Chinese culture, plays a crucial\nrole in the inheritance and study of ancient literature. However, existing\nnatural language processing models primarily optimize for Modern Chinese,\nresulting in inadequate performance on Classical Chinese. This paper presents a\ncomprehensive solution for Classical Chinese language processing. By continuing\npre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we\nconstruct a large language model, WenyanGPT, which is specifically designed for\nClassical Chinese tasks. Additionally, we develop an evaluation benchmark\ndataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that\nWenyanGPT significantly outperforms current advanced LLMs in various Classical\nChinese tasks. We make the model's training data, instruction fine-tuning\ndata\\footnote, and evaluation benchmark dataset publicly available to promote\nfurther research and development in the field of Classical Chinese processing."}
{"id": "2504.20643", "pdf": "https://arxiv.org/pdf/2504.20643", "abs": "https://arxiv.org/abs/2504.20643", "authors": ["Moran Mizrahi", "Chen Shani", "Gabriel Stanovsky", "Dan Jurafsky", "Dafna Shahaf"], "title": "Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 8 figures", "summary": "Large Language Models (LLMs) excel at countless tasks, yet struggle with\ncreativity. In this paper, we introduce a novel approach that couples LLMs with\nstructured representations and cognitively inspired manipulations to generate\nmore creative and diverse ideas. Our notion of creativity goes beyond\nsuperficial token-level variations; rather, we explicitly recombine structured\nrepresentations of existing ideas, allowing our algorithm to effectively\nexplore the more abstract landscape of ideas. We demonstrate our approach in\nthe culinary domain with DishCOVER, a model that generates creative recipes.\nExperiments comparing our model's results to those of GPT-4o show greater\ndiversity. Domain expert evaluations reveal that our outputs, which are mostly\ncoherent and feasible culinary creations, significantly surpass GPT-4o in terms\nof novelty, thus outperforming it in creative generation. We hope our work\ninspires further research into structured creativity in AI."}
{"id": "2504.20668", "pdf": "https://arxiv.org/pdf/2504.20668", "abs": "https://arxiv.org/abs/2504.20668", "authors": ["Ivan Vykopal", "Martin Hyben", "Robert Moro", "Michal Gregor", "Jakub Simko"], "title": "A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages", "categories": ["cs.CL"], "comment": null, "summary": "Online disinformation poses a global challenge, placing significant demands\non fact-checkers who must verify claims efficiently to prevent the spread of\nfalse information. A major issue in this process is the redundant verification\nof already fact-checked claims, which increases workload and delays responses\nto newly emerging claims. This research introduces an approach that retrieves\npreviously fact-checked claims, evaluates their relevance to a given input, and\nprovides supplementary information to support fact-checkers. Our method employs\nlarge language models (LLMs) to filter irrelevant fact-checks and generate\nconcise summaries and explanations, enabling fact-checkers to faster assess\nwhether a claim has been verified before. In addition, we evaluate our approach\nthrough both automatic and human assessments, where humans interact with the\ndeveloped tool to review its effectiveness. Our results demonstrate that LLMs\nare able to filter out many irrelevant fact-checks and, therefore, reduce\neffort and streamline the fact-checking process."}
{"id": "2504.20678", "pdf": "https://arxiv.org/pdf/2504.20678", "abs": "https://arxiv.org/abs/2504.20678", "authors": ["Yaroslav Getman", "Tamás Grósz", "Mikko Kurimo", "Giampiero Salvi"], "title": "Non-native Children's Automatic Speech Assessment Challenge (NOCASA)", "categories": ["cs.CL", "eess.AS"], "comment": "First draft of the baseline paper for the NOCASA competition\n  (https://teflon.aalto.fi/nocasa-2025/), 5 pages", "summary": "This paper presents the \"Non-native Children's Automatic Speech Assessment\"\n(NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA\nchallenges participants to develop new systems that can assess single-word\npronunciations of young second language (L2) learners as part of a gamified\npronunciation training app. To achieve this, several issues must be addressed,\nmost notably the limited nature of available training data and the highly\nunbalanced distribution among the pronunciation level categories. To expedite\nthe development, we provide a pseudo-anonymized training data (TeflonNorL2),\ncontaining 10,334 recordings from 44 speakers attempting to pronounce 205\ndistinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that\nshould be given in the game). In addition to the data, two already trained\nsystems are released as official baselines: an SVM classifier trained on the\nComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter\nachieves the best performance on the challenge test set, with an unweighted\naverage recall (UAR) of 36.37%."}
{"id": "2504.20679", "pdf": "https://arxiv.org/pdf/2504.20679", "abs": "https://arxiv.org/abs/2504.20679", "authors": ["Wing Yan Li", "Zeqiang Wang", "Jon Johnson", "Suparna De"], "title": "Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Automated detection of semantically equivalent questions in longitudinal\nsocial science surveys is crucial for long-term studies informing empirical\nresearch in the social, economic, and health sciences. Retrieving equivalent\nquestions faces dual challenges: inconsistent representation of theoretical\nconstructs (i.e. concept/sub-concept) across studies as well as between\nquestion and response options, and the evolution of vocabulary and structure in\nlongitudinal text. To address these challenges, our multi-disciplinary\ncollaboration of computer scientists and survey specialists presents a new\ninformation retrieval (IR) task of identifying concept (e.g. Housing, Job,\netc.) equivalence across question and response options to harmonise\nlongitudinal population studies. This paper investigates multiple unsupervised\napproaches on a survey dataset spanning 1946-2020, including probabilistic\nmodels, linear probing of language models, and pre-trained neural networks\nspecialised for IR. We show that IR-specialised neural models achieve the\nhighest overall performance with other approaches performing comparably.\nAdditionally, the re-ranking of the probabilistic model's results with neural\nmodels only introduces modest improvements of 0.07 at most in F1-score.\nQualitative post-hoc evaluation by survey specialists shows that models\ngenerally have a low sensitivity to questions with high lexical overlap,\nparticularly in cases where sub-concepts are mismatched. Altogether, our\nanalysis serves to further research on harmonising longitudinal studies in\nsocial science."}
{"id": "2504.20699", "pdf": "https://arxiv.org/pdf/2504.20699", "abs": "https://arxiv.org/abs/2504.20699", "authors": ["Evangelia Gogoulou", "Shorouq Zahra", "Liane Guillou", "Luise Dürlich", "Joakim Nivre"], "title": "Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A frequently observed problem with LLMs is their tendency to generate output\nthat is nonsensical, illogical, or factually incorrect, often referred to\nbroadly as hallucination. Building on the recently proposed HalluciGen task for\nhallucination detection and generation, we evaluate a suite of open-access LLMs\non their ability to detect intrinsic hallucinations in two conditional\ngeneration tasks: translation and paraphrasing. We study how model performance\nvaries across tasks and language and we investigate the impact of model size,\ninstruction tuning, and prompt choice. We find that performance varies across\nmodels but is consistent across prompts. Finally, we find that NLI models\nperform comparably well, suggesting that LLM-based detectors are not the only\nviable option for this specific task."}
{"id": "2504.20703", "pdf": "https://arxiv.org/pdf/2504.20703", "abs": "https://arxiv.org/abs/2504.20703", "authors": ["Foteini Papadopoulou", "Osman Mutlu", "Neris Özen", "Bas H. M. van der Velden", "Iris Hendrickx", "Ali Hürriyetoğlu"], "title": "BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents our system developed for the SemEval-2025 Task 9: The\nFood Hazard Detection Challenge. The shared task's objective is to evaluate\nexplainable classification systems for classifying hazards and products in two\nlevels of granularity from food recall incident reports. In this work, we\npropose text augmentation techniques as a way to improve poor performance on\nminority classes and compare their effect for each category on various\ntransformer and machine learning models. We explore three word-level data\naugmentation techniques, namely synonym replacement, random word swapping, and\ncontextual word insertion. The results show that transformer models tend to\nhave a better overall performance. None of the three augmentation techniques\nconsistently improved overall performance for classifying hazards and products.\nWe observed a statistically significant improvement (P < 0.05) in the\nfine-grained categories when using the BERT model to compare the baseline with\neach augmented model. Compared to the baseline, the contextual words insertion\naugmentation improved the accuracy of predictions for the minority hazard\nclasses by 6%. This suggests that targeted augmentation of minority classes can\nimprove the performance of transformer models."}
{"id": "2504.20708", "pdf": "https://arxiv.org/pdf/2504.20708", "abs": "https://arxiv.org/abs/2504.20708", "authors": ["Hasan Abed Al Kader Hammoud", "Hani Itani", "Bernard Ghanem"], "title": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex\nproblems. Standard evaluation practice involves generating a complete reasoning\ntrace and assessing the correctness of the final answer presented at its\nconclusion. In this paper, we challenge the reliance on the final answer by\nposing the following two questions: Does the final answer reliably represent\nthe model's optimal conclusion? Can alternative reasoning paths yield different\nresults? To answer these questions, we analyze intermediate reasoning steps,\ntermed subthoughts, and propose a method based on our findings. Our approach\ninvolves segmenting a reasoning trace into sequential subthoughts based on\nlinguistic cues. We start by prompting the model to generate continuations from\nthe end-point of each intermediate subthought. We extract a potential answer\nfrom every completed continuation originating from different subthoughts. We\nfind that aggregating these answers by selecting the most frequent one (the\nmode) often yields significantly higher accuracy compared to relying solely on\nthe answer derived from the original complete trace. Analyzing the consistency\namong the answers derived from different subthoughts reveals characteristics\nthat correlate with the model's confidence and correctness, suggesting\npotential for identifying less reliable answers. Our experiments across various\nLLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)\nshow consistent accuracy improvements, with gains reaching up to 13\\% and 10\\%\nrespectively. Implementation is available at:\nhttps://github.com/hammoudhasan/SubthoughtReasoner."}
{"id": "2504.20734", "pdf": "https://arxiv.org/pdf/2504.20734", "abs": "https://arxiv.org/abs/2504.20734", "authors": ["Woongyeong Yeo", "Kangsan Kim", "Soyeong Jeong", "Jinheon Baek", "Sung Ju Hwang"], "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": "Project page : https://universalrag.github.io", "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines."}
{"id": "2504.20752", "pdf": "https://arxiv.org/pdf/2504.20752", "abs": "https://arxiv.org/abs/2504.20752", "authors": ["Roman Abramov", "Felix Steinbauer", "Gjergji Kasneci"], "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.3; I.7"], "comment": null, "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing $\\phi_r$ drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models."}
{"id": "2504.20769", "pdf": "https://arxiv.org/pdf/2504.20769", "abs": "https://arxiv.org/abs/2504.20769", "authors": ["Wenxiao Wang", "Parsa Hosseini", "Soheil Feizi"], "title": "Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought prompting has demonstrated great success in facilitating the\nreasoning abilities of large language models. In this work, we explore how\nthese enhanced reasoning abilities can be exploited to improve the robustness\nof large language models in tasks that are not necessarily reasoning-focused.\nIn particular, we show how a wide range of large language models exhibit\nsignificantly improved robustness against reference corruption using a simple\nmethod called chain-of-defensive-thought, where only a few exemplars with\nstructured and defensive reasoning are provided as demonstrations. Empirically,\nthe improvements can be astounding, especially given the simplicity and\napplicability of the method. For example, in the Natural Questions task, the\naccuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting\nwhen 1 out of 10 references provided is corrupted with prompt injection\nattacks. In contrast, GPT-4o using chain-of-defensive-thought prompting\nmaintains an accuracy of 50%."}
{"id": "2504.20771", "pdf": "https://arxiv.org/pdf/2504.20771", "abs": "https://arxiv.org/abs/2504.20771", "authors": ["Haitao Wu", "Zongbo Han", "Huaxi Huang", "Changqing Zhang"], "title": "Turing Machine Evaluation for Large Language Model", "categories": ["cs.CL"], "comment": null, "summary": "With the rapid development and widespread application of Large Language\nModels (LLMs), rigorous evaluation has become particularly crucial. This\nresearch adopts a novel perspective, focusing on evaluating the core\ncomputational reasoning ability of LLMs, defined as the capacity of model to\naccurately understand rules, and execute logically computing operations. This\ncapability assesses the reliability of LLMs as precise executors, and is\ncritical to advanced tasks such as complex code generation and multi-step\nproblem-solving. We propose an evaluation framework based on Universal Turing\nMachine (UTM) simulation. This framework requires LLMs to strictly follow\ninstructions and track dynamic states, such as tape content and read/write head\nposition, during multi-step computations. To enable standardized evaluation, we\ndeveloped TMBench, a benchmark for systematically studying the computational\nreasoning capabilities of LLMs. TMBench provides several key advantages,\nincluding knowledge-agnostic evaluation, adjustable difficulty, foundational\ncoverage through Turing machine encoding, and unlimited capacity for instance\ngeneration, ensuring scalability as models continue to evolve. We find that\nmodel performance on TMBench correlates strongly with performance on other\nrecognized reasoning benchmarks (Pearson correlation coefficient is 0.73),\nclearly demonstrating that computational reasoning is a significant dimension\nfor measuring the deep capabilities of LLMs. Code and data are available at\nhttps://github.com/HaitaoWuTJU/Turing-Machine-Bench."}
{"id": "2504.20839", "pdf": "https://arxiv.org/pdf/2504.20839", "abs": "https://arxiv.org/abs/2504.20839", "authors": ["D. -F. Qin"], "title": "Universal language model with the intervention of quantum theory", "categories": ["cs.CL", "quant-ph"], "comment": null, "summary": "This paper examines language modeling based on the theory of quantum\nmechanics. It focuses on the introduction of quantum mechanics into the\nsymbol-meaning pairs of language in order to build a representation model of\nnatural language. At the same time, it is realized that word embedding, which\nis widely used as a basic technique for statistical language modeling, can be\nexplained and improved by the mathematical framework of quantum mechanics. On\nthis basis, this paper continues to try to use quantum statistics and other\nrelated theories to study the mathematical representation, natural evolution\nand statistical properties of natural language. It is also assumed that the\nsource of such quantum properties is the physicality of information. The\nfeasibility of using quantum theory to model natural language is pointed out\nthrough the construction of a experimental code. The paper discusses, in terms\nof applications, the possible help of the theory in constructing generative\nmodels that are popular nowadays. A preliminary discussion of future\napplications of the theory to quantum computers is also presented."}
{"id": "2504.20849", "pdf": "https://arxiv.org/pdf/2504.20849", "abs": "https://arxiv.org/abs/2504.20849", "authors": ["Anum Afzal", "Alexandre Mercier", "Florian Matthes"], "title": "JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry", "categories": ["cs.CL"], "comment": null, "summary": "Online platforms are increasingly interested in using Data-to-Text\ntechnologies to generate content and help their users. Unfortunately,\ntraditional generative methods often fall into repetitive patterns, resulting\nin monotonous galleries of texts after only a few iterations. In this paper, we\ninvestigate LLM-based data-to-text approaches to automatically generate\nmarketing texts that are of sufficient quality and diverse enough for broad\nadoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in\nconjunction with fine-tuning, few-shot, and zero-shot approaches to set a\nbaseline for diverse marketing texts. We also introduce a metric JaccDiv to\nevaluate the diversity of a set of texts. This research extends its relevance\nbeyond the music industry, proving beneficial in various fields where\nrepetitive automated content generation is prevalent."}
{"id": "2504.20922", "pdf": "https://arxiv.org/pdf/2504.20922", "abs": "https://arxiv.org/abs/2504.20922", "authors": ["Miguel Nogales", "Matteo Gambella", "Manuel Roveri"], "title": "DYNAMAX: Dynamic computing for Transformers and Mamba based architectures", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary), 68T07 (Secondary)"], "comment": "Accepted to IJCNN 2025", "summary": "Early exits (EEs) offer a promising approach to reducing computational costs\nand latency by dynamically terminating inference once a satisfactory prediction\nconfidence on a data sample is achieved. Although many works integrate EEs into\nencoder-only Transformers, their application to decoder-only architectures and,\nmore importantly, Mamba models, a novel family of state-space architectures in\nthe LLM realm, remains insufficiently explored. This work introduces DYNAMAX,\nthe first framework to exploit the unique properties of Mamba architectures for\nearly exit mechanisms. We not only integrate EEs into Mamba but also repurpose\nMamba as an efficient EE classifier for both Mamba-based and transformer-based\nLLMs, showcasing its versatility. Our experiments employ the Mistral 7B\ntransformer compared to the Codestral 7B Mamba model, using data sets such as\nTruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and\nconsistency. The results highlight the adaptability of Mamba as a powerful EE\nclassifier and its efficiency in balancing computational cost and performance\nquality across NLP tasks. By leveraging Mamba's inherent design for dynamic\nprocessing, we open pathways for scalable and efficient inference in embedded\napplications and resource-constrained environments. This study underscores the\ntransformative potential of Mamba in redefining dynamic computing paradigms for\nLLMs."}
{"id": "2504.20946", "pdf": "https://arxiv.org/pdf/2504.20946", "abs": "https://arxiv.org/abs/2504.20946", "authors": ["Tyler McDonald", "Ali Emami"], "title": "Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) continue to be leveraged for daily tasks,\nprompt engineering remains an active field of contribution within computational\nlinguistics, particularly in domains requiring specialized knowledge such as\narithmetic reasoning. While these LLMs are optimized for a variety of tasks,\ntheir exhaustive employment may become computationally or financially\ncumbersome for small teams. Additionally, complete reliance on proprietary,\nclosed-source models often limits customization and adaptability, posing\nsignificant challenges in research and application scalability. Instead, by\nleveraging open-source models at or below 7 billion parameters, we can optimize\nour resource usage while still observing remarkable gains over standard\nprompting approaches. To cultivate this notion, we introduce Trace-of-Thought\nPrompting, a simple, zero-shot prompt engineering method that instructs LLMs to\ncreate observable subproblems using critical problem-solving, specifically\ndesigned to enhance arithmetic reasoning capabilities. When applied to\nopen-source models in tandem with GPT-4, we observe that Trace-of-Thought not\nonly allows novel insight into the problem-solving process but also introduces\nperformance gains as large as 125% on language models at or below 7 billion\nparameters. This approach underscores the potential of open-source initiatives\nin democratizing AI research and improving the accessibility of high-quality\ncomputational linguistics applications."}
{"id": "2504.20951", "pdf": "https://arxiv.org/pdf/2504.20951", "abs": "https://arxiv.org/abs/2504.20951", "authors": ["Maryna Vyshnyvetska"], "title": "Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models", "categories": ["cs.CL"], "comment": "12 pages, 1 figure", "summary": "We propose a theoretical model called \"information gravity\" to describe the\ntext generation process in large language models (LLMs). The model uses\nphysical apparatus from field theory and spacetime geometry to formalize the\ninteraction between user queries and the probability distribution of generated\ntokens. A query is viewed as an object with \"information mass\" that curves the\nsemantic space of the model, creating gravitational potential wells that\n\"attract\" tokens during generation. This model offers a mechanism to explain\nseveral observed phenomena in LLM behavior, including hallucinations (emerging\nfrom low-density semantic voids), sensitivity to query formulation (due to\nsemantic field curvature changes), and the influence of sampling temperature on\noutput diversity."}
{"id": "2504.20964", "pdf": "https://arxiv.org/pdf/2504.20964", "abs": "https://arxiv.org/abs/2504.20964", "authors": ["Shangyu Li", "Juyong Jiang", "Tiancheng Zhao", "Jiasi Shen"], "title": "OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification", "categories": ["cs.CL", "cs.AI", "cs.OS", "cs.PL", "cs.SE"], "comment": null, "summary": "We introduce OSVBench, a new benchmark for evaluating Large Language Models\n(LLMs) in generating complete specification code pertaining to operating system\nkernel verification tasks. The benchmark first defines the specification\ngeneration problem into a program synthesis problem within a confined scope of\nsyntax and semantics by providing LLMs with the programming model. The LLMs are\nrequired to understand the provided verification assumption and the potential\nsyntax and semantics space to search for, then generate the complete\nspecification for the potentially buggy operating system code implementation\nunder the guidance of the high-level functional description of the operating\nsystem. This benchmark is built upon a real-world operating system kernel,\nHyperkernel, and consists of 245 complex specification generation tasks in\ntotal, each is a long context task of about 20k-30k tokens. Our comprehensive\nevaluation of 12 LLMs exhibits the limited performance of the current LLMs on\nthe specification generation tasks for operating system verification.\nSignificant disparities in their performance on the benchmark highlight\ndifferences in their ability to handle long-context code generation tasks. The\nevaluation toolkit and benchmark are available at\nhttps://github.com/lishangyu-hkust/OSVBench."}
{"id": "2504.20972", "pdf": "https://arxiv.org/pdf/2504.20972", "abs": "https://arxiv.org/abs/2504.20972", "authors": ["Yifan Wei", "Xiaoyan Yu", "Ran Song", "Hao Peng", "Angsheng Li"], "title": "SetKE: Knowledge Editing for Knowledge Elements Overlap", "categories": ["cs.CL"], "comment": "The CR version will be updated subsequently", "summary": "Large Language Models (LLMs) excel in tasks such as retrieval and question\nanswering but require updates to incorporate new knowledge and reduce\ninaccuracies and hallucinations. Traditional updating methods, like fine-tuning\nand incremental learning, face challenges such as overfitting and high\ncomputational costs. Knowledge Editing (KE) provides a promising alternative\nbut often overlooks the Knowledge Element Overlap (KEO) phenomenon, where\nmultiple triplets share common elements, leading to editing conflicts. We\nidentify the prevalence of KEO in existing KE datasets and show its significant\nimpact on current KE methods, causing performance degradation in handling such\ntriplets. To address this, we propose a new formulation, Knowledge Set Editing\n(KSE), and introduce SetKE, a method that edits sets of triplets\nsimultaneously. Experimental results demonstrate that SetKE outperforms\nexisting methods in KEO scenarios on mainstream LLMs. Additionally, we\nintroduce EditSet, a dataset containing KEO triplets, providing a comprehensive\nbenchmark."}
{"id": "2504.20059", "pdf": "https://arxiv.org/pdf/2504.20059", "abs": "https://arxiv.org/abs/2504.20059", "authors": ["Joey Chan", "Qiao Jin", "Nicholas Wan", "Charalampos S. Floudas", "Elisabetta Xue", "Zhiyong Lu"], "title": "Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "10 pages with 2 figures and 2 tables", "summary": "Clinical trials are crucial for assessing new treatments; however,\nrecruitment challenges - such as limited awareness, complex eligibility\ncriteria, and referral barriers - hinder their success. With the growth of\nonline platforms, patients increasingly turn to social media and health\ncommunities for support, research, and advocacy, expanding recruitment pools\nand established enrollment pathways. Recognizing this potential, we utilized\nTrialGPT, a framework that leverages a large language model (LLM) as its\nbackbone, to match 50 online patient cases (collected from published case\nreports and a social media website) to clinical trials and evaluate performance\nagainst traditional keyword-based searches. Our results show that TrialGPT\noutperforms traditional methods by 46% in identifying eligible trials, with\neach patient, on average, being eligible for around 7 trials. Additionally, our\noutreach efforts to case authors and trial organizers regarding these\npatient-trial matches yielded highly positive feedback, which we present from\nboth perspectives."}
{"id": "2504.20073", "pdf": "https://arxiv.org/pdf/2504.20073", "abs": "https://arxiv.org/abs/2504.20073", "authors": ["Zihan Wang", "Kangrui Wang", "Qineng Wang", "Pingyue Zhang", "Linjie Li", "Zhengyuan Yang", "Kefan Yu", "Minh Nhat Nguyen", "Licheng Liu", "Eli Gottlieb", "Monica Lam", "Yiping Lu", "Kyunghyun Cho", "Jiajun Wu", "Li Fei-Fei", "Lijuan Wang", "Yejin Choi", "Manling Li"], "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) as interactive agents presents unique\nchallenges including long-horizon decision making and interacting with\nstochastic environment feedback. While reinforcement learning (RL) has enabled\nprogress in static tasks, multi-turn agent RL training remains underexplored.\nWe propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a\ngeneral framework for trajectory-level agent RL, and introduce RAGEN, a modular\nsystem for training and evaluating LLM agents. Our study on three stylized\nenvironments reveals three core findings. First, our agent RL training shows a\nrecurring mode of Echo Trap where reward variance cliffs and gradient spikes;\nwe address this with StarPO-S, a stabilized variant with trajectory filtering,\ncritic incorporation, and decoupled clipping. Second, we find the shaping of RL\nrollouts would benefit from diverse initial states, medium interaction\ngranularity and more frequent sampling. Third, we show that without\nfine-grained, reasoning-aware reward signals, agent reasoning hardly emerge\nthrough multi-turn RL and they may show shallow strategies or hallucinated\nthoughts. Code and environments are available at\nhttps://github.com/RAGEN-AI/RAGEN."}
{"id": "2504.20084", "pdf": "https://arxiv.org/pdf/2504.20084", "abs": "https://arxiv.org/abs/2504.20084", "authors": ["Xiaojian Li", "Haoyuan Shi", "Rongwu Xu", "Wei Xu"], "title": "AI Awareness", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Recent breakthroughs in artificial intelligence (AI) have brought about\nincreasingly capable systems that demonstrate remarkable abilities in\nreasoning, language understanding, and problem-solving. These advancements have\nprompted a renewed examination of AI awareness, not as a philosophical question\nof consciousness, but as a measurable, functional capacity. In this review, we\nexplore the emerging landscape of AI awareness, which includes meta-cognition\n(the ability to represent and reason about its own state), self-awareness\n(recognizing its own identity, knowledge, limitations, inter alia), social\nawareness (modeling the knowledge, intentions, and behaviors of other agents),\nand situational awareness (assessing and responding to the context in which it\noperates).\n  First, we draw on insights from cognitive science, psychology, and\ncomputational theory to trace the theoretical foundations of awareness and\nexamine how the four distinct forms of AI awareness manifest in\nstate-of-the-art AI. Next, we systematically analyze current evaluation methods\nand empirical findings to better understand these manifestations. Building on\nthis, we explore how AI awareness is closely linked to AI capabilities,\ndemonstrating that more aware AI agents tend to exhibit higher levels of\nintelligent behaviors. Finally, we discuss the risks associated with AI\nawareness, including key topics in AI safety, alignment, and broader ethical\nconcerns.\n  AI awareness is a double-edged sword: it improves general capabilities, i.e.,\nreasoning, safety, while also raises concerns around misalignment and societal\nrisks, demanding careful oversight as AI capabilities grow. On the whole, our\ninterdisciplinary review provides a roadmap for future research and aims to\nclarify the role of AI awareness in the ongoing development of intelligent\nmachines."}
{"id": "2504.20094", "pdf": "https://arxiv.org/pdf/2504.20094", "abs": "https://arxiv.org/abs/2504.20094", "authors": ["Zheng Hui", "Xiaokai Wei", "Yexi Jiang", "Kevin Gao", "Chen Wang", "Frank Ong", "Se-eun Yoon", "Rachit Pareek", "Michelle Gong"], "title": "MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?", "categories": ["cs.IR", "cs.CL", "cs.HC"], "comment": null, "summary": "In this paper, we propose a multi-agent collaboration framework called MATCHA\nfor conversational recommendation system, leveraging large language models\n(LLMs) to enhance personalization and user engagement. Users can request\nrecommendations via free-form text and receive curated lists aligned with their\ninterests, preferences, and constraints. Our system introduces specialized\nagents for intent analysis, candidate generation, ranking, re-ranking,\nexplainability, and safeguards. These agents collaboratively improve\nrecommendations accuracy, diversity, and safety. On eight metrics, our model\nachieves superior or comparable performance to the current state-of-the-art.\nThrough comparisons with six baseline models, our approach addresses key\nchallenges in conversational recommendation systems for game recommendations,\nincluding: (1) handling complex, user-specific requests, (2) enhancing\npersonalization through multi-agent collaboration, (3) empirical evaluation and\ndeployment, and (4) ensuring safe and trustworthy interactions."}
{"id": "2504.20117", "pdf": "https://arxiv.org/pdf/2504.20117", "abs": "https://arxiv.org/abs/2504.20117", "authors": ["Shubham Gandhi", "Dhruv Shah", "Manasi Patwardhan", "Lovekesh Vig", "Gautam Shroff"], "title": "ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "In this paper we introduce ResearchCodeAgent, a novel multi-agent system\nleveraging large language models (LLMs) agents to automate the codification of\nresearch methodologies described in machine learning literature. The system\nbridges the gap between high-level research concepts and their practical\nimplementation, allowing researchers auto-generating code of existing research\npapers for benchmarking or building on top-of existing methods specified in the\nliterature with availability of partial or complete starter code.\nResearchCodeAgent employs a flexible agent architecture with a comprehensive\naction suite, enabling context-aware interactions with the research\nenvironment. The system incorporates a dynamic planning mechanism, utilizing\nboth short and long-term memory to adapt its approach iteratively. We evaluate\nResearchCodeAgent on three distinct machine learning tasks with distinct task\ncomplexity and representing different parts of the ML pipeline: data\naugmentation, optimization, and data batching. Our results demonstrate the\nsystem's effectiveness and generalizability, with 46.9% of generated code being\nhigh-quality and error-free, and 25% showing performance improvements over\nbaseline implementations. Empirical analysis shows an average reduction of\n57.9% in coding time compared to manual implementation. We observe higher gains\nfor more complex tasks. ResearchCodeAgent represents a significant step towards\nautomating the research implementation process, potentially accelerating the\npace of machine learning research."}
{"id": "2504.20199", "pdf": "https://arxiv.org/pdf/2504.20199", "abs": "https://arxiv.org/abs/2504.20199", "authors": ["Juntian Zhang", "Chuanqi cheng", "Yuhan Liu", "Wei Liu", "Jian Luan", "Rui Yan"], "title": "Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Vision-language models (VLMs) achieve remarkable success in single-image\ntasks. However, real-world scenarios often involve intricate multi-image\ninputs, leading to a notable performance decline as models struggle to\ndisentangle critical information scattered across complex visual features. In\nthis work, we propose Focus-Centric Visual Chain, a novel paradigm that\nenhances VLMs'perception, comprehension, and reasoning abilities in multi-image\nscenarios. To facilitate this paradigm, we propose Focus-Centric Data\nSynthesis, a scalable bottom-up approach for synthesizing high-quality data\nwith elaborate reasoning paths. Through this approach, We construct VISC-150K,\na large-scale dataset with reasoning data in the form of Focus-Centric Visual\nChain, specifically designed for multi-image tasks. Experimental results on\nseven multi-image benchmarks demonstrate that our method achieves average\nperformance gains of 3.16% and 2.24% across two distinct model architectures,\nwithout compromising the general vision-language capabilities. our study\nrepresents a significant step toward more robust and capable vision-language\nsystems that can handle complex visual scenarios."}
{"id": "2504.20294", "pdf": "https://arxiv.org/pdf/2504.20294", "abs": "https://arxiv.org/abs/2504.20294", "authors": ["William P. McCarthy", "Saujas Vaduguru", "Karl D. D. Willis", "Justin Matejka", "Judith E. Fan", "Daniel Fried", "Yewen Pu"], "title": "mrCAD: Multimodal Refinement of Computer-aided Designs", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "the first two authors contributed equally", "summary": "A key feature of human collaboration is the ability to iteratively refine the\nconcepts we have communicated. In contrast, while generative AI excels at the\n\\textit{generation} of content, it often struggles to make specific\nlanguage-guided \\textit{modifications} of its prior outputs. To bridge the gap\nbetween how humans and machines perform edits, we present mrCAD, a dataset of\nmultimodal instructions in a communication game. In each game, players created\ncomputer aided designs (CADs) and refined them over several rounds to match\nspecific target designs. Only one player, the Designer, could see the target,\nand they must instruct the other player, the Maker, using text, drawing, or a\ncombination of modalities. mrCAD consists of 6,082 communication games, 15,163\ninstruction-execution rounds, played between 1,092 pairs of human players. We\nanalyze the dataset and find that generation and refinement instructions differ\nin their composition of drawing and text. Using the mrCAD task as a benchmark,\nwe find that state-of-the-art VLMs are better at following generation\ninstructions than refinement instructions. These results lay a foundation for\nanalyzing and modeling a multimodal language of refinement that is not\nrepresented in previous datasets."}
{"id": "2504.20456", "pdf": "https://arxiv.org/pdf/2504.20456", "abs": "https://arxiv.org/abs/2504.20456", "authors": ["Gabe Guo", "Stefano Ermon"], "title": "Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In arbitrary-order language models, it is an open question how to sample\ntokens in parallel from the correct joint distribution. With discrete diffusion\nmodels, the more tokens they generate in parallel, the less their predicted\ndistributions adhere to the originally learned data distribution, as they rely\non a conditional independence assumption that only works with infinitesimally\nsmall timesteps. We find that a different class of models, any-subset\nautoregressive models (AS-ARMs), holds the solution. As implied by the name,\nAS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs\nsupport parallelized joint probability density estimation, allowing them to\ncorrect their own parallel-generated token distributions, via our Any-Subset\nSpeculative Decoding (ASSD) algorithm. ASSD provably enables generation of\ntokens from the correct joint distribution, with the number of neural network\ncalls upper bounded by the number of tokens predicted. We empirically verify\nthat ASSD speeds up language generation, without sacrificing quality.\nFurthermore, we provide a mathematically justified scheme for training AS-ARMs\nfor generation, and show that AS-ARMs achieve state-of-the-art performance\namong sub-200M parameter models on infilling benchmark tasks, and nearly match\nthe performance of models 50X larger on code generation. Our theoretical and\nempirical results indicate that the once-forgotten AS-ARMs are a promising\ndirection of language modeling."}
{"id": "2504.20458", "pdf": "https://arxiv.org/pdf/2504.20458", "abs": "https://arxiv.org/abs/2504.20458", "authors": ["Xiaolei Wang", "Chunxuan Xia", "Junyi Li", "Fanzhe Meng", "Lei Huang", "Jinpeng Wang", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted by SIGIR 2025", "summary": "Conversational recommendation systems (CRSs) use multi-turn interaction to\ncapture user preferences and provide personalized recommendations. A\nfundamental challenge in CRSs lies in effectively understanding user\npreferences from conversations. User preferences can be multifaceted and\ncomplex, posing significant challenges for accurate recommendations even with\naccess to abundant external knowledge. While interaction with users can clarify\ntheir true preferences, frequent user involvement can lead to a degraded user\nexperience.\n  To address this problem, we propose a generative reward model based simulated\nuser, named GRSU, for automatic interaction with CRSs. The simulated user\nprovides feedback to the items recommended by CRSs, enabling them to better\ncapture intricate user preferences through multi-turn interaction. Inspired by\ngenerative reward models, we design two types of feedback actions for the\nsimulated user: i.e., generative item scoring, which offers coarse-grained\nfeedback, and attribute-based item critique, which provides fine-grained\nfeedback. To ensure seamless integration, these feedback actions are unified\ninto an instruction-based format, allowing the development of a unified\nsimulated user via instruction tuning on synthesized data. With this simulated\nuser, automatic multi-turn interaction with CRSs can be effectively conducted.\nFurthermore, to strike a balance between effectiveness and efficiency, we draw\ninspiration from the paradigm of reward-guided search in complex reasoning\ntasks and employ beam search for the interaction process. On top of this, we\npropose an efficient candidate ranking method to improve the recommendation\nresults derived from interaction. Extensive experiments on public datasets\ndemonstrate the effectiveness, efficiency, and transferability of our approach."}
{"id": "2504.20571", "pdf": "https://arxiv.org/pdf/2504.20571", "abs": "https://arxiv.org/abs/2504.20571", "authors": ["Yiping Wang", "Qing Yang", "Zhiyuan Zeng", "Liliang Ren", "Lucas Liu", "Baolin Peng", "Hao Cheng", "Xuehai He", "Kuan Wang", "Jianfeng Gao", "Weizhu Chen", "Shuohang Wang", "Simon Shaolei Du", "Yelong Shen"], "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages, 12 figures, link: https://github.com/ypwang61/One-Shot-RLVR", "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR"}
{"id": "2504.20595", "pdf": "https://arxiv.org/pdf/2504.20595", "abs": "https://arxiv.org/abs/2504.20595", "authors": ["Rulin Shao", "Rui Qiao", "Varsha Kishore", "Niklas Muennighoff", "Xi Victoria Lin", "Daniela Rus", "Bryan Kian Hsiang Low", "Sewon Min", "Wen-tau Yih", "Pang Wei Koh", "Luke Zettlemoyer"], "title": "ReasonIR: Training Retrievers for Reasoning Tasks", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Our code is released at\n  \\url{https://github.com/facebookresearch/ReasonIR}", "summary": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model."}
{"id": "2504.20859", "pdf": "https://arxiv.org/pdf/2504.20859", "abs": "https://arxiv.org/abs/2504.20859", "authors": ["Guy Hadad", "Haggai Roitman", "Yotam Eshel", "Bracha Shapira", "Lior Rokach"], "title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication in SIGIR '25", "summary": "As new products are emerging daily, recommendation systems are required to\nquickly adapt to possible new domains without needing extensive retraining.\nThis work presents ``X-Cross'' -- a novel cross-domain\nsequential-recommendation model that recommends products in new domains by\nintegrating several domain-specific language models; each model is fine-tuned\nwith low-rank adapters (LoRA). Given a recommendation prompt, operating layer\nby layer, X-Cross dynamically refines the representation of each source\nlanguage model by integrating knowledge from all other models. These refined\nrepresentations are propagated from one layer to the next, leveraging the\nactivations from each domain adapter to ensure domain-specific nuances are\npreserved while enabling adaptability across domains. Using Amazon datasets for\nsequential recommendation, X-Cross achieves performance comparable to a model\nthat is fine-tuned with LoRA, while using only 25% of the additional\nparameters. In cross-domain tasks, such as adapting from Toys domain to Tools,\nElectronics or Sports, X-Cross demonstrates robust performance, while requiring\nabout 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.\nFurthermore, X-Cross achieves significant improvement in accuracy over\nalternative cross-domain baselines. Overall, X-Cross enables scalable and\nadaptive cross-domain recommendations, reducing computational overhead and\nproviding an efficient solution for data-constrained environments."}
{"id": "2504.20879", "pdf": "https://arxiv.org/pdf/2504.20879", "abs": "https://arxiv.org/abs/2504.20879", "authors": ["Shivalika Singh", "Yiyang Nan", "Alex Wang", "Daniel D'Souza", "Sayash Kapoor", "Ahmet Üstün", "Sanmi Koyejo", "Yuntian Deng", "Shayne Longpre", "Noah Smith", "Beyza Ermis", "Marzieh Fadaee", "Sara Hooker"], "title": "The Leaderboard Illusion", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ME"], "comment": "68 pages, 18 figures, 9 tables", "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field"}
{"id": "2504.20930", "pdf": "https://arxiv.org/pdf/2504.20930", "abs": "https://arxiv.org/abs/2504.20930", "authors": ["Ziqing Fan", "Cheng Liang", "Chaoyi Wu", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs."}
{"id": "2504.20938", "pdf": "https://arxiv.org/pdf/2504.20938", "abs": "https://arxiv.org/abs/2504.20938", "authors": ["Zhengfu He", "Junxuan Wang", "Rui Lin", "Xuyang Ge", "Wentao Shu", "Qiong Tang", "Junping Zhang", "Xipeng Qiu"], "title": "Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of\nTransformer attention layers to disentangle original Multi Head Self Attention\n(MHSA) into individually comprehensible components. Lorsa is designed to\naddress the challenge of attention superposition to understand\nattention-mediated interaction between features in different token positions.\nWe show that Lorsa heads find cleaner and finer-grained versions of previously\ndiscovered MHSA behaviors like induction heads, successor heads and attention\nsink behavior (i.e., heavily attending to the first token). Lorsa and Sparse\nAutoencoder (SAE) are both sparse dictionary learning methods applied to\ndifferent Transformer components, and lead to consistent findings in many ways.\nFor instance, we discover a comprehensive family of arithmetic-specific Lorsa\nheads, each corresponding to an atomic operation in Llama-3.1-8B. Automated\ninterpretability analysis indicates that Lorsa achieves parity with SAE in\ninterpretability while Lorsa exhibits superior circuit discovery properties,\nespecially for features computed collectively by multiple MHSA heads. We also\nconduct extensive experiments on architectural design ablation, Lorsa scaling\nlaw and error analysis."}
