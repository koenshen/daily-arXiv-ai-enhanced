{"id": "2505.00725", "pdf": "https://arxiv.org/pdf/2505.00725", "abs": "https://arxiv.org/abs/2505.00725", "authors": ["Bithiah Yuan"], "title": "FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.1; H.3.3"], "comment": "Submitted in partial fulfillment of the requirements for the Master\n  of Science degree in Computer Science at the University of Freiburg, July 31,\n  2020", "summary": "Motivated by the emerging demand in the financial industry for the automatic\nanalysis of unstructured and structured data at scale, Question Answering (QA)\nsystems can provide lucrative and competitive advantages to companies by\nfacilitating the decision making of financial advisers. Consequently, we\npropose a novel financial QA system using the transformer-based pre-trained\nBERT language model to address the limitations of data scarcity and language\nspecificity in the financial domain. Our system focuses on financial\nnon-factoid answer selection, which retrieves a set of passage-level texts and\nselects the most relevant as the answer. To increase efficiency, we formulate\nthe answer selection task as a re-ranking problem, in which our system consists\nof an Answer Retriever using BM25, a simple information retrieval approach, to\nfirst return a list of candidate answers, and an Answer Re-ranker built with\nvariants of pre-trained BERT language models to re-rank and select the most\nrelevant answers. We investigate various learning, further pre-training, and\nfine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a\nmodel built from applying the Transfer and Adapt further fine-tuning and\npointwise learning approach, is the most effective, improving the\nstate-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on\nNDCG, and 21% on Precision@1."}
{"id": "2505.00753", "pdf": "https://arxiv.org/pdf/2505.00753", "abs": "https://arxiv.org/abs/2505.00753", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Yankai Chen", "Chunyu Miao", "Hoang Nguyen", "Yue Zhou", "Weizhi Zhang", "Liancheng Fang", "Langzhou He", "Yangning Li", "Yuwei Cao", "Dongyuan Li", "Renhe Jiang", "Philip S. Yu"], "title": "A Survey on Large Language Model based Human-Agent Systems", "categories": ["cs.CL", "cs.LG"], "comment": "Paper lists and resources are available at\n  \\url{https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers}", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThis paper provides the first comprehensive and structured survey of LLM-HAS.\nIt clarifies fundamental concepts, systematically presents core components\nshaping these systems, including environment & profiling, human feedback,\ninteraction types, orchestration and communication, explores emerging\napplications, and discusses unique challenges and opportunities. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers."}
{"id": "2505.00776", "pdf": "https://arxiv.org/pdf/2505.00776", "abs": "https://arxiv.org/abs/2505.00776", "authors": ["Alessandro Raganato", "Rafael Peñaloza", "Marco Viviani", "Gabriella Pasi"], "title": "Reasoning Capabilities and Invariability of Large Language Models", "categories": ["cs.CL"], "comment": "Accepted for publication in the Proceedings of the 23rd IEEE/WIC\n  International Conference on Web Intelligence and Intelligent Agent Technology\n  (WI-IAT 2024)", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nmanipulating natural language across multiple applications, but their ability\nto handle simple reasoning tasks is often questioned. In this work, we aim to\nprovide a comprehensive analysis of LLMs' reasoning competence, specifically\nfocusing on their prompt dependency. In particular, we introduce a new\nbenchmark dataset with a series of simple reasoning questions demanding shallow\nlogical reasoning. Aligned with cognitive psychology standards, the questions\nare confined to a basic domain revolving around geometric figures, ensuring\nthat responses are independent of any pre-existing intuition about the world\nand rely solely on deduction. An empirical analysis involving zero-shot and\nfew-shot prompting across 24 LLMs of different sizes reveals that, while LLMs\nwith over 70 billion parameters perform better in the zero-shot setting, there\nis still a large room for improvement. An additional test with chain-of-thought\nprompting over 22 LLMs shows that this additional prompt can aid or damage the\nperformance of models, depending on whether the rationale is required before or\nafter the answer."}
{"id": "2505.00814", "pdf": "https://arxiv.org/pdf/2505.00814", "abs": "https://arxiv.org/abs/2505.00814", "authors": ["Mario Sänger", "Ulf Leser"], "title": "Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction", "categories": ["cs.CL"], "comment": null, "summary": "Automatic relationship extraction (RE) from biomedical literature is critical\nfor managing the vast amount of scientific knowledge produced each year. In\nrecent years, utilizing pre-trained language models (PLMs) has become the\nprevalent approach in RE. Several studies report improved performance when\nincorporating additional context information while fine-tuning PLMs for RE.\nHowever, variations in the PLMs applied, the databases used for augmentation,\nhyper-parameter optimization, and evaluation methods complicate direct\ncomparisons between studies and raise questions about the generalizability of\nthese findings. Our study addresses this research gap by evaluating PLMs\nenhanced with contextual information on five datasets spanning four relation\nscenarios within a consistent evaluation framework. We evaluate three baseline\nPLMs and first conduct extensive hyperparameter optimization. After selecting\nthe top-performing model, we enhance it with additional data, including textual\nentity descriptions, relational information from knowledge graphs, and\nmolecular structure encodings. Our findings illustrate the importance of i) the\nchoice of the underlying language model and ii) a comprehensive hyperparameter\noptimization for achieving strong extraction performance. Although inclusion of\ncontext information yield only minor overall improvements, an ablation study\nreveals substantial benefits for smaller PLMs when such external data was\nincluded during fine-tuning."}
{"id": "2505.00931", "pdf": "https://arxiv.org/pdf/2505.00931", "abs": "https://arxiv.org/abs/2505.00931", "authors": ["Timur Jaganov", "John Blake", "Julián Villegas", "Nicholas Carr"], "title": "Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 8 Figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "This study investigates the potential for Large Language Models (LLMs) to\nscale-up Dynamic Assessment (DA). To facilitate such an investigation, we first\ndeveloped DynaWrite-a modular, microservices-based grammatical tutoring\napplication which supports multiple LLMs to generate dynamic feedback to\nlearners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural\nchat to have the most potential to scale-up DA in the language learning\nclassroom. Further testing of these two candidates found both models performed\nsimilarly in their ability to accurately identify grammatical errors in user\nsentences. However, GPT-4o consistently outperformed neural chat in the quality\nof its DA by generating clear, consistent, and progressively explicit hints.\nReal-time responsiveness and system stability were also confirmed through\ndetailed performance testing, with GPT-4o exhibiting sufficient speed and\nstability. This study shows that LLMs can be used to scale-up dynamic\nassessment and thus enable dynamic assessment to be delivered to larger groups\nthan possible in traditional teacher-learner settings."}
{"id": "2505.00949", "pdf": "https://arxiv.org/pdf/2505.00949", "abs": "https://arxiv.org/abs/2505.00949", "authors": ["Akhiad Bercovich", "Itay Levy", "Izik Golan", "Mohammad Dabbah", "Ran El-Yaniv", "Omri Puny", "Ido Galil", "Zach Moshe", "Tomer Ronen", "Najeeb Nabwani", "Ido Shahaf", "Oren Tropp", "Ehud Karpas", "Ran Zilberstein", "Jiaqi Zeng", "Soumye Singhal", "Alexander Bukharin", "Yian Zhang", "Tugrul Konuk", "Gerald Shen", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Yoshi Suhara", "Olivier Delalleau", "Zijia Chen", "Zhilin Wang", "David Mosallanezhad", "Adi Renduchintala", "Haifeng Qian", "Dima Rekesh", "Fei Jia", "Somshubra Majumdar", "Vahid Noroozi", "Wasi Uddin Ahmad", "Sean Narenthiran", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Igor Gitman", "Ivan Moshkov", "Wei Du", "Shubham Toshniwal", "George Armstrong", "Branislav Kisacanin", "Matvei Novikov", "Daria Gitman", "Evelina Bakhturina", "Jane Polak Scowcroft", "John Kamalu", "Dan Su", "Kezhi Kong", "Markus Kliegl", "Rabeeh Karimi", "Ying Lin", "Sanjeev Satheesh", "Jupinder Parmar", "Pritam Gundecha", "Brandon Norick", "Joseph Jennings", "Shrimai Prabhumoye", "Syeda Nahida Akter", "Mostofa Patwary", "Abhinav Khattar", "Deepak Narayanan", "Roger Waleffe", "Jimmy Zhang", "Bor-Yiing Su", "Guyue Huang", "Terry Kong", "Parth Chadha", "Sahil Jain", "Christine Harvey", "Elad Segal", "Jining Huang", "Sergey Kashirsky", "Robert McQueen", "Izzy Putterman", "George Lam", "Arun Venkatesan", "Sherry Wu", "Vinh Nguyen", "Manoj Kilaru", "Andrew Wang", "Anna Warno", "Abhilash Somasamudramath", "Sandip Bhaskar", "Maka Dong", "Nave Assaf", "Shahar Mor", "Omer Ullman Argov", "Scot Junkin", "Oleksandr Romanenko", "Pedro Larroy", "Monika Katariya", "Marco Rovinelli", "Viji Balas", "Nicholas Edelman", "Anahita Bhiwandiwalla", "Muthu Subramaniam", "Smita Ithape", "Karthik Ramamoorthy", "Yuting Wu", "Suguna Varshini Velury", "Omri Almog", "Joyjit Daw", "Denys Fridman", "Erick Galinkin", "Michael Evans", "Katherine Luna", "Leon Derczynski", "Nikki Pope", "Eileen Long", "Seth Schneider", "Guillermo Siman", "Tomasz Grzegorzek", "Pablo Ribalta", "Monika Katariya", "Joey Conway", "Trisha Saar", "Ann Guan", "Krzysztof Pawelec", "Shyamala Prayaga", "Oleksii Kuchaiev", "Boris Ginsburg", "Oluwatobi Olabiyi", "Kari Briski", "Jonathan Cohen", "Bryan Catanzaro", "Jonah Alben", "Yonatan Geifman", "Eric Chung"], "title": "Llama-Nemotron: Efficient Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM."}
{"id": "2505.00977", "pdf": "https://arxiv.org/pdf/2505.00977", "abs": "https://arxiv.org/abs/2505.00977", "authors": ["Yingquan Chen", "Qianmu Li", "Xiaocong Wu", "Huifeng Li", "Qing Chang"], "title": "A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Generating high-quality steganographic text is a fundamental challenge in the\nfield of generative linguistic steganography. This challenge arises primarily\nfrom two aspects: firstly, the capabilities of existing models in text\ngeneration are limited; secondly, embedding algorithms fail to effectively\nmitigate the negative impacts of sensitive information's properties, such as\nsemantic content or randomness. Specifically, to ensure that the recipient can\naccurately extract hidden information, embedding algorithms often have to\nconsider selecting candidate words with relatively low probabilities. This\nphenomenon leads to a decrease in the number of high-probability candidate\nwords and an increase in low-probability candidate words, thereby compromising\nthe semantic coherence and logical fluency of the steganographic text and\ndiminishing the overall quality of the generated steganographic material. To\naddress this issue, this paper proposes a novel embedding algorithm,\ncharacter-based diffusion embedding algorithm (CDEA). Unlike existing embedding\nalgorithms that strive to eliminate the impact of sensitive information's\nproperties on the generation process, CDEA leverages sensitive information's\nproperties. It enhances the selection frequency of high-probability candidate\nwords in the candidate pool based on general statistical properties at the\ncharacter level and grouping methods based on power-law distributions, while\nreducing the selection frequency of low-probability candidate words in the\ncandidate pool. Furthermore, to ensure the effective transformation of\nsensitive information in long sequences, we also introduce the XLNet model.\nExperimental results demonstrate that the combination of CDEA and XLNet\nsignificantly improves the quality of generated steganographic text,\nparticularly in terms of perceptual-imperceptibility."}
{"id": "2505.00979", "pdf": "https://arxiv.org/pdf/2505.00979", "abs": "https://arxiv.org/abs/2505.00979", "authors": ["Xuhui Jiang", "Shengjie Ma", "Chengjin Xu", "Cehao Yang", "Liyu Zhang", "Jian Guo"], "title": "Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success but remain\ndata-inefficient, especially when learning from small, specialized corpora with\nlimited and proprietary data. Existing synthetic data generation methods for\ncontinue pre-training focus on intra-document content and overlook\ncross-document knowledge associations, limiting content diversity and depth. We\npropose Synthetic-on-Graph (SoG), a synthetic data generation framework that\nincorporates cross-document knowledge associations for efficient corpus\nexpansion. SoG constructs a context graph by extracting entities and concepts\nfrom the original corpus, representing cross-document associations, and\nemploying a graph walk strategy for knowledge-associated sampling. This\nenhances synthetic data diversity and coherence, enabling models to learn\ncomplex knowledge structures and handle rare knowledge. To further improve\nsynthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive\nClarifying (CC) synthetic, enhancing reasoning processes and discriminative\npower. Experiments show that SoG outperforms the state-of-the-art (SOTA) method\nin a multi-hop document Q&A dataset while performing comparably to the SOTA\nmethod on the reading comprehension task datasets, which also underscores the\nbetter generalization capability of SoG. Our work advances synthetic data\ngeneration and provides practical solutions for efficient knowledge acquisition\nin LLMs, especially in domains with limited data availability."}
{"id": "2505.00985", "pdf": "https://arxiv.org/pdf/2505.00985", "abs": "https://arxiv.org/abs/2505.00985", "authors": ["Ayan Sengupta", "Yash Goel", "Tanmoy Chakraborty"], "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling", "categories": ["cs.CL"], "comment": null, "summary": "We challenge the dominant focus on neural scaling laws and advocate for a\nparadigm shift toward downscaling in the development of large language models\n(LLMs). While scaling laws have provided critical insights into performance\nimprovements through increasing model and dataset size, we emphasize the\nsignificant limitations of this approach, particularly in terms of\ncomputational inefficiency, environmental impact, and deployment constraints.\nTo address these challenges, we propose a holistic framework for downscaling\nLLMs that seeks to maintain performance while drastically reducing resource\ndemands. This paper outlines practical strategies for transitioning away from\ntraditional scaling paradigms, advocating for a more sustainable, efficient,\nand accessible approach to LLM development."}
{"id": "2505.00989", "pdf": "https://arxiv.org/pdf/2505.00989", "abs": "https://arxiv.org/abs/2505.00989", "authors": ["Sijin Sun", "Liangbin Zhao", "Ming Deng", "Xiuju Fu"], "title": "VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language", "categories": ["cs.CL"], "comment": "8 pages, 5 figures, 7 tablels, submitted to ITSC2025", "summary": "Vessel Traffic Services (VTS) are essential for maritime safety and\nregulatory compliance through real-time traffic management. However, with\nincreasing traffic complexity and the prevalence of heterogeneous, multimodal\ndata, existing VTS systems face limitations in spatiotemporal reasoning and\nintuitive human interaction. In this work, we propose VTS-LLM Agent, the first\ndomain-adaptive large LLM agent tailored for interactive decision support in\nVTS operations. We formalize risk-prone vessel identification as a\nknowledge-augmented Text-to-SQL task, combining structured vessel databases\nwith external maritime knowledge. To support this, we construct a curated\nbenchmark dataset consisting of a custom schema, domain-specific corpus, and a\nquery-SQL test set in multiple linguistic styles. Our framework incorporates\nNER-based relational reasoning, agent-based domain knowledge injection,\nsemantic algebra intermediate representation, and query rethink mechanisms to\nenhance domain grounding and context-aware understanding. Experimental results\nshow that VTS-LLM outperforms both general-purpose and SQL-focused baselines\nunder command-style, operational-style, and formal natural language queries,\nrespectively. Moreover, our analysis provides the first empirical evidence that\nlinguistic style variation introduces systematic performance challenges in\nText-to-SQL modeling. This work lays the foundation for natural language\ninterfaces in vessel traffic services and opens new opportunities for\nproactive, LLM-driven maritime real-time traffic management."}
{"id": "2505.01006", "pdf": "https://arxiv.org/pdf/2505.01006", "abs": "https://arxiv.org/abs/2505.01006", "authors": ["Sumit Mamtani", "Maitreya Sonawane", "Kanika Agarwal", "Nishanth Sanjeev"], "title": "Token-free Models for Sarcasm Detection", "categories": ["cs.CL"], "comment": null, "summary": "Tokenization is a foundational step in most natural language processing (NLP)\npipelines, yet it introduces challenges such as vocabulary mismatch and\nout-of-vocabulary issues. Recent work has shown that models operating directly\non raw text at the byte or character level can mitigate these limitations. In\nthis paper, we evaluate two token-free models, ByT5 and CANINE, on the task of\nsarcasm detection in both social media (Twitter) and non-social media (news\nheadlines) domains. We fine-tune and benchmark these models against token-based\nbaselines and state-of-the-art approaches. Our results show that ByT5-small and\nCANINE outperform token-based counterparts and achieve new state-of-the-art\nperformance, improving accuracy by 0.77% and 0.49% on the News Headlines and\nTwitter Sarcasm datasets, respectively. These findings underscore the potential\nof token-free models for robust NLP in noisy and informal domains such as\nsocial media."}
{"id": "2505.01015", "pdf": "https://arxiv.org/pdf/2505.01015", "abs": "https://arxiv.org/abs/2505.01015", "authors": ["Jongwook Han", "Dongmin Choi", "Woojung Song", "Eun-Ju Lee", "Yohan Jo"], "title": "Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "32 pages, 7 figures", "summary": "The importance of benchmarks for assessing the values of language models has\nbeen pronounced due to the growing need of more authentic, human-aligned\nresponses. However, existing benchmarks rely on human or machine annotations\nthat are vulnerable to value-related biases. Furthermore, the tested scenarios\noften diverge from real-world contexts in which models are commonly used to\ngenerate text and express values. To address these issues, we propose the Value\nPortrait benchmark, a reliable framework for evaluating LLMs' value\norientations with two key characteristics. First, the benchmark consists of\nitems that capture real-life user-LLM interactions, enhancing the relevance of\nassessment results to real-world LLM usage and thus ecological validity.\nSecond, each item is rated by human subjects based on its similarity to their\nown thoughts, and correlations between these ratings and the subjects' actual\nvalue scores are derived. This psychometrically validated approach ensures that\nitems strongly correlated with specific values serve as reliable items for\nassessing those values. Through evaluating 27 LLMs with our benchmark, we find\nthat these models prioritize Benevolence, Security, and Self-Direction values\nwhile placing less emphasis on Tradition, Power, and Achievement values. Also,\nour analysis reveals biases in how LLMs perceive various demographic groups,\ndeviating from real human data."}
{"id": "2505.01035", "pdf": "https://arxiv.org/pdf/2505.01035", "abs": "https://arxiv.org/abs/2505.01035", "authors": ["Lui Yoshida"], "title": "Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?", "categories": ["cs.CL"], "comment": "Accepted in AIED 2025. This preprint has not undergone any\n  post-submission improvements or corrections", "summary": "This study investigates the necessity and impact of a detailed rubric in\nautomated essay scoring (AES) using large language models (LLMs). While using\nrubrics are standard in LLM-based AES, creating detailed rubrics requires\nsubstantial ef-fort and increases token usage. We examined how different levels\nof rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11\ndataset. Our experiments compared three conditions: a full rubric, a simplified\nrubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5\nFlash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of\nfour models maintained similar scoring accuracy with the simplified rubric\ncompared to the detailed one, while significantly reducing token usage.\nHowever, one model (Gemini 1.5 Flash) showed decreased performance with more\ndetailed rubrics. The findings suggest that simplified rubrics may be\nsufficient for most LLM-based AES applications, offering a more efficient\nalternative without compromis-ing scoring accuracy. However, model-specific\nevaluation remains crucial as per-formance patterns vary across different LLMs."}
{"id": "2505.01068", "pdf": "https://arxiv.org/pdf/2505.01068", "abs": "https://arxiv.org/abs/2505.01068", "authors": ["Yijie Jin", "Junjie Peng", "Xuanchao Lin", "Haochen Yuan", "Lan Wang", "Cangzhi Zheng"], "title": "Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Sentiment Analysis (MSA) is a rapidly developing field that\nintegrates multimodal information to recognize sentiments, and existing models\nhave made significant progress in this area. The central challenge in MSA is\nmultimodal fusion, which is predominantly addressed by Multimodal Transformers\n(MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns.\nIn this work, from the perspective of efficiency optimization, we propose and\nprove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and\nwe introduce the graph-structured representation pattern of MulTs. Based on\nthis pattern, we propose an Interlaced Mask (IM) mechanism to design the\nGraph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is\nformally equivalent to MulTs which achieves an efficient weight-sharing\nmechanism without information disorder through IM, enabling All-Modal-In-One\nfusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called\nDecomposition is implemented to ensure avoiding additional computational\noverhead. Moreover, it achieves significantly higher performance than\ntraditional MulTs. To further validate the effectiveness of GsiT itself and the\nHMHG concept, we integrate them into multiple state-of-the-art models and\ndemonstrate notable performance improvements and parameter reduction on widely\nused MSA datasets."}
{"id": "2505.01110", "pdf": "https://arxiv.org/pdf/2505.01110", "abs": "https://arxiv.org/abs/2505.01110", "authors": ["Murtadha Ahmed", "Wenbo", "Liu yunfeng"], "title": "MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nIn-Context Learning (ICL). However, the fixed position length constraints in\npre-trained models limit the number of demonstration examples. Recent efforts\nto extend context suffer from attention dispersion as the number of\ndemonstrations increases. In this paper, we introduce Mitigating Attention\nDispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective\nself-attention as the context size grows. We first split the context into\nmultiple windows, each filled to the model's context capacity, which are\nprocessed separately. Then, we introduce an additional layer to recalibrate the\nattention weights, prioritizing the query tokens as the number of\ndemonstrations increases. Our empirical results show that MateICL can\neffectively leverage larger contexts to improve ICL performance. Compared to\nretrieval-based baselines, MateICL consistently achieves better performance\nwithout requiring an externally trained retrieval model. Despite recent\nadvances in inference strategies (e.g., 32k token contexts), our results\ndemonstrate that MateICL remains beneficial in computationally\nresource-constrained settings. The code is publicly available at\nhttps://github.com/amurtadha/MateICL."}
{"id": "2505.01162", "pdf": "https://arxiv.org/pdf/2505.01162", "abs": "https://arxiv.org/abs/2505.01162", "authors": ["Chebrolu Niranjan", "Kokil Jaidka", "Gerard Christopher Yeo"], "title": "On the Limitations of Steering in Language Model Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Steering vectors are a promising approach to aligning language model behavior\nat inference time. In this paper, we propose a framework to assess the\nlimitations of steering vectors as alignment mechanisms. Using a framework of\ntransformer hook interventions and antonym-based function vectors, we evaluate\nthe role of prompt structure and context complexity in steering effectiveness.\nOur findings indicate that steering vectors are promising for specific\nalignment tasks, such as value alignment, but may not provide a robust\nfoundation for general-purpose alignment in LLMs, particularly in complex\nscenarios. We establish a methodological foundation for future investigations\ninto steering capabilities of reasoning models."}
{"id": "2505.01198", "pdf": "https://arxiv.org/pdf/2505.01198", "abs": "https://arxiv.org/abs/2505.01198", "authors": ["Mahdi Dhaini", "Ege Erdogan", "Nils Feldhus", "Gjergji Kasneci"], "title": "Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT) 2025", "summary": "While research on applications and evaluations of explanation methods\ncontinues to expand, fairness of the explanation methods concerning disparities\nin their performance across subgroups remains an often overlooked aspect. In\nthis paper, we address this gap by showing that, across three tasks and five\nlanguage models, widely used post-hoc feature attribution methods exhibit\nsignificant gender disparity with respect to their faithfulness, robustness,\nand complexity. These disparities persist even when the models are pre-trained\nor fine-tuned on particularly unbiased datasets, indicating that the\ndisparities we observe are not merely consequences of biased training data. Our\nresults highlight the importance of addressing disparities in explanations when\ndeveloping and applying explainability methods, as these can lead to biased\noutcomes against certain subgroups, with particularly critical implications in\nhigh-stakes contexts. Furthermore, our findings underscore the importance of\nincorporating the fairness of explanations, alongside overall model fairness\nand explainability, as a requirement in regulatory frameworks."}
{"id": "2505.01238", "pdf": "https://arxiv.org/pdf/2505.01238", "abs": "https://arxiv.org/abs/2505.01238", "authors": ["Mahdi Dhaini", "Kafaite Zahra Hussain", "Efstratios Zaradoukas", "Gjergji Kasneci"], "title": "EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to the xAI World Conference (2025) - System Demonstration", "summary": "As Natural Language Processing (NLP) models continue to evolve and become\nintegral to high-stakes applications, ensuring their interpretability remains a\ncritical challenge. Given the growing variety of explainability methods and\ndiverse stakeholder requirements, frameworks that help stakeholders select\nappropriate explanations tailored to their specific use cases are increasingly\nimportant. To address this need, we introduce EvalxNLP, a Python framework for\nbenchmarking state-of-the-art feature attribution methods for transformer-based\nNLP models. EvalxNLP integrates eight widely recognized explainability\ntechniques from the Explainable AI (XAI) literature, enabling users to generate\nand evaluate explanations based on key properties such as faithfulness,\nplausibility, and complexity. Our framework also provides interactive,\nLLM-based textual explanations, facilitating user understanding of the\ngenerated explanations and evaluation outcomes. Human evaluation results\nindicate high user satisfaction with EvalxNLP, suggesting it is a promising\nframework for benchmarking explanation methods across diverse user groups. By\noffering a user-friendly and extensible platform, EvalxNLP aims at\ndemocratizing explainability tools and supporting the systematic comparison and\nadvancement of XAI techniques in NLP."}
{"id": "2505.01255", "pdf": "https://arxiv.org/pdf/2505.01255", "abs": "https://arxiv.org/abs/2505.01255", "authors": ["Wei Han", "Hui Chen", "Soujanya Poria"], "title": "PREMISE: Matching-based Prediction for Accurate Review Recommendation", "categories": ["cs.CL", "cs.IR", "cs.MM"], "comment": "19 pages, 16 figures", "summary": "We present PREMISE (PREdict with Matching ScorEs), a new architecture for the\nmatching-based learning in the multimodal fields for the multimodal review\nhelpfulness (MRHP) task. Distinct to previous fusion-based methods which\nobtains multimodal representations via cross-modal attention for downstream\ntasks, PREMISE computes the multi-scale and multi-field representations,\nfilters duplicated semantics, and then obtained a set of matching scores as\nfeature vectors for the downstream recommendation task. This new architecture\nsignificantly boosts the performance for such multimodal tasks whose context\nmatching content are highly correlated to the targets of that task, compared to\nthe state-of-the-art fusion-based methods. Experimental results on two publicly\navailable datasets show that PREMISE achieves promising performance with less\ncomputational cost."}
{"id": "2505.01273", "pdf": "https://arxiv.org/pdf/2505.01273", "abs": "https://arxiv.org/abs/2505.01273", "authors": ["Xuan Li", "Zhe Yin", "Xiaodong Gu", "Beijun Shen"], "title": "Anti-adversarial Learning: Desensitizing Prompts for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the widespread use of LLMs, preserving privacy in user prompts has\nbecome crucial, as prompts risk exposing privacy and sensitive data to the\ncloud LLMs. Traditional techniques like homomorphic encryption, secure\nmulti-party computation, and federated learning face challenges due to heavy\ncomputational costs and user participation requirements, limiting their\napplicability in LLM scenarios. In this paper, we propose PromptObfus, a novel\nmethod for desensitizing LLM prompts. The core idea of PromptObfus is\n\"anti-adversarial\" learning, which perturbs privacy words in the prompt to\nobscure sensitive information while retaining the stability of model\npredictions. Specifically, PromptObfus frames prompt desensitization as a\nmasked language modeling task, replacing privacy-sensitive terms with a [MASK]\ntoken. A desensitization model is trained to generate candidate replacements\nfor each masked position. These candidates are subsequently selected based on\ngradient feedback from a surrogate model, ensuring minimal disruption to the\ntask output. We demonstrate the effectiveness of our approach on three NLP\ntasks. Results show that PromptObfus effectively prevents privacy inference\nfrom remote LLMs while preserving task performance."}
{"id": "2505.01311", "pdf": "https://arxiv.org/pdf/2505.01311", "abs": "https://arxiv.org/abs/2505.01311", "authors": ["Svenja Kenneweg", "Jörg Deigmöller", "Julian Eggert", "Philipp Cimiano"], "title": "A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types", "categories": ["cs.CL"], "comment": "7 pages, 1 figure, to be published in CogSci Proceedings 2025", "summary": "Vague temporal adverbials, such as recently, just, and a long time ago,\ndescribe the temporal distance between a past event and the utterance time but\nleave the exact duration underspecified. In this paper, we introduce a\nfactorized model that captures the semantics of these adverbials as\nprobabilistic distributions. These distributions are composed with\nevent-specific distributions to yield a contextualized meaning for an adverbial\napplied to a specific event. We fit the model's parameters using existing data\ncapturing judgments of native speakers regarding the applicability of these\nvague temporal adverbials to events that took place a given time ago. Comparing\nour approach to a non-factorized model based on a single Gaussian distribution\nfor each pair of event and temporal adverbial, we find that while both models\nhave similar predictive power, our model is preferable in terms of Occam's\nrazor, as it is simpler and has better extendability."}
{"id": "2505.01314", "pdf": "https://arxiv.org/pdf/2505.01314", "abs": "https://arxiv.org/abs/2505.01314", "authors": ["Shang Wang", "Huanrong Tang", "Jianquan Ouyang"], "title": "A Transformer-based Neural Architecture Search Method", "categories": ["cs.CL", "cs.NE"], "comment": "GECCO 2023", "summary": "This paper presents a neural architecture search method based on Transformer\narchitecture, searching cross multihead attention computation ways for\ndifferent number of encoder and decoder combinations. In order to search for\nneural network structures with better translation results, we considered\nperplexity as an auxiliary evaluation metric for the algorithm in addition to\nBLEU scores and iteratively improved each individual neural network within the\npopulation by a multi-objective genetic algorithm. Experimental results show\nthat the neural network structures searched by the algorithm outperform all the\nbaseline models, and that the introduction of the auxiliary evaluation metric\ncan find better models than considering only the BLEU score as an evaluation\nmetric."}
{"id": "2505.01315", "pdf": "https://arxiv.org/pdf/2505.01315", "abs": "https://arxiv.org/abs/2505.01315", "authors": ["Sheikh Samit Muhaimin", "Spyridon Mastorakis"], "title": "Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses."}
{"id": "2505.01325", "pdf": "https://arxiv.org/pdf/2505.01325", "abs": "https://arxiv.org/abs/2505.01325", "authors": ["Svenja Kenneweg", "Jörg Deigmöller", "Philipp Cimiano", "Julian Eggert"], "title": "TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References", "categories": ["cs.CL"], "comment": "24 pages, 6 figures, submitted to Springer Nature Computer Science", "summary": "Understanding and resolving temporal references is essential in Natural\nLanguage Understanding as we often refer to the past or future in daily\ncommunication. Although existing benchmarks address a system's ability to\nreason about and resolve temporal references, systematic evaluation of specific\ntemporal references remains limited. Towards closing this gap, we introduce\nTRAVELER, a novel synthetic benchmark dataset that follows a Question Answering\nparadigm and consists of questions involving temporal references with the\ncorresponding correct answers. TRAVELER assesses models' abilities to resolve\nexplicit, implicit relative to speech time, and vague temporal references.\nBeyond investigating the performance of state-of-the-art LLMs depending on the\ntype of temporal reference, our benchmark also allows evaluation of performance\nin relation to the length of the set of events. For the category of vague\ntemporal references, ground-truth answers were established via human surveys on\nProlific, following a procedure similar to the one from Kenneweg et al. To\ndemonstrate the benchmark's applicability, we evaluate four state-of-the-art\nLLMs using a question-answering task encompassing 3,300 questions. Our findings\nshow that while the benchmarked LLMs can answer questions over event sets with\na handful of events and explicit temporal references successfully, performance\nclearly deteriorates with larger event set length and when temporal references\nget less explicit. Notably, the vague question category exhibits the lowest\nperformance across all models.\n  The benchmark is publicly available at:\nhttps://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER"}
{"id": "2505.00759", "pdf": "https://arxiv.org/pdf/2505.00759", "abs": "https://arxiv.org/abs/2505.00759", "authors": ["Jiahui Chen", "Candace Ross", "Reyhane Askari-Hemmat", "Koustuv Sinha", "Melissa Hall", "Michal Drozdzal", "Adriana Romero-Soriano"], "title": "Multi-Modal Language Models as Text-to-Image Model Evaluators", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The steady improvements of text-to-image (T2I) generative models lead to slow\ndeprecation of automatic evaluation benchmarks that rely on static datasets,\nmotivating researchers to seek alternative ways to evaluate the T2I progress.\nIn this paper, we explore the potential of multi-modal large language models\n(MLLMs) as evaluator agents that interact with a T2I model, with the objective\nof assessing prompt-generation consistency and image aesthetics. We present\nMultimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively\ngenerates prompts for evaluation, scores generated images and matches T2I\nevaluation of existing benchmarks with a fraction of the prompts used in\nexisting static benchmarks. Moreover, we show that MT2IE's prompt-generation\nconsistency scores have higher correlation with human judgment than scores\npreviously introduced in the literature. MT2IE generates prompts that are\nefficient at probing T2I model performance, producing the same relative T2I\nmodel rankings as existing benchmarks while using only 1/80th the number of\nprompts for evaluation."}
{"id": "2505.00808", "pdf": "https://arxiv.org/pdf/2505.00808", "abs": "https://arxiv.org/abs/2505.00808", "authors": ["Kola Ayonrinde", "Louis Jaburi"], "title": "A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "15 pages (plus appendices), 2 figures", "summary": "Mechanistic Interpretability aims to understand neural networks through\ncausal explanations. We argue for the Explanatory View Hypothesis: that\nMechanistic Interpretability research is a principled approach to understanding\nmodels because neural networks contain implicit explanations which can be\nextracted and understood. We hence show that Explanatory Faithfulness, an\nassessment of how well an explanation fits a model, is well-defined. We propose\na definition of Mechanistic Interpretability (MI) as the practice of producing\nModel-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural\nnetworks, allowing us to distinguish MI from other interpretability paradigms\nand detail MI's inherent limits. We formulate the Principle of Explanatory\nOptimism, a conjecture which we argue is a necessary precondition for the\nsuccess of Mechanistic Interpretability."}
{"id": "2505.00831", "pdf": "https://arxiv.org/pdf/2505.00831", "abs": "https://arxiv.org/abs/2505.00831", "authors": ["Quang P. M. Pham", "Khoi T. N. Nguyen", "Nhi H. Doan", "Cuong A. Pham", "Kentaro Inui", "Dezhen Song"], "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation", "categories": ["cs.RO", "cs.CL"], "comment": "Paper is under review", "summary": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics."}
{"id": "2505.00903", "pdf": "https://arxiv.org/pdf/2505.00903", "abs": "https://arxiv.org/abs/2505.00903", "authors": ["Daria Gitman", "Igor Gitman", "Evelina Bakhturina"], "title": "NeMo-Inspector: A Visualization Tool for LLM Generation Analysis", "categories": ["cs.LG", "cs.CL"], "comment": "Presented at the NAACL 2025 conference", "summary": "Adapting Large Language Models (LLMs) to novel tasks and enhancing their\noverall capabilities often requires large, high-quality training datasets.\nSynthetic data, generated at scale, serves a valuable alternative when\nreal-world data is scarce or difficult to obtain. However, ensuring the quality\nof synthetic datasets is challenging, as developers must manually inspect and\nrefine numerous samples to identify errors and areas for improvement. This\nprocess is time-consuming and requires specialized tools. We introduce\nNeMo-Inspector, an open-source tool designed to simplify the analysis of\nsynthetic datasets with integrated inference capabilities. We demonstrate its\neffectiveness through two real-world cases. Analysis and cleaning of the\nsynthetically generated GSM-Plus dataset with NeMo-Inspector led to a\nsignificant decrease in low-quality samples from 46.99% to 19.51%. The tool\nalso helped identify and correct generation errors in OpenMath models,\nimproving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K\ndataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from\nNemotron-4-340B."}
{"id": "2505.00926", "pdf": "https://arxiv.org/pdf/2505.00926", "abs": "https://arxiv.org/abs/2505.00926", "authors": ["Ruiquan Huang", "Yingbin Liang", "Jing Yang"], "title": "How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "accepted by ICML 2025", "summary": "Language recognition tasks are fundamental in natural language processing\n(NLP) and have been widely used to benchmark the performance of large language\nmodels (LLMs). These tasks also play a crucial role in explaining the working\nmechanisms of transformers. In this work, we focus on two representative tasks\nin the category of regular language recognition, known as `even pairs' and\n`parity check', the aim of which is to determine whether the occurrences of\ncertain subsequences in a given sequence are even. Our goal is to explore how a\none-layer transformer, consisting of an attention layer followed by a linear\nlayer, learns to solve these tasks by theoretically analyzing its training\ndynamics under gradient descent. While even pairs can be solved directly by a\none-layer transformer, parity check need to be solved by integrating\nChain-of-Thought (CoT), either into the inference stage of a transformer\nwell-trained for the even pairs task, or into the training of a one-layer\ntransformer. For both problems, our analysis shows that the joint training of\nattention and linear layers exhibits two distinct phases. In the first phase,\nthe attention layer grows rapidly, mapping data sequences into separable\nvectors. In the second phase, the attention layer becomes stable, while the\nlinear layer grows logarithmically and approaches in direction to a max-margin\nhyperplane that correctly separates the attention layer outputs into positive\nand negative samples, and the loss decreases at a rate of $O(1/t)$. Our\nexperiments validate those theoretical results."}
{"id": "2505.00976", "pdf": "https://arxiv.org/pdf/2505.00976", "abs": "https://arxiv.org/abs/2505.00976", "authors": ["Zhiyu Liao", "Kang Chen", "Yuanguo Lin", "Kangkang Li", "Yunxuan Liu", "Hefeng Chen", "Xingwang Huang", "Yuanhui Yu"], "title": "Attack and defense techniques in large language models: A survey and new perspectives", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have become central to numerous natural language\nprocessing tasks, but their vulnerabilities present significant security and\nethical challenges. This systematic survey explores the evolving landscape of\nattack and defense techniques in LLMs. We classify attacks into adversarial\nprompt attack, optimized attacks, model theft, as well as attacks on\napplication of LLMs, detailing their mechanisms and implications. Consequently,\nwe analyze defense strategies, including prevention-based and detection-based\ndefense methods. Although advances have been made, challenges remain to adapt\nto the dynamic threat landscape, balance usability with robustness, and address\nresource constraints in defense implementation. We highlight open problems,\nincluding the need for adaptive scalable defenses, explainable security\ntechniques, and standardized evaluation frameworks. This survey provides\nactionable insights and directions for developing secure and resilient LLMs,\nemphasizing the importance of interdisciplinary collaboration and ethical\nconsiderations to mitigate risks in real-world applications."}
{"id": "2505.01007", "pdf": "https://arxiv.org/pdf/2505.01007", "abs": "https://arxiv.org/abs/2505.01007", "authors": ["Ling Tang", "Yuefeng Chen", "Hui Xue", "Quanshi Zhang"], "title": "Towards the Resistance of Neural Network Watermarking to Fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper proves a new watermarking method to embed the ownership\ninformation into a deep neural network (DNN), which is robust to fine-tuning.\nSpecifically, we prove that when the input feature of a convolutional layer\nonly contains low-frequency components, specific frequency components of the\nconvolutional filter will not be changed by gradient descent during the\nfine-tuning process, where we propose a revised Fourier transform to extract\nfrequency components from the convolutional filter. Additionally, we also prove\nthat these frequency components are equivariant to weight scaling and weight\npermutations. In this way, we design a watermark module to encode the watermark\ninformation to specific frequency components in a convolutional filter.\nPreliminary experiments demonstrate the effectiveness of our method."}
{"id": "2505.01096", "pdf": "https://arxiv.org/pdf/2505.01096", "abs": "https://arxiv.org/abs/2505.01096", "authors": ["Marco Salmè", "Rosa Sicilia", "Paolo Soda", "Valerio Guarrasi"], "title": "Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The integration of artificial intelligence in healthcare has opened new\nhorizons for improving medical diagnostics and patient care. However,\nchallenges persist in developing systems capable of generating accurate and\ncontextually relevant radiology reports, particularly in low-resource\nlanguages. In this study, we present a comprehensive benchmark to evaluate the\nperformance of instruction-tuned Vision-Language Models (VLMs) in the\nspecialized task of radiology report generation across three low-resource\nlanguages: Italian, German, and Spanish. Employing the LLaVA architectural\nframework, we conducted a systematic evaluation of pre-trained models utilizing\ngeneral datasets, domain-specific datasets, and low-resource language-specific\ndatasets. In light of the unavailability of models that possess prior knowledge\nof both the medical domain and low-resource languages, we analyzed various\nadaptations to determine the most effective approach for these contexts. The\nresults revealed that language-specific models substantially outperformed both\ngeneral and domain-specific models in generating radiology reports, emphasizing\nthe critical role of linguistic adaptation. Additionally, models fine-tuned\nwith medical terminology exhibited enhanced performance across all languages\ncompared to models with generic knowledge, highlighting the importance of\ndomain-specific training. We also explored the influence of the temperature\nparameter on the coherence of report generation, providing insights for optimal\nmodel settings. Our findings highlight the importance of tailored language and\ndomain-specific training for improving the quality and accuracy of radiological\nreports in multilingual settings. This research not only advances our\nunderstanding of VLMs adaptability in healthcare but also points to significant\navenues for future investigations into model tuning and language-specific\nadaptations."}
{"id": "2505.01372", "pdf": "https://arxiv.org/pdf/2505.01372", "abs": "https://arxiv.org/abs/2505.01372", "authors": ["Kola Ayonrinde", "Louis Jaburi"], "title": "Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "comment": "13 pages (plus appendices), 5 figures", "summary": "Mechanistic Interpretability (MI) aims to understand neural networks through\ncausal explanations. Though MI has many explanation-generating methods,\nprogress has been limited by the lack of a universal approach to evaluating\nexplanations. Here we analyse the fundamental question \"What makes a good\nexplanation?\" We introduce a pluralist Explanatory Virtues Framework drawing on\nfour perspectives from the Philosophy of Science - the Bayesian, Kuhnian,\nDeutschian, and Nomological - to systematically evaluate and improve\nexplanations in MI. We find that Compact Proofs consider many explanatory\nvirtues and are hence a promising approach. Fruitful research directions\nimplied by our framework include (1) clearly defining explanatory simplicity,\n(2) focusing on unifying explanations and (3) deriving universal principles for\nneural networks. Improved MI methods enhance our ability to monitor, predict,\nand steer AI systems."}
