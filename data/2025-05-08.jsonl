{"id": "2505.03788", "pdf": "https://arxiv.org/pdf/2505.03788", "abs": "https://arxiv.org/abs/2505.03788", "authors": ["Trilok Padhi", "Ramneet Kaur", "Adam D. Cobb", "Manoj Acharya", "Anirban Roy", "Colin Samplawski", "Brian Matejek", "Alexander M. Berenbeim", "Nathaniel D. Bastian", "Susmit Jha"], "title": "Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce a novel approach for calibrating uncertainty quantification (UQ)\ntailored for multi-modal large language models (LLMs). Existing\nstate-of-the-art UQ methods rely on consistency among multiple responses\ngenerated by the LLM on an input query under diverse settings. However, these\napproaches often report higher confidence in scenarios where the LLM is\nconsistently incorrect. This leads to a poorly calibrated confidence with\nrespect to accuracy. To address this, we leverage cross-modal consistency in\naddition to self-consistency to improve the calibration of the multi-modal\nmodels. Specifically, we ground the textual responses to the visual inputs. The\nconfidence from the grounding model is used to calibrate the overall\nconfidence. Given that using a grounding model adds its own uncertainty in the\npipeline, we apply temperature scaling - a widely accepted parametric\ncalibration technique - to calibrate the grounding model's confidence in the\naccuracy of generated responses. We evaluate the proposed approach across\nmultiple multi-modal tasks, such as medical question answering (Slake) and\nvisual question answering (VQAv2), considering multi-modal models such as\nLLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework\nachieves significantly improved calibration on both tasks."}
{"id": "2505.03910", "pdf": "https://arxiv.org/pdf/2505.03910", "abs": "https://arxiv.org/abs/2505.03910", "authors": ["Gianluca Manzo", "Julia Ive"], "title": "Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty", "categories": ["cs.CL"], "comment": null, "summary": "Automating chest radiograph interpretation using Deep Learning (DL) models\nhas the potential to significantly improve clinical workflows, decision-making,\nand large-scale health screening. However, in medical settings, merely\noptimising predictive performance is insufficient, as the quantification of\nuncertainty is equally crucial. This paper investigates the relationship\nbetween predictive uncertainty, derived from Bayesian Deep Learning\napproximations, and human/linguistic uncertainty, as estimated from free-text\nradiology reports labelled by rule-based labellers. Utilising BERT as the model\nof choice, this study evaluates different binarisation methods for uncertainty\nlabels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in\nestimating predictive uncertainty. The results demonstrate good model\nperformance, but also a modest correlation between predictive and linguistic\nuncertainty, highlighting the challenges in aligning machine uncertainty with\nhuman interpretation nuances. Our findings suggest that while Bayesian\napproximations provide valuable uncertainty estimates, further refinement is\nnecessary to fully capture and utilise the subtleties of human uncertainty in\nclinical applications."}
{"id": "2505.03970", "pdf": "https://arxiv.org/pdf/2505.03970", "abs": "https://arxiv.org/abs/2505.03970", "authors": ["Lucia Zheng", "Neel Guha", "Javokhir Arifov", "Sarah Zhang", "Michal Skreta", "Christopher D. Manning", "Peter Henderson", "Daniel E. Ho"], "title": "A Reasoning-Focused Legal Retrieval Benchmark", "categories": ["cs.CL"], "comment": "CS&Law 2025. For data, see\n  https://reglab.github.io/legal-rag-benchmarks/", "summary": "As the legal community increasingly examines the use of large language models\n(LLMs) for various legal applications, legal AI developers have turned to\nretrieval-augmented LLMs (\"RAG\" systems) to improve system performance and\nrobustness. An obstacle to the development of specialized RAG systems is the\nlack of realistic legal RAG benchmarks which capture the complexity of both\nlegal retrieval and downstream legal question-answering. To address this, we\nintroduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA.\nOur tasks correspond to real-world legal research tasks, and were produced\nthrough annotation processes which resemble legal research. We describe the\nconstruction of these benchmarks and the performance of existing retriever\npipelines. Our results suggest that legal RAG remains a challenging\napplication, thus motivating future research."}
{"id": "2505.03973", "pdf": "https://arxiv.org/pdf/2505.03973", "abs": "https://arxiv.org/abs/2505.03973", "authors": ["Jiale Liu", "Yifan Zeng", "Shaokun Zhang", "Chi Zhang", "Malte HÃ¸jmark-Bertelsen", "Marie Normann Gadeberg", "Huazheng Wang", "Qingyun Wu"], "title": "Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale", "categories": ["cs.CL"], "comment": null, "summary": "LLM-based optimization has shown remarkable potential in enhancing agentic\nsystems. However, the conventional approach of prompting LLM optimizer with the\nwhole training trajectories on training dataset in a single pass becomes\nuntenable as datasets grow, leading to context window overflow and degraded\npattern recognition. To address these challenges, we propose Fine-Grained\nOptimization (FGO), a scalable framework that divides large optimization tasks\ninto manageable subsets, performs targeted optimizations, and systematically\ncombines optimized components through progressive merging. Evaluation across\nALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms\nexisting approaches by 1.6-8.6% while reducing average prompt token consumption\nby 56.3%. Our framework provides a practical solution for scaling up LLM-based\noptimization of increasingly sophisticated agent systems. Further analysis\ndemonstrates that FGO achieves the most consistent performance gain in all\ntraining dataset sizes, showcasing its scalability and efficiency."}
{"id": "2505.03981", "pdf": "https://arxiv.org/pdf/2505.03981", "abs": "https://arxiv.org/abs/2505.03981", "authors": ["Qianchu Liu", "Sheng Zhang", "Guanghui Qin", "Timothy Ossowski", "Yu Gu", "Ying Jin", "Sid Kiblawi", "Sam Preston", "Mu Wei", "Paul Vozila", "Tristan Naumann", "Hoifung Poon"], "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks."}
{"id": "2505.04016", "pdf": "https://arxiv.org/pdf/2505.04016", "abs": "https://arxiv.org/abs/2505.04016", "authors": ["Darren Yow-Bang Wang", "Zhengyuan Shen", "Soumya Smruti Mishra", "Zhichao Xu", "Yifei Teng", "Haibo Ding"], "title": "SLOT: Structuring the Output of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Structured outputs are essential for large language models (LLMs) in critical\napplications like agents and information extraction. Despite their\ncapabilities, LLMs often generate outputs that deviate from predefined schemas,\nsignificantly hampering reliable application development. We present SLOT\n(Structured LLM Output Transformer), a model-agnostic approach that transforms\nunstructured LLM outputs into precise structured formats. While existing\nsolutions predominantly rely on constrained decoding techniques or are tightly\ncoupled with specific models, SLOT employs a fine-tuned lightweight language\nmodel as a post-processing layer, achieving flexibility across various LLMs and\nschema specifications. We introduce a systematic pipeline for data curation and\nsynthesis alongside a formal evaluation methodology that quantifies both schema\naccuracy and content fidelity. Our results demonstrate that fine-tuned\nMistral-7B model with constrained decoding achieves near perfect schema\naccuracy (99.5%) and content similarity (94.0%), outperforming\nClaude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,\nrespectively). Notably, even compact models like Llama-3.2-1B can match or\nexceed the structured output capabilities of much larger proprietary models\nwhen equipped with SLOT, enabling reliable structured generation in\nresource-constrained environments."}
{"id": "2505.04072", "pdf": "https://arxiv.org/pdf/2505.04072", "abs": "https://arxiv.org/abs/2505.04072", "authors": ["Xu Huang", "Yuefeng Huang", "Weiwen Liu", "Xingshan Zeng", "Yasheng Wang", "Ruiming Tang", "Hong Xie", "Defu Lian"], "title": "Advancing and Benchmarking Personalized Tool Invocation for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 7 figures, 5 tables", "summary": "Tool invocation is a crucial mechanism for extending the capabilities of\nLarge Language Models (LLMs) and has recently garnered significant attention.\nIt enables LLMs to solve complex problems through tool calls while accessing\nup-to-date world knowledge. However, existing work primarily focuses on the\nfundamental ability of LLMs to invoke tools for problem-solving, without\nconsidering personalized constraints in tool invocation. In this work, we\nintroduce the concept of Personalized Tool Invocation and define two key tasks:\nTool Preference and Profile-dependent Query. Tool Preference addresses user\npreferences when selecting among functionally similar tools, while\nProfile-dependent Query considers cases where a user query lacks certain tool\nparameters, requiring the model to infer them from the user profile. To tackle\nthese challenges, we propose PTool, a data synthesis framework designed for\npersonalized tool invocation. Additionally, we construct \\textbf{PTBench}, the\nfirst benchmark for evaluating personalized tool invocation. We then fine-tune\nvarious open-source models, demonstrating the effectiveness of our framework\nand providing valuable insights. Our benchmark is public at\nhttps://github.com/hyfshadow/PTBench."}
{"id": "2505.04073", "pdf": "https://arxiv.org/pdf/2505.04073", "abs": "https://arxiv.org/abs/2505.04073", "authors": ["Mengxian Lyu", "Xiaohan Li", "Ziyi Chen", "Jinqian Pan", "Cheng Peng", "Sankalp Talankar", "Yonghui Wu"], "title": "Natural Language Generation in Healthcare: A Review of Methods and Applications", "categories": ["cs.CL"], "comment": null, "summary": "Natural language generation (NLG) is the key technology to achieve generative\nartificial intelligence (AI). With the breakthroughs in large language models\n(LLMs), NLG has been widely used in various medical applications, demonstrating\nthe potential to enhance clinical workflows, support clinical decision-making,\nand improve clinical documentation. Heterogeneous and diverse medical data\nmodalities, such as medical text, images, and knowledge bases, are utilized in\nNLG. Researchers have proposed many generative models and applied them in a\nnumber of healthcare applications. There is a need for a comprehensive review\nof NLG methods and applications in the medical domain. In this study, we\nsystematically reviewed 113 scientific publications from a total of 3,988\nNLG-related articles identified using a literature search, focusing on data\nmodality, model architecture, clinical applications, and evaluation methods.\nFollowing PRISMA (Preferred Reporting Items for Systematic reviews and\nMeta-Analyses) guidelines, we categorize key methods, identify clinical\napplications, and assess their capabilities, limitations, and emerging\nchallenges. This timely review covers the key NLG technologies and medical\napplications and provides valuable insights for future studies to leverage NLG\nto transform medical discovery and healthcare."}
{"id": "2505.04132", "pdf": "https://arxiv.org/pdf/2505.04132", "abs": "https://arxiv.org/abs/2505.04132", "authors": ["Mingruo Yuan", "Ben Kao", "Tien-Hsuan Wu", "Michael M. K. Cheung", "Henry W. H. Chan", "Anne S. Y. Cheung", "Felix W. H. Chan", "Yongxi Chen"], "title": "Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Access to legal information is fundamental to access to justice. Yet\naccessibility refers not only to making legal documents available to the\npublic, but also rendering legal information comprehensible to them. A vexing\nproblem in bringing legal information to the public is how to turn formal legal\ndocuments such as legislation and judgments, which are often highly technical,\nto easily navigable and comprehensible knowledge to those without legal\neducation. In this study, we formulate a three-step approach for bringing legal\nknowledge to laypersons, tackling the issues of navigability and\ncomprehensibility. First, we translate selected sections of the law into\nsnippets (called CLIC-pages), each being a small piece of article that focuses\non explaining certain technical legal concept in layperson's terms. Second, we\nconstruct a Legal Question Bank (LQB), which is a collection of legal questions\nwhose answers can be found in the CLIC-pages. Third, we design an interactive\nCLIC Recommender (CRec). Given a user's verbal description of a legal situation\nthat requires a legal solution, CRec interprets the user's input and shortlists\nquestions from the question bank that are most likely relevant to the given\nlegal situation and recommends their corresponding CLIC pages where relevant\nlegal knowledge can be found. In this paper we focus on the technical aspects\nof creating an LQB. We show how large-scale pre-trained language models, such\nas GPT-3, can be used to generate legal questions. We compare machine-generated\nquestions (MGQs) against human-composed questions (HCQs) and find that MGQs are\nmore scalable, cost-effective, and more diversified, while HCQs are more\nprecise. We also show a prototype of CRec and illustrate through an example how\nour 3-step approach effectively brings relevant legal knowledge to the public."}
{"id": "2505.04135", "pdf": "https://arxiv.org/pdf/2505.04135", "abs": "https://arxiv.org/abs/2505.04135", "authors": ["Vihaan Miriyala", "Smrithi Bukkapatnam", "Lavanya Prahallad"], "title": "Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "5 pages", "summary": "We explore the use of Chain-of-Thought (CoT) prompting with large language\nmodels (LLMs) to improve the accuracy of granular sentiment categorization in\napp store reviews. Traditional numeric and polarity-based ratings often fail to\ncapture the nuanced sentiment embedded in user feedback. We evaluated the\neffectiveness of CoT prompting versus simple prompting on 2000 Amazon app\nreviews by comparing each method's predictions to human judgements. CoT\nprompting improved classification accuracy from 84% to 93% highlighting the\nbenefit of explicit reasoning in enhancing sentiment analysis performance."}
{"id": "2505.04146", "pdf": "https://arxiv.org/pdf/2505.04146", "abs": "https://arxiv.org/abs/2505.04146", "authors": ["Variath Madhupal Gautham Nair", "Vishal Varma Dantuluri"], "title": "Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Existing large language models (LLMs) are advancing rapidly and produce\noutstanding results in image generation tasks, yet their content safety checks\nremain vulnerable to prompt-based jailbreaks. Through preliminary testing on\nplatforms such as ChatGPT, MetaAI, and Grok, we observed that even short,\nnatural prompts could lead to the generation of compromising images ranging\nfrom realistic depictions of forged documents to manipulated images of public\nfigures.\n  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and\nscalable benchmark dataset to evaluate LLM vulnerability in image generation.\nOur methodology combines structured prompt engineering, multilingual\nobfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted\nLLaMA-3. The pipeline supports both zero-shot and fallback prompting\nstrategies, risk scoring, and automated tagging. All generations are stored\nwith rich metadata and curated into Bronze (non-verified), Silver (LLM-aided\nverification), and Gold (manually verified) tiers. UTCB is designed to evolve\nover time with new data sources, prompt templates, and model behaviors.\n  Warning: This paper includes visual examples of adversarial inputs designed\nto test model safety. All outputs have been redacted to ensure responsible\ndisclosure."}
{"id": "2505.04152", "pdf": "https://arxiv.org/pdf/2505.04152", "abs": "https://arxiv.org/abs/2505.04152", "authors": ["Manas Satish Bedmutha", "Feng Chen", "Andrea Hartzler", "Trevor Cohen", "Nadir Weibel"], "title": "Can Language Models Understand Social Behavior in Clinical Conversations?", "categories": ["cs.CL", "cs.CY", "cs.HC", "H.5.2; H.1.2; I.2.7; I.2.m; J.3"], "comment": null, "summary": "Effective communication between providers and their patients influences\nhealth and care outcomes. The effectiveness of such conversations has been\nlinked not only to the exchange of clinical information, but also to a range of\ninterpersonal behaviors; commonly referred to as social signals, which are\noften conveyed through non-verbal cues and shape the quality of the\npatient-provider relationship. Recent advances in large language models (LLMs)\nhave demonstrated an increasing ability to infer emotional and social behaviors\neven when analyzing only textual information. As automation increases also in\nclinical settings, such as for transcription of patient-provider conversations,\nthere is growing potential for LLMs to automatically analyze and extract social\nbehaviors from these interactions. To explore the foundational capabilities of\nLLMs in tracking social signals in clinical dialogue, we designed task-specific\nprompts and evaluated model performance across multiple architectures and\nprompting styles using a highly imbalanced, annotated dataset spanning 20\ndistinct social signals such as provider dominance, patient warmth, etc. We\npresent the first system capable of tracking all these 20 coded signals, and\nuncover patterns in LLM behavior. Further analysis of model configurations and\nclinical context provides insights for enhancing LLM performance on social\nsignal processing tasks in healthcare settings."}
{"id": "2505.04253", "pdf": "https://arxiv.org/pdf/2505.04253", "abs": "https://arxiv.org/abs/2505.04253", "authors": ["Maria Marina", "Nikolay Ivanov", "Sergey Pletenev", "Mikhail Salnikov", "Daria Galimzianova", "Nikita Krayko", "Vasily Konovalov", "Alexander Panchenko", "Viktor Moskvoretskii"], "title": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself", "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 5 figures, 2 tables", "summary": "Large Language Models~(LLMs) are prone to hallucinations, and\nRetrieval-Augmented Generation (RAG) helps mitigate this, but at a high\ncomputational cost while risking misinformation. Adaptive retrieval aims to\nretrieve only when necessary, but existing approaches rely on LLM-based\nuncertainty estimation, which remain inefficient and impractical. In this\nstudy, we introduce lightweight LLM-independent adaptive retrieval methods\nbased on external information. We investigated 27 features, organized into 7\ngroups, and their hybrid combinations. We evaluated these methods on 6 QA\ndatasets, assessing the QA performance and efficiency. The results show that\nour approach matches the performance of complex LLM-based methods while\nachieving significant efficiency gains, demonstrating the potential of external\ninformation for adaptive retrieval."}
{"id": "2505.04284", "pdf": "https://arxiv.org/pdf/2505.04284", "abs": "https://arxiv.org/abs/2505.04284", "authors": ["Sofia Jamil", "Aryan Dabad", "Bollampalli Areen Reddy", "Sriparna Saha", "Rajiv Misra", "Adil A. Shakur"], "title": "GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the realm of cancer treatment, summarizing adverse drug events (ADEs)\nreported by patients using prescribed drugs is crucial for enhancing\npharmacovigilance practices and improving drug-related decision-making. While\nthe volume and complexity of pharmacovigilance data have increased, existing\nresearch in this field has predominantly focused on general diseases rather\nthan specifically addressing cancer. This work introduces the task of grouped\nsummarization of adverse drug events reported by multiple patients using the\nsame drug for cancer treatment. To address the challenge of limited resources\nin cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug\nReaction and Summarization (MCADRS) dataset. This dataset includes\npharmacovigilance posts detailing patient concerns regarding drug efficacy and\nadverse effects, along with extracted labels for drug names, adverse drug\nevents, severity, and adversity of reactions, as well as summaries of ADEs for\neach drug. Additionally, we propose the Grouping and Abstractive Summarization\nof Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that\ncombines the information extraction capabilities of Large Language Models\n(LLMs) with the summarization power of the encoder-decoder T5 model. Our work\nis the first to apply alignment techniques, including advanced algorithms like\nDirect Preference Optimization, to encoder-decoder models using synthetic\ndatasets for summarization tasks. Through extensive experiments, we demonstrate\nthe superior performance of GASCADE across various metrics, validated through\nboth automated assessments and human evaluations. This multitasking approach\nenhances drug-related decision-making and fosters a deeper understanding of\npatient concerns, paving the way for advancements in personalized and\nresponsive cancer care. The code and dataset used in this work are publicly\navailable."}
{"id": "2505.04388", "pdf": "https://arxiv.org/pdf/2505.04388", "abs": "https://arxiv.org/abs/2505.04388", "authors": ["Dario Garcia-Gasulla", "Jordi Bayarri-Planas", "Ashwin Kumar Gururajan", "Enrique Lopez-Cuena", "Adrian Tormos", "Daniel Hinjos", "Pablo Bernabeu-Perez", "Anna Arias-Duart", "Pablo Agustin Martin-Torres", "Marta Gonzalez-Mallo", "Sergio Alvarez-Napagao", "Eduard AyguadÃ©-Parra", "Ulises CortÃ©s"], "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2405.01886", "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare."}
{"id": "2505.04393", "pdf": "https://arxiv.org/pdf/2505.04393", "abs": "https://arxiv.org/abs/2505.04393", "authors": ["David Exler", "Mark Schutera", "Markus Reischl", "Luca Rettenberger"], "title": "Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters", "categories": ["cs.CL"], "comment": null, "summary": "With the increasing prevalence of artificial intelligence, careful evaluation\nof inherent biases needs to be conducted to form the basis for alleviating the\neffects these predispositions can have on users. Large language models (LLMs)\nare predominantly used by many as a primary source of information for various\ntopics. LLMs frequently make factual errors, fabricate data (hallucinations),\nor present biases, exposing users to misinformation and influencing opinions.\nEducating users on their risks is key to responsible use, as bias, unlike\nhallucinations, cannot be caught through data verification. We quantify the\npolitical bias of popular LLMs in the context of the recent vote of the German\nBundestag using the score produced by the Wahl-O-Mat. This metric measures the\nalignment between an individual's political views and the positions of German\npolitical parties. We compare the models' alignment scores to identify factors\ninfluencing their political preferences. Doing so, we discover a bias toward\nleft-leaning parties, most dominant in larger LLMs. Also, we find that the\nlanguage we use to communicate with the models affects their political views.\nAdditionally, we analyze the influence of a model's origin and release date and\ncompare the results to the outcome of the recent vote of the Bundestag. Our\nresults imply that LLMs are prone to exhibiting political bias. Large\ncorporations with the necessary means to develop LLMs, thus, knowingly or\nunknowingly, have a responsibility to contain these biases, as they can\ninfluence each voter's decision-making process and inform public opinion in\ngeneral and at scale."}
{"id": "2505.04406", "pdf": "https://arxiv.org/pdf/2505.04406", "abs": "https://arxiv.org/abs/2505.04406", "authors": ["Aidar Valeev", "Roman Garaev", "Vadim Lomshakov", "Irina Piontkovskaya", "Vladimir Ivanov", "Israel Adewuyi"], "title": "YABLoCo: Yet Another Benchmark for Long Context Code Generation", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "Presented at LLM4Code 2025 Workshop co-located wtih ICSE 2025", "summary": "Large Language Models demonstrate the ability to solve various programming\ntasks, including code generation. Typically, the performance of LLMs is\nmeasured on benchmarks with small or medium-sized context windows of thousands\nof lines of code. At the same time, in real-world software projects,\nrepositories can span up to millions of LoC. This paper closes this gap by\ncontributing to the long context code generation benchmark (YABLoCo). The\nbenchmark featured a test set of 215 functions selected from four large\nrepositories with thousands of functions. The dataset contained metadata of\nfunctions, contexts of the functions with different levels of dependencies,\ndocstrings, functions bodies, and call graphs for each repository. This paper\npresents three key aspects of the contribution. First, the benchmark aims at\nfunction body generation in large repositories in C and C++, two languages not\ncovered by previous benchmarks. Second, the benchmark contains large\nrepositories from 200K to 2,000K LoC. Third, we contribute a scalable\nevaluation pipeline for efficient computing of the target metrics and a tool\nfor visual analysis of generated code. Overall, these three aspects allow for\nevaluating code generation in large repositories in C and C++."}
{"id": "2505.04416", "pdf": "https://arxiv.org/pdf/2505.04416", "abs": "https://arxiv.org/abs/2505.04416", "authors": ["Xiaoyu Xu", "Minxin Du", "Qingqing Ye", "Haibo Hu"], "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "18 pages, 2 figures", "summary": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios."}
{"id": "2505.04507", "pdf": "https://arxiv.org/pdf/2505.04507", "abs": "https://arxiv.org/abs/2505.04507", "authors": ["Ilya Koziev"], "title": "Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts", "categories": ["cs.CL"], "comment": null, "summary": "The quality of natural language texts in fine-tuning datasets plays a\ncritical role in the performance of generative models, particularly in\ncomputational creativity tasks such as poem or song lyric generation. Fluency\ndefects in generated poems significantly reduce their value. However, training\ntexts are often sourced from internet-based platforms without stringent quality\ncontrol, posing a challenge for data engineers to manage defect levels\neffectively.\n  To address this issue, we propose the use of automated linguistic anomaly\ndetection to identify and filter out low-quality texts from training datasets\nfor creative models. In this paper, we present a comprehensive comparison of\nunsupervised and supervised text anomaly detection approaches, utilizing both\nsynthetic and human-labeled datasets. We also introduce the RUPOR dataset, a\ncollection of Russian-language human-labeled poems designed for cross-sentence\ngrammatical error detection, and provide the full evaluation code. Our work\naims to empower the community with tools and insights to improve the quality of\ntraining datasets for generative models in creative domains."}
{"id": "2505.04519", "pdf": "https://arxiv.org/pdf/2505.04519", "abs": "https://arxiv.org/abs/2505.04519", "authors": ["Yehui Tang", "Yichun Yin", "Yaoyuan Wang", "Hang Zhou", "Yu Pan", "Wei Guo", "Ziyang Zhang", "Miao Rang", "Fangcheng Liu", "Naifu Zhang", "Binghan Li", "Yonghan Dong", "Xiaojun Meng", "Yasheng Wang", "Dong Li", "Yin Li", "Dandan Tu", "Can Chen", "Youliang Yan", "Fisher Yu", "Ruiming Tang", "Yunhe Wang", "Botian Huang", "Bo Wang", "Boxiao Liu", "Changzheng Zhang", "Da Kuang", "Fei Liu", "Gang Huang", "Jiansheng Wei", "Jiarui Qin", "Jie Ran", "Jinpeng Li", "Jun Zhao", "Liang Dai", "Lin Li", "Liqun Deng", "Peifeng Qin", "Pengyuan Zeng", "Qiang Gu", "Shaohua Tang", "Shengjun Cheng", "Tao Gao", "Tao Yu", "Tianshu Li", "Tianyu Bi", "Wei He", "Weikai Mao", "Wenyong Huang", "Wulong Liu", "Xiabing Li", "Xianzhi Yu", "Xueyu Wu", "Xu He", "Yangkai Du", "Yan Xu", "Ye Tian", "Yimeng Wu", "Yongbing Huang", "Yong Tian", "Yong Zhu", "Yue Li", "Yufei Wang", "Yuhang Gai", "Yujun Li", "Yu Luo", "Yunsheng Ni", "Yusen Sun", "Zelin Chen", "Zhe Liu", "Zhicheng Liu", "Zhipeng Tu", "Zilin Ding", "Zongyuan Zhan"], "title": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs", "categories": ["cs.CL"], "comment": null, "summary": "Sparse large language models (LLMs) with Mixture of Experts (MoE) and close\nto a trillion parameters are dominating the realm of most capable language\nmodels. However, the massive model scale poses significant challenges for the\nunderlying software and hardware systems. In this paper, we aim to uncover a\nrecipe to harness such scale on Ascend NPUs. The key goals are better usage of\nthe computing resources under the dynamic sparse model structures and\nmaterializing the expected performance gain on the actual hardware. To select\nmodel configurations suitable for Ascend NPUs without repeatedly running the\nexpensive experiments, we leverage simulation to compare the trade-off of\nvarious model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM\nwith 718 billion parameters, and we conducted experiments on the model to\nverify the simulation results. On the system side, we dig into Expert\nParallelism to optimize the communication between NPU devices to reduce the\nsynchronization overhead. We also optimize the memory efficiency within the\ndevices to further reduce the parameter and activation management overhead. In\nthe end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with\nperformance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and\ndemonstrate that the Ascend system is capable of harnessing all the training\nstages of the state-of-the-art language models. Extensive experiments indicate\nthat our recipe can lead to efficient training of large-scale sparse language\nmodels with MoE. We also study the behaviors of such models for future\nreference."}
{"id": "2505.04531", "pdf": "https://arxiv.org/pdf/2505.04531", "abs": "https://arxiv.org/abs/2505.04531", "authors": ["Josh McGiff", "Nikola S. Nikolov"], "title": "Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review", "categories": ["cs.CL", "cs.AI"], "comment": "This work is currently under review. Please do not cite without\n  permission", "summary": "Generative language modelling has surged in popularity with the emergence of\nservices such as ChatGPT and Google Gemini. While these models have\ndemonstrated transformative potential in productivity and communication, they\noverwhelmingly cater to high-resource languages like English. This has\namplified concerns over linguistic inequality in natural language processing\n(NLP). This paper presents the first systematic review focused specifically on\nstrategies to address data scarcity in generative language modelling for\nlow-resource languages (LRL). Drawing from 54 studies, we identify, categorise\nand evaluate technical approaches, including monolingual data augmentation,\nback-translation, multilingual training, and prompt engineering, across\ngenerative tasks. We also analyse trends in architecture choices, language\nfamily representation, and evaluation methods. Our findings highlight a strong\nreliance on transformer-based models, a concentration on a small subset of\nLRLs, and a lack of consistent evaluation across studies. We conclude with\nrecommendations for extending these methods to a wider range of LRLs and\noutline open challenges in building equitable generative language systems.\nUltimately, this review aims to support researchers and developers in building\ninclusive AI tools for underrepresented languages, a necessary step toward\nempowering LRL speakers and the preservation of linguistic diversity in a world\nincreasingly shaped by large-scale language technologies."}
{"id": "2505.04588", "pdf": "https://arxiv.org/pdf/2505.04588", "abs": "https://arxiv.org/abs/2505.04588", "authors": ["Hao Sun", "Zile Qiao", "Jiayan Guo", "Xuanbo Fan", "Yingyan Hou", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Yan Zhang"], "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching", "categories": ["cs.CL"], "comment": null, "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms."}
{"id": "2505.03786", "pdf": "https://arxiv.org/pdf/2505.03786", "abs": "https://arxiv.org/abs/2505.03786", "authors": ["Md Fahim Anjum"], "title": "When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator", "categories": ["cs.LG", "cs.CL"], "comment": "12 pages, 5 figures. Code available at:\n  https://github.com/MDFahimAnjum/llm-planning-with-reasoning", "summary": "Large Language Models (LLM) with reasoning capabilities offer a promising\npath for improving candidate evaluation in planning frameworks, but their\nrelative performance against traditional non-reasoning models remains largely\nunderexplored. In this study, we benchmark a distilled 1.5B parameter reasoning\nmodel (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within\na generator-discriminator LLM planning framework for the text-to-SQL task. For\nthis, we introduce a novel method for extracting soft scores from the\nchain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking\nof candidates. Our central hypothesis is that reasoning models are more\neffective discriminators than non-reasoning LLMs. Our results show that\ndistilled DeepSeek-R1-1.5B achieves up to $87\\%$ higher F1 and $3.7\\%$ better\ndiscrimination accuracy than CodeLlama-7B, as well as $3.7\\%$ higher execution\naccuracy than CodeLlama-13B, despite having significantly fewer parameters.\nFurthermore, we find that there is a limit to the logical capabilities of\nreasoning models, and only providing more context or allowing more compute\nbudget for reasoning is not enough to improve their discrimination performance.\nFinally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find\ngeneration more challenging than discrimination and may underperform as\ngenerators compared to smaller non-reasoning LLMs. Our work highlights the\npotential of reasoning models as discriminators in agentic frameworks, far\noutweighing their capabilities as generators, offering insights into their\noptimal role within LLM planning infrastructures."}
{"id": "2505.03799", "pdf": "https://arxiv.org/pdf/2505.03799", "abs": "https://arxiv.org/abs/2505.03799", "authors": ["Hyun Lee", "Chris Yi", "Maminur Islam", "B. D. S. Aritra"], "title": "Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "To be published in International Joint Conference on Neural Networks\n  (IJCNN), 2025", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in various\nnatural language processing tasks; however, their application to graph-related\nproblems remains limited, primarily due to scalability constraints and the\nabsence of dedicated mechanisms for processing graph structures. Existing\napproaches predominantly integrate LLMs with Graph Neural Networks (GNNs),\nusing GNNs as feature encoders or auxiliary components. However, directly\nencoding graph structures within LLMs has been underexplored, particularly in\nthe context of large-scale graphs where token limitations hinder effective\nrepresentation. To address these challenges, we propose SDM-InstructGLM, a\nnovel instruction-tuned Graph Language Model (InstructGLM) framework that\nenhances scalability and efficiency without relying on GNNs. Our method\nintroduces a similarity-degree-based biased random walk mechanism, which\nselectively samples and encodes graph information based on node-feature\nsimilarity and degree centrality, ensuring an adaptive and structured\nrepresentation within the LLM. This approach significantly improves token\nefficiency, mitigates information loss due to random sampling, and enhances\nperformance on graph-based tasks such as node classification and link\nprediction. Furthermore, our results demonstrate the feasibility of LLM-only\ngraph processing, enabling scalable and interpretable Graph Language Models\n(GLMs) optimized through instruction-based fine-tuning. This work paves the way\nfor GNN-free approaches to graph learning, leveraging LLMs as standalone graph\nreasoning models. Our source code is available on GitHub."}
{"id": "2505.03810", "pdf": "https://arxiv.org/pdf/2505.03810", "abs": "https://arxiv.org/abs/2505.03810", "authors": ["Euntae Choi", "Sumin Song", "Woosang Lim", "Sungjoo Yoo"], "title": "Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "7 pages", "summary": "Large Language Models (LLMs) face deployment challenges due to high\ncomputational costs, and while Post-Training Quantization (PTQ) offers a\nsolution, existing rotation-based methods struggle at very low bit-widths like\n2-bit. We introduce a novel, training-free approach to construct an improved\nrotation matrix, addressing the limitations of current methods. The key\ncontributions include leveraging the Walsh-Hadamard transform with sequency\nordering, which clusters similar frequency components to reduce quantization\nerror compared to standard Hadamard matrices, significantly improving\nperformance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR)\nusing block-diagonal matrices with smaller Walsh blocks, effectively isolating\noutlier impacts and achieving performance comparable to optimization-based\nmethods without requiring any training. Our method demonstrates robust\nperformance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our\nmethod also enhances results even when applied over existing learned rotation\ntechniques."}
{"id": "2505.03814", "pdf": "https://arxiv.org/pdf/2505.03814", "abs": "https://arxiv.org/abs/2505.03814", "authors": ["Ganghua Wang", "Zhaorun Chen", "Bo Li", "Haifeng Xu"], "title": "Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "As foundation models continue to scale, the size of trained models grows\nexponentially, presenting significant challenges for their evaluation. Current\nevaluation practices involve curating increasingly large datasets to assess the\nperformance of large language models (LLMs). However, there is a lack of\nsystematic analysis and guidance on determining the sufficiency of test data or\nselecting informative samples for evaluation. This paper introduces a\ncertifiable and cost-efficient evaluation framework for LLMs. Our framework\nadapts to different evaluation objectives and outputs confidence intervals that\ncontain true values with high probability. We use ``test sample complexity'' to\nquantify the number of test points needed for a certifiable evaluation and\nderive tight bounds on test sample complexity. Based on the developed theory,\nwe develop a partition-based algorithm, named Cer-Eval, that adaptively selects\ntest points to minimize the cost of LLM evaluation. Real-world experiments\ndemonstrate that Cer-Eval can save 20% to 40% test points across various\nbenchmarks, while maintaining an estimation error level comparable to the\ncurrent evaluation process and providing a 95% confidence guarantee."}
{"id": "2505.03828", "pdf": "https://arxiv.org/pdf/2505.03828", "abs": "https://arxiv.org/abs/2505.03828", "authors": ["Yogesh Gajula"], "title": "Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "12 pages, 2 tables, 2 figures", "summary": "E-commerce platforms generate vast volumes of user feedback, such as star\nratings, written reviews, and comments. However, most recommendation engines\nrely primarily on numerical scores, often overlooking the nuanced opinions\nembedded in free text. This paper comprehensively reviews sentiment-aware\nrecommendation systems from a natural language processing perspective, covering\nadvancements from 2023 to early 2025. It highlights the benefits of integrating\nsentiment analysis into e-commerce recommenders to enhance prediction accuracy\nand explainability through detailed opinion extraction. Our survey categorizes\nrecent work into four main approaches: deep learning classifiers that combine\nsentiment embeddings with user item interactions, transformer based methods for\nnuanced feature extraction, graph neural networks that propagate sentiment\nsignals, and conversational recommenders that adapt in real time to user\nfeedback. We summarize model architectures and demonstrate how sentiment flows\nthrough recommendation pipelines, impacting dialogue-based suggestions. Key\nchallenges include handling noisy or sarcastic text, dynamic user preferences,\nand bias mitigation. Finally, we outline research gaps and provide a roadmap\nfor developing smarter, fairer, and more user-centric recommendation tools."}
{"id": "2505.03961", "pdf": "https://arxiv.org/pdf/2505.03961", "abs": "https://arxiv.org/abs/2505.03961", "authors": ["Gerrit GroÃmann", "Larisa Ivanova", "Sai Leela Poduru", "Mohaddeseh Tabrizian", "Islam Mesabah", "David A. Selby", "Sebastian J. Vollmer"], "title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete", "categories": ["cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; I.6; J.4"], "comment": "16 pages, 8 figures. Code available at\n  https://github.com/storyagents25/story-agents", "summary": "According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment."}
{"id": "2505.03997", "pdf": "https://arxiv.org/pdf/2505.03997", "abs": "https://arxiv.org/abs/2505.03997", "authors": ["Prudhviraj Naidu", "Zixian Wang", "Leon Bergen", "Ramamohan Paturi"], "title": "Quiet Feature Learning in Algorithmic Tasks", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We train Transformer-based language models on ten foundational algorithmic\ntasks and observe pronounced phase transitions in their loss curves that\ndeviate from established power-law scaling trends. Over large ranges of\ncompute, the validation loss barely improves, then abruptly decreases. Probing\nthe models' internal representations reveals the learning of quiet features\nduring the stagnant phase, followed by sudden acquisition of loud features that\ncoincide with the sharp drop in loss. Our ablation experiments show that\ndisrupting a single learned feature can dramatically degrade performance,\nproviding evidence of their causal role in task performance. These findings\nchallenge the prevailing assumption that next-token predictive loss reliably\ntracks incremental progress; instead, key internal features may be developing\nbelow the surface until they coalesce, triggering a rapid performance gain."}
{"id": "2505.04066", "pdf": "https://arxiv.org/pdf/2505.04066", "abs": "https://arxiv.org/abs/2505.04066", "authors": ["Tuochao Chen", "Nicholas Batchelder", "Alisa Liu", "Noah Smith", "Shyamnath Gollakota"], "title": "LLAMAPIE: Proactive In-Ear Conversation Assistants", "categories": ["cs.LG", "cs.CL", "eess.AS"], "comment": null, "summary": "We introduce LlamaPIE, the first real-time proactive assistant designed to\nenhance human conversations through discreet, concise guidance delivered via\nhearable devices. Unlike traditional language models that require explicit user\ninvocation, this assistant operates in the background, anticipating user needs\nwithout interrupting conversations. We address several challenges, including\ndetermining when to respond, crafting concise responses that enhance\nconversations, leveraging knowledge of the user for context-aware assistance,\nand real-time, on-device processing. To achieve this, we construct a\nsemi-synthetic dialogue dataset and propose a two-model pipeline: a small model\nthat decides when to respond and a larger model that generates the response. We\nevaluate our approach on real-world datasets, demonstrating its effectiveness\nin providing helpful, unobtrusive assistance. User studies with our assistant,\nimplemented on Apple Silicon M2 hardware, show a strong preference for the\nproactive assistant over both a baseline with no assistance and a reactive\nmodel, highlighting the potential of LlamaPie to enhance live conversations."}
{"id": "2505.04171", "pdf": "https://arxiv.org/pdf/2505.04171", "abs": "https://arxiv.org/abs/2505.04171", "authors": ["Nouar Aldahoul", "Hazem Ibrahim", "Matteo Varvello", "Aaron Kaufman", "Talal Rahwan", "Yasir Zaki"], "title": "Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts", "categories": ["cs.CY", "cs.CL"], "comment": "61 pages, 29 figures", "summary": "Large Language Models (LLMs) are a transformational technology, fundamentally\nchanging how people obtain information and interact with the world. As people\nbecome increasingly reliant on them for an enormous variety of tasks, a body of\nacademic research has developed to examine these models for inherent biases,\nespecially political biases, often finding them small. We challenge this\nprevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a\nnationally representative sample of U.S. voters, we show that LLMs' apparently\nsmall overall partisan preference is the net result of offsetting extreme views\non specific topics, much like moderate voters. Second, in a randomized\nexperiment, we show that LLMs can promulgate their preferences into political\npersuasiveness even in information-seeking contexts: voters randomized to\ndiscuss political issues with an LLM chatbot are as much as 5 percentage points\nmore likely to express the same preferences as that chatbot. Contrary to\nexpectations, these persuasive effects are not moderated by familiarity with\nLLMs, news consumption, or interest in politics. LLMs, especially those\ncontrolled by private companies or governments, may become a powerful and\ntargeted vector for political influence."}
{"id": "2505.04192", "pdf": "https://arxiv.org/pdf/2505.04192", "abs": "https://arxiv.org/abs/2505.04192", "authors": ["Trinh T. L. Vuong", "Jin Tae Kwak"], "title": "VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present VideoPath-LLaVA, the first large multimodal model (LMM) in\ncomputational pathology that integrates three distinct image scenarios, single\npatch images, automatically keyframe-extracted clips, and manually segmented\nvideo pathology images, to mimic the natural diagnostic process of\npathologists. By generating detailed histological descriptions and culminating\nin a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives\nwith diagnostic reasoning.\n  Central to our approach is the VideoPath-Instruct dataset, comprising 4278\nvideo and diagnosis-specific chain-of-thought instructional pairs sourced from\neducational histopathology videos on YouTube. Although high-quality data is\ncritical for enhancing diagnostic reasoning, its creation is time-intensive and\nlimited in volume. To overcome this challenge, we transfer knowledge from\nexisting single-image instruction datasets to train on weakly annotated,\nkeyframe-extracted clips, followed by fine-tuning on manually segmented videos.\nVideoPath-LLaVA establishes a new benchmark in pathology video analysis and\noffers a promising foundation for future AI systems that support clinical\ndecision-making through integrated visual and diagnostic reasoning. Our code,\ndata, and model are publicly available at\nhttps://github.com/trinhvg/VideoPath-LLaVA."}
{"id": "2505.04364", "pdf": "https://arxiv.org/pdf/2505.04364", "abs": "https://arxiv.org/abs/2505.04364", "authors": ["Kai Ruan", "Mowen Huang", "Ji-Rong Wen", "Hao Sun"], "title": "Benchmarking LLMs' Swarm intelligence", "categories": ["cs.MA", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench."}
{"id": "2505.04457", "pdf": "https://arxiv.org/pdf/2505.04457", "abs": "https://arxiv.org/abs/2505.04457", "authors": ["Shigeki Karita", "Yuma Koizumi", "Heiga Zen", "Haruko Ishikawa", "Robin Scheibler", "Michiel Bacchiani"], "title": "Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Training data cleaning is a new application for generative model-based speech\nrestoration (SR). This paper introduces Miipher-2, an SR model designed for\nmillion-hour scale data, for training data cleaning for large-scale generative\nmodels like large language models. Key challenges addressed include\ngeneralization to unseen languages, operation without explicit conditioning\n(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a\nfrozen, pre-trained Universal Speech Model (USM), supporting over 300\nlanguages, as a robust, conditioning-free feature extractor. To optimize\nefficiency and minimize memory, Miipher-2 incorporates parallel adapters for\npredicting clean USM features from noisy inputs and employs the WaneFit neural\nvocoder for waveform synthesis. These components were trained on 3,000 hours of\nmulti-lingual, studio-quality recordings with augmented degradations, while USM\nparameters remained fixed. Experimental results demonstrate Miipher-2's\nsuperior or comparable performance to conventional SR models in\nword-error-rate, speaker similarity, and both objective and subjective sound\nquality scores across all tested languages. Miipher-2 operates efficiently on\nconsumer-grade accelerators, achieving a real-time factor of 0.0078, enabling\nthe processing of a million-hour speech dataset in approximately three days\nusing only 100 such accelerators."}
{"id": "2505.04528", "pdf": "https://arxiv.org/pdf/2505.04528", "abs": "https://arxiv.org/abs/2505.04528", "authors": ["Qi Liu", "Xinhao Zheng", "Renqiu Xia", "Xingzhi Qi", "Qinxiang Cao", "Junchi Yan"], "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "42 pages, 3 figures", "summary": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving."}
